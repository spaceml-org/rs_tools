{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to <code>RSTools</code>","text":""},{"location":"installation/","title":"Installation","text":"<p>We can install via the github repo</p> <pre><code>pip install git+https://github.com/space-ml/rs_tools.git\n</code></pre>"},{"location":"installation/#development-version","title":"Development Version","text":"<pre><code>git clone https://github.com/space-ml/rs_tools.git\ncd rs_tools\npoetry install\n</code></pre> <p>Tip</p> <p>We advise you to create a virtual environment before installing:</p> <pre><code>conda env create -f environment.yaml\nconda activate rs_tools\n</code></pre> <p>and recommend you check your installation passes the supplied unit tests:</p> <pre><code>poetry run pytest tests/\n</code></pre>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>RSTOOLS<ul> <li>Source<ul> <li>Data<ul> <li>GOES<ul> <li>Download</li> <li>Downloader_Goes16</li> </ul> </li> <li>MODIS<ul> <li>Bands</li> <li>Download</li> <li>Downloader_Aqua</li> <li>Downloader_Terra</li> </ul> </li> <li>MSG<ul> <li>Download</li> <li>Downloader_Msg</li> </ul> </li> </ul> </li> <li>Datamodule<ul> <li>.Ipynb_Checkpoints<ul> <li>__Init__-Checkpoint</li> <li>Datasets-Checkpoint</li> </ul> </li> <li>Datasets</li> <li>Editor</li> <li>Utils</li> </ul> </li> <li>Geoprocessing<ul> <li>GOES<ul> <li>Geoprocessor_Goes16</li> <li>Interp</li> <li>Reproject</li> <li>Validation</li> </ul> </li> <li>Grid</li> <li>Interp</li> <li>Match</li> <li>MODIS<ul> <li>Geoprocessor_Modis</li> <li>Interp</li> <li>Reproject</li> <li>Rescale</li> </ul> </li> <li>MSG<ul> <li>Geoprocessor_Msg</li> <li>Reproject</li> </ul> </li> <li>Reproject</li> <li>Units</li> <li>Utils</li> </ul> </li> <li>Preprocessing<ul> <li>Normalize</li> <li>Prepatcher</li> </ul> </li> <li>Utils<ul> <li>Io</li> <li>Math</li> </ul> </li> </ul> </li> <li>Train</li> </ul> </li> </ul>"},{"location":"api/train/","title":"Train","text":""},{"location":"api/_src/data/goes/download/","title":"Download","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download","title":"<code>goes_download(start_date, end_date=None, start_time='00:00:00', end_time='23:59:00', daily_window_t0='00:00:00', daily_window_t1='23:59:00', time_step=None, satellite_number=16, save_dir='.', instrument='ABI', processing_level='L1b', data_product='Rad', domain='F', bands='all', check_bands_downloaded=False)</code>","text":"<p>Downloads GOES satellite data for a specified time period and set of bands.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data download in the format 'YYYY-MM-DD'.</p> required <code>end_date</code> <code>str</code> <p>The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.</p> <code>None</code> <code>start_time</code> <code>str</code> <p>The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.</p> <code>'00:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.</p> <code>'23:59:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.</p> <code>'00:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.</p> <code>'23:59:00'</code> <code>time_step</code> <code>str</code> <p>The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.</p> <code>None</code> <code>satellite_number</code> <code>int</code> <p>The satellite number. Default is 16.</p> <code>16</code> <code>save_dir</code> <code>str</code> <p>The directory where the downloaded files will be saved. Default is the current directory.</p> <code>'.'</code> <code>instrument</code> <code>str</code> <p>The instrument name. Default is 'ABI'.</p> <code>'ABI'</code> <code>processing_level</code> <code>str</code> <p>The processing level of the data. Default is 'L1b'.</p> <code>'L1b'</code> <code>data_product</code> <code>str</code> <p>The data product to download. Default is 'Rad'.</p> <code>'Rad'</code> <code>domain</code> <code>str</code> <p>The domain of the data. Default is 'F' - Full Disk.</p> <code>'F'</code> <code>bands</code> <code>str</code> <p>The bands to download. Default is 'all'.</p> <code>'all'</code> <code>check_bands_downloaded</code> <code>bool</code> <p>Whether to check if all bands were successfully downloaded for each time step. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths for the downloaded files.</p> <p>Examples:</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_1","title":"=========================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--goes-level-1b-test-cases","title":"GOES LEVEL 1B Test Cases","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_2","title":"=========================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day","title":"custom day","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day-end-points","title":"custom day + end points","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day-end-points-time-window","title":"custom day + end points + time window","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--custom-day-end-points-time-window-timestep","title":"custom day + end points + time window + timestep","text":"<p>python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00 --time-step 06:00:00</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_3","title":"===================================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--goes-level-2-cloud-mask-test-cases","title":"GOES LEVEL 2 CLOUD MASK Test Cases","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_4","title":"===================================","text":"<p>python scripts/goes-download.py 2020-10-01 --start-time 10:00:00 --end-time 11:00:00 --processing-level L2 --data-product ACM</p>"},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_5","title":"====================","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--failure-test-cases","title":"FAILURE TEST CASES","text":""},{"location":"api/_src/data/goes/download/#rs_tools._src.data.goes.download.goes_download--_6","title":"====================","text":"<p>python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/ python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/ --check-bands-downloaded</p> Source code in <code>rs_tools/_src/data/goes/download.py</code> <pre><code>def goes_download(\n    start_date: str,\n    end_date: Optional[str]=None,\n    start_time: Optional[str]='00:00:00',\n    end_time: Optional[str]='23:59:00',\n    daily_window_t0: Optional[str]='00:00:00',\n    daily_window_t1: Optional[str]='23:59:00',\n    time_step: Optional[str]=None,\n    satellite_number: int=16,\n    save_dir: Optional[str] = \".\",\n    instrument: str = \"ABI\",\n    processing_level: str = 'L1b',\n    data_product: str = 'Rad',\n    domain: str = 'F',\n    bands: Optional[str] = \"all\",\n    check_bands_downloaded: bool = False,\n):\n    \"\"\"\n    Downloads GOES satellite data for a specified time period and set of bands.\n\n    Args:\n        start_date (str): The start date of the data download in the format 'YYYY-MM-DD'.\n        end_date (str, optional): The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.\n        start_time (str, optional): The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.\n        end_time (str, optional): The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.\n        daily_window_t0 (str, optional): The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.\n        daily_window_t1 (str, optional): The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.\n        time_step (str, optional): The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.\n        satellite_number (int, optional): The satellite number. Default is 16.\n        save_dir (str, optional): The directory where the downloaded files will be saved. Default is the current directory.\n        instrument (str, optional): The instrument name. Default is 'ABI'.\n        processing_level (str, optional): The processing level of the data. Default is 'L1b'.\n        data_product (str, optional): The data product to download. Default is 'Rad'.\n        domain (str, optional): The domain of the data. Default is 'F' - Full Disk.\n        bands (str, optional): The bands to download. Default is 'all'.\n        check_bands_downloaded (bool, optional): Whether to check if all bands were successfully downloaded for each time step. Default is False.\n\n    Returns:\n        list: A list of file paths for the downloaded files.\n\n    Examples:\n        # =========================\n        # GOES LEVEL 1B Test Cases\n        # =========================\n        # custom day\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01\n        # custom day + end points\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00\n        # custom day + end points + time window\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00\n        # custom day + end points + time window + timestep\n        python scripts/goes-download.py 2020-10-01 --end-date 2020-10-01 --start-time 00:00:00 --end-time 23:00:00 --daily-window-t0 08:30:00 --daily-window-t1 21:30:00 --time-step 06:00:00\n        # ===================================\n        # GOES LEVEL 2 CLOUD MASK Test Cases\n        # ===================================\n        python scripts/goes-download.py 2020-10-01 --start-time 10:00:00 --end-time 11:00:00 --processing-level L2 --data-product ACM\n\n        # ====================\n        # FAILURE TEST CASES\n        # ====================\n        python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/\n        python scripts/goes-download.py 2018-10-01 --end-date 2018-10-01 --daily-window-t0 17:00:00 --daily-window-t1 17:14:00 --time-step 00:15:00 --save-dir /home/juanjohn/data/ --check-bands-downloaded\n    \"\"\"\n\n    # run checks\n    # check satellite details\n    _check_input_processing_level(processing_level=processing_level)\n    _check_instrument(instrument=instrument)\n    _check_satellite_number(satellite_number=satellite_number)\n    logger.info(f\"Satellite Number: {satellite_number}\")\n    _check_domain(domain=domain)\n    # compile bands\n    if processing_level == 'L1b':\n        list_of_bands = _check_bands(bands=bands)\n    elif processing_level == 'L2':\n        list_of_bands = None\n    else:\n        raise ValueError('bands not correctly specified for given processing level')\n    # check data product\n    data_product = f\"{instrument}-{processing_level}-{data_product}{domain}\"\n    logger.info(f\"Data Product: {data_product}\")\n    _check_data_product_name(data_product=data_product)\n\n    # check start/end dates/times\n    if end_date is None:\n        end_date = start_date\n\n    # combine date and time information\n    start_datetime_str = start_date + ' ' + start_time\n    end_datetime_str = end_date + ' ' + end_time\n    _check_datetime_format(start_datetime_str, end_datetime_str)\n    # datetime conversion \n    start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n\n    # define time step for data query                       \n    if time_step is None: \n        time_step = '1:00:00'\n        logger.info(\"No timedelta specified. Default is 1 hour.\")\n    _check_timedelta_format(time_delta=time_step)\n\n    # convert str to datetime object\n    hours, minutes, seconds = convert_str2time(time=time_step)\n    time_delta = timedelta(hours=hours, minutes=minutes, seconds=seconds)\n\n    _check_timedelta(time_delta=time_delta, domain=domain)\n\n    # Compile list of dates/times\n    list_of_dates = np.arange(start_datetime, end_datetime + time_delta, time_delta).astype(datetime)\n    print('Times to check: ',list_of_dates[0], list_of_dates[-1])\n\n    window_date = '1991-10-19' # Add arbitrary date to convert into proper datetime object\n    start_datetime_window_str = window_date + ' ' + str(daily_window_t0)\n    end_datetime_window_str = window_date + ' ' + str(daily_window_t1)\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n    # datetime conversion \n    daily_window_t0_datetime = datetime.strptime(start_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    daily_window_t1_datetime = datetime.strptime(end_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=daily_window_t0_datetime, end_datetime=daily_window_t1_datetime)\n\n    # filter function - check that query times fall within desired time window\n    def is_in_between(date):\n       return daily_window_t0_datetime.time() &lt;= date.time() &lt;= daily_window_t1_datetime.time()\n\n    # compile new list of dates within desired time window\n    list_of_dates = list(filter(is_in_between, list_of_dates))\n    if list_of_dates == []:\n        msg = f\"No times within the specified time window {daily_window_t0_datetime.time()} - {daily_window_t1_datetime.time()} for time step {time_step}\"\n        msg += f\"\\n adjust daily window times or time step\"\n        raise ValueError(msg)\n\n    # check if save_dir is valid before attempting to download\n    _check_save_dir(save_dir=save_dir)\n\n    files = []\n\n    # create progress bars for dates and bands\n    pbar_time = tqdm.tqdm(list_of_dates)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Time - {itime}\")\n\n        if processing_level == 'L1b':\n            sub_files_list = _goes_level1_download(\n                time=itime, \n                list_of_bands=list_of_bands,\n                satellite_number=satellite_number,\n                data_product=data_product,\n                domain=domain,\n                save_dir=save_dir,\n                check_bands_downloaded=check_bands_downloaded,\n                )\n        elif processing_level == 'L2':\n            sub_files_list = _goes_level2_download(\n                time=itime, \n                satellite_number=satellite_number,\n                data_product=data_product,\n                domain=domain,\n                save_dir=save_dir)\n        else:\n            raise ValueError(f\"Unrecognized processing level: {processing_level}\")\n\n        files += sub_files_list\n\n    return files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/","title":"Downloader Goes16","text":""},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.GOES16Download","title":"<code>GOES16Download</code>  <code>dataclass</code>","text":"<p>A class for downloading GOES 16 data and cloud mask</p> <p>Attributes:</p> Name Type Description <code>start_date</code> <code>str</code> <p>The start date of the data to download.</p> <code>end_date</code> <code>str</code> <p>The end date of the data to download.</p> <code>start_time</code> <code>str</code> <p>The start time of the data to download.</p> <code>end_time</code> <code>str</code> <p>The end time of the data to download.</p> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window for data download.</p> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window for data download.</p> <code>time_step</code> <code>str</code> <p>The time step for data download.</p> <code>save_dir</code> <code>str</code> <p>The directory to save the downloaded data.</p> <p>Methods:</p> Name Description <code>download</code> <p>Downloads GOES 16 data.</p> <code>download_cloud_mask</code> <p>Downloads GOES 16 cloud mask.</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>@dataclass\nclass GOES16Download:\n    \"\"\"A class for downloading GOES 16 data and cloud mask\n\n    Attributes:\n        start_date (str): The start date of the data to download.\n        end_date (str): The end date of the data to download.\n        start_time (str): The start time of the data to download.\n        end_time (str): The end time of the data to download.\n        daily_window_t0 (str): The start time of the daily window for data download.\n        daily_window_t1 (str): The end time of the daily window for data download.\n        time_step (str): The time step for data download.\n        save_dir (str): The directory to save the downloaded data.\n\n    Methods:\n        download: Downloads GOES 16 data.\n        download_cloud_mask: Downloads GOES 16 cloud mask.  \n    \"\"\"\n\n    start_date: str\n    end_date: str \n    start_time: str \n    end_time: str \n    daily_window_t0: str  \n    daily_window_t1: str  \n    time_step: str\n    save_dir: str \n\n    def download(self) -&gt; List[str]:\n        \"\"\"Downloads GOES 16 data\"\"\"\n        goes_files = goes_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite_number=16,\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            instrument=\"ABI\",\n            processing_level='L1b',\n            data_product='Rad',\n            domain='F',\n            bands='all',\n            check_bands_downloaded=True,\n        )\n        return goes_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        \"\"\"Downloads GOES 16 cloud mask\"\"\"\n        goes_files = goes_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite_number=16,\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            instrument=\"ABI\",\n            processing_level='L2',\n            data_product='ACM',\n            domain='F',\n            bands='all',\n            check_bands_downloaded=True,\n        )\n        return goes_files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.GOES16Download.download","title":"<code>download()</code>","text":"<p>Downloads GOES 16 data</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>def download(self) -&gt; List[str]:\n    \"\"\"Downloads GOES 16 data\"\"\"\n    goes_files = goes_download(\n        start_date=self.start_date,\n        end_date=self.end_date,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        daily_window_t0=self.daily_window_t0, \n        daily_window_t1=self.daily_window_t1, \n        time_step=self.time_step,\n        satellite_number=16,\n        save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n        instrument=\"ABI\",\n        processing_level='L1b',\n        data_product='Rad',\n        domain='F',\n        bands='all',\n        check_bands_downloaded=True,\n    )\n    return goes_files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.GOES16Download.download_cloud_mask","title":"<code>download_cloud_mask()</code>","text":"<p>Downloads GOES 16 cloud mask</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>def download_cloud_mask(self) -&gt; List[str]:\n    \"\"\"Downloads GOES 16 cloud mask\"\"\"\n    goes_files = goes_download(\n        start_date=self.start_date,\n        end_date=self.end_date,\n        start_time=self.start_time,\n        end_time=self.end_time,\n        daily_window_t0=self.daily_window_t0, \n        daily_window_t1=self.daily_window_t1, \n        time_step=self.time_step,\n        satellite_number=16,\n        save_dir=Path(self.save_dir).joinpath(\"CM\"),\n        instrument=\"ABI\",\n        processing_level='L2',\n        data_product='ACM',\n        domain='F',\n        bands='all',\n        check_bands_downloaded=True,\n    )\n    return goes_files\n</code></pre>"},{"location":"api/_src/data/goes/downloader_goes16/#rs_tools._src.data.goes.downloader_goes16.download","title":"<code>download(start_date='2020-10-02', end_date='2020-10-02', start_time='14:00:00', end_time='20:00:00', daily_window_t0='14:00:00', daily_window_t1='14:30:00', time_step='00:15:00', save_dir='./data/', cloud_mask=True)</code>","text":"<p>Downloads GOES 16 data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>end_date</code> <code>str</code> <p>The end date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>start_time</code> <code>str</code> <p>The start time of the data to download (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data to download (format: 'HH:MM:SS')</p> <code>'20:00:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window (format: 'HH:MM:SS')</p> <code>'14:30:00'</code> <code>time_step</code> <code>str</code> <p>The time step between consecutive data downloads (format: 'HH:MM:SS')</p> <code>'00:15:00'</code> <code>save_path</code> <code>str</code> <p>The path to save the downloaded data</p> required <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <p>List[str]: List of downloaded file names</p> Source code in <code>rs_tools/_src/data/goes/downloader_goes16.py</code> <pre><code>def download(\n        start_date: str=\"2020-10-02\",\n        end_date: str=\"2020-10-02\", \n        start_time: str=\"14:00:00\", \n        end_time: str=\"20:00:00\", \n        daily_window_t0: str=\"14:00:00\",  \n        daily_window_t1: str=\"14:30:00\",  \n        time_step: str=\"00:15:00\",\n        save_dir: str='./data/', \n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads GOES 16 data including cloud mask\n\n    Args:\n        start_date (str): The start date of the data to download (format: 'YYYY-MM-DD')\n        end_date (str): The end date of the data to download (format: 'YYYY-MM-DD')\n        start_time (str): The start time of the data to download (format: 'HH:MM:SS')\n        end_time (str): The end time of the data to download (format: 'HH:MM:SS')\n        daily_window_t0 (str): The start time of the daily window (format: 'HH:MM:SS')\n        daily_window_t1 (str): The end time of the daily window (format: 'HH:MM:SS')\n        time_step (str): The time step between consecutive data downloads (format: 'HH:MM:SS')\n        save_path (str): The path to save the downloaded data\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True)\n\n    Returns:\n        List[str]: List of downloaded file names\n    \"\"\"\n    logger.debug(f\"TIME STEP: {time_step}\")\n    # Initialize GOES 16 Downloader\n    logger.info(\"Initializing GOES16 Downloader...\")\n    dc_goes16_download = GOES16Download(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        daily_window_t0=daily_window_t0,\n        daily_window_t1=daily_window_t1,\n        time_step=time_step,\n        save_dir=Path(save_dir).joinpath(\"goes16\"),\n    )\n    logger.info(\"Downloading GOES 16 Data...\")\n    goes16_filenames = dc_goes16_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading GOES 16 Cloud Mask...\")\n        goes16_filenames = dc_goes16_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished GOES 16 Downloading Script...\")\n</code></pre>"},{"location":"api/_src/data/modis/bands/","title":"Bands","text":""},{"location":"api/_src/data/modis/download/","title":"Download","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.get_daily_window","title":"<code>get_daily_window(daily_start, end_time)</code>","text":"<p>computes tuple of start and end date/time for each day for earthaccess call</p> Source code in <code>rs_tools/_src/data/modis/download.py</code> <pre><code>def get_daily_window(daily_start, end_time):\n    \"\"\"computes tuple of start and end date/time for each day for earthaccess call\"\"\"\n    day = daily_start.strftime(\"%Y-%m-%d\")\n    daily_end = day + ' ' + end_time\n    return (daily_start.strftime(\"%Y-%m-%d %H:%M:%S\"), daily_end)\n</code></pre>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download","title":"<code>modis_download(start_date, end_date=None, start_time='00:00:00', end_time='23:59:00', day_step=1, satellite='Terra', save_dir='.', processing_level='L1b', resolution='1KM', bounding_box=(-180, -90, 180, 90), earthdata_username='', earthdata_password='', day_night_flag=None, identifier='02')</code>","text":"<p>Downloads MODIS satellite data for a specified time period and location.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data download in the format 'YYYY-MM-DD'.</p> required <code>end_date</code> <code>str</code> <p>The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.</p> <code>None</code> <code>start_time</code> <code>str</code> <p>The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.</p> <code>'00:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.</p> <code>'23:59:00'</code> <code>day_step</code> <code>int</code> <p>The time step (in days) between downloads. This is to allow the user to download data every e.g. 2 days. If not provided, the default is daily downloads.</p> <code>1</code> <code>satellite</code> <code>str</code> <p>The satellite. Options are \"Terra\" and \"Aqua\", with \"Terra\" as default.</p> <code>'Terra'</code> <code>save_dir</code> <code>str</code> <p>The directory where the downloaded files will be saved. Default is the current directory.</p> <code>'.'</code> <code>processing_level</code> <code>str</code> <p>The processing level of the data. Default is 'L1b'.</p> <code>'L1b'</code> <code>resolution</code> <code>str</code> <p>The resolution of the data. Options are \"QKM\" (250m), \"HKM (500m), \"1KM\" (1000m), with \"1KM\" as default. Not all bands are measured at all resolutions.</p> <code>'1KM'</code> <code>bounding_box</code> <code>tuple</code> <p>The region to be downloaded.</p> <code>(-180, -90, 180, 90)</code> <code>earthdata_username</code> <code>str</code> <p>Username associated with the NASA Earth Data login. Required for download.</p> <code>''</code> <code>earthdata_password</code> <code>str</code> <p>Password associated with the NASA Earth Data login. Required for download.</p> <code>''</code> <code>day_night_flag</code> <code>str</code> <p>The time of day for the data. Options are \"day\" and \"night\". If not provided, both day and night data will be downloaded.</p> <code>None</code> <code>identifier</code> <code>str</code> <p>The MODIS data product identifier. Options are \"02\" and \"35\". Default is \"02\".</p> <code>'02'</code> <p>Returns:     list: A list of file paths for the downloaded files.</p> <p>Examples:</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_1","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--modis-level-1b-test-cases","title":"MODIS LEVEL 1B Test Cases","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_2","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--one-day-successfully-downloaded-4-granules-all-nighttime","title":"one day - successfully downloaded 4 granules (all nighttime)","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/test_script/</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--multiple-days-finds-62-granules-stopped-download-for-times-sake-but-seemed-to-work","title":"multiple days - finds 62 granules, stopped download for times sake but seemed to work","text":"<p>python scripts/modis-download.py 2018-10-01 --end-date 2018-10-9 --day-step 3 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--test-bounding-box-successfully-downloaded-4-files-all-daytime","title":"test bounding box - successfully downloaded 4 files (all daytime)","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 20 5</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--test-daynight-flag-successfully-downloaded-1-file-daytime-only","title":"test day/night flag - successfully downloaded 1 file (daytime only)","text":"<p>python scripts/modis-download.py 2018-10-15 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 10 -5 15 --day-night-flag day</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_3","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--modis-level-2-cloud-mask-test-cases","title":"MODIS LEVEL 2 CLOUD MASK Test Cases","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_4","title":"=========================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--one-day-successfully-downloaded-4-granules-all-nighttime_1","title":"one day - successfully downloaded 4 granules (all nighttime)","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/ --processing-level L2 --identifier 35</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_5","title":"====================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--failure-test-cases","title":"FAILURE TEST CASES","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--_6","title":"====================","text":""},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--bounding-box-input-invalid-throws-error-as-expected","title":"bounding box input invalid - throws error as expected","text":"<p>python scripts/modis-download.py 2018-10-01 --bounding-box a b c d</p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--end-date-before-start-date-throws-error-as-expected","title":"end date before start date - throws error as expected","text":"<p>python scripts/modis-download.py 2018-10-01  --end-date 2018-09-01 </p>"},{"location":"api/_src/data/modis/download/#rs_tools._src.data.modis.download.modis_download--empty-results-warns-user-as-expected","title":"empty results - warns user as expected","text":"<p>python scripts/modis-download.py 2018-10-01 --start-time 07:00:00 --end-time 7:10:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 -5 -5</p> Source code in <code>rs_tools/_src/data/modis/download.py</code> <pre><code>def modis_download(\n    start_date: str,\n    end_date: Optional[str]=None,\n    start_time: Optional[str]='00:00:00', # used for daily window\n    end_time: Optional[str]='23:59:00', # used for daily window\n    day_step: Optional[int]=1, \n    satellite: str='Terra',\n    save_dir: Optional[str]=\".\",\n    processing_level: str = 'L1b',\n    resolution: str = \"1KM\",\n    bounding_box: Optional[tuple[float, float, float, float]]=(-180, -90, 180, 90), # TODO: Add polygon option\n    earthdata_username: Optional[str]=\"\",\n    earthdata_password: Optional[str]=\"\",\n    day_night_flag: Optional[str]=None, \n    identifier: Optional[str] = \"02\"\n):\n    \"\"\"\n    Downloads MODIS satellite data for a specified time period and location.\n\n    Args:        \n        start_date (str): The start date of the data download in the format 'YYYY-MM-DD'.\n        end_date (str, optional): The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.\n        start_time (str, optional): The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.\n        end_time (str, optional): The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.\n        day_step (int, optional): The time step (in days) between downloads. This is to allow the user to download data every e.g. 2 days. If not provided, the default is daily downloads.\n        satellite (str, optional): The satellite. Options are \"Terra\" and \"Aqua\", with \"Terra\" as default.\n        save_dir (str, optional): The directory where the downloaded files will be saved. Default is the current directory.\n        processing_level (str, optional): The processing level of the data. Default is 'L1b'.\n        resolution (str, optional): The resolution of the data. Options are \"QKM\" (250m), \"HKM (500m), \"1KM\" (1000m), with \"1KM\" as default. Not all bands are measured at all resolutions.\n        bounding_box (tuple, optional): The region to be downloaded.\n        earthdata_username (str): Username associated with the NASA Earth Data login. Required for download.\n        earthdata_password (str): Password associated with the NASA Earth Data login. Required for download.\n        day_night_flag (str, optional): The time of day for the data. Options are \"day\" and \"night\". If not provided, both day and night data will be downloaded.\n        identifier (str, optional): The MODIS data product identifier. Options are \"02\" and \"35\". Default is \"02\".\n    Returns:\n        list: A list of file paths for the downloaded files.\n\n    Examples:\n    # =========================\n    # MODIS LEVEL 1B Test Cases\n    # =========================\n    # one day - successfully downloaded 4 granules (all nighttime)\n    python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/test_script/\n\n    # multiple days - finds 62 granules, stopped download for times sake but seemed to work\n    python scripts/modis-download.py 2018-10-01 --end-date 2018-10-9 --day-step 3 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/\n\n    # test bounding box - successfully downloaded 4 files (all daytime)\n    python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 13:00:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 20 5\n\n    # test day/night flag - successfully downloaded 1 file (daytime only)\n    python scripts/modis-download.py 2018-10-15 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 10 -5 15 --day-night-flag day\n\n    # =========================\n    # MODIS LEVEL 2 CLOUD MASK Test Cases\n    # =========================\n\n    # one day - successfully downloaded 4 granules (all nighttime)\n    python scripts/modis-download.py 2018-10-01 --start-time 08:00:00 --end-time 8:10:00 --save-dir ./notebooks/modisdata/ --processing-level L2 --identifier 35\n\n    # ====================\n    # FAILURE TEST CASES\n    # ====================\n    # bounding box input invalid - throws error as expected\n    python scripts/modis-download.py 2018-10-01 --bounding-box a b c d\n\n    # end date before start date - throws error as expected\n    python scripts/modis-download.py 2018-10-01  --end-date 2018-09-01 \n\n    # empty results - warns user as expected\n    python scripts/modis-download.py 2018-10-01 --start-time 07:00:00 --end-time 7:10:00 --save-dir ./notebooks/modisdata/test_script/ --bounding-box -10 -10 -5 -5\n\n    \"\"\"\n    # check if earthdata login is available\n    _check_earthdata_login(earthdata_username=earthdata_username, earthdata_password=earthdata_password)\n\n    # check if netcdf4 backend is available\n    _check_netcdf4_backend()\n\n    # run checks\n    # translate str inputs to modis specific names\n    _check_input_processing_level(processing_level=processing_level)\n    _check_identifier(identifier=identifier)\n    satellite_code = _check_satellite(satellite=satellite)\n    resolution_code = _check_resolution(resolution=resolution)\n    logger.info(f\"Satellite: {satellite}\")\n    # check data product\n    if processing_level == 'L1b':\n        data_product = f\"{satellite_code}{identifier}{resolution_code}\"\n    elif processing_level == 'L2':\n        # TODO: Implement other level-2 products or allow passing in data_product?\n        # NOTE: Resolution argument not needed for cloud mask download\n        data_product = f\"{satellite_code}{identifier}_{processing_level}\"\n    else:\n        raise ValueError(\"Incorrect processing level, downloader only implemented for 'L1b' and 'L2'\")\n\n    logger.info(f\"Data Product: {data_product}\")\n    _check_data_product_name(data_product=data_product)\n\n    # check start/end dates/times\n    if end_date is None:\n        end_date = start_date\n\n    # combine date and time information\n    start_datetime_str = start_date + ' ' + start_time\n    end_datetime_str = end_date + ' ' + end_time\n    _check_datetime_format(start_datetime_str=start_datetime_str, end_datetime_str=end_datetime_str) \n    # datetime conversion\n    start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_dates(start_datetime=start_datetime, end_datetime=end_datetime)\n\n    # compile list of dates/times \n    day_delta = timedelta(days=day_step)\n    list_of_dates = np.arange(start_datetime, end_datetime, day_delta).astype(datetime)\n\n    list_of_daily_windows = [get_daily_window(daily_start, end_time) for daily_start in list_of_dates]\n\n    # check if save_dir is valid before attempting to download\n    _check_save_dir(save_dir=save_dir)\n\n    # check that bounding box is valid\n    # TODO: Add option to add multiple location requests\n    # NOTE: earthaccess allows other ways to specify spatial extent, e.g. polygon, point \n    # NOTE: extend to allow these options\n    _check_bounding_box(bounding_box=bounding_box)\n\n    # create dictionary of earthaccess search parameters\n    search_params = {\n        \"short_name\": data_product,\n        \"bounding_box\": bounding_box,\n    }\n\n    # if day_night_flag was provided, check that day_night_flag is valid\n    if day_night_flag: \n        _check_day_night_flag(day_night_flag=day_night_flag)\n        # add day_night_flag to search parameters\n        search_params[\"day_night_flag\"] = day_night_flag\n\n    # TODO: remove - logging search_params for testing\n    logger.info(f\"Search parameters: {search_params}\")\n\n    files = []\n\n    # create progress bar for dates\n    pbar_time = tqdm.tqdm(list_of_daily_windows)\n\n    for itime in pbar_time:\n        pbar_time.set_description(f\"Time - {itime[0]} to {itime[1]}\")\n        success_flag = True\n\n        # add daytime window to search parameters\n        search_params[\"temporal\"] = itime\n\n        # search for data\n        results_day = earthaccess.search_data(**search_params)\n\n        # check if any results were returned\n        if not results_day:\n            # if not: log warning and continue to next date\n            success_flag = False\n            warn = f\"No data found for {itime[0]} to {itime[1]} in the specified bounding box\"\n            if day_night_flag: warn += f\" for {day_night_flag}-time measurements only\"\n            logger.warning(warn)\n            continue\n\n        files_day = earthaccess.download(results_day, save_dir) \n        # TODO: can this fail? if yes, use try / except to prevent the programme from crashing\n        # TODO: check file sizes - if less than X MB (ca 70MB) the download failed\n        if success_flag:\n            files += files_day\n\n    return files    \n</code></pre>"},{"location":"api/_src/data/modis/downloader_aqua/","title":"Downloader Aqua","text":"<p>A General Pipeline for create ML-Ready Data - Downloading the Data - Data Harmonization - Normalizing - Patching</p>"},{"location":"api/_src/data/modis/downloader_aqua/#rs_tools._src.data.modis.downloader_aqua.MODISAquaDownload","title":"<code>MODISAquaDownload</code>  <code>dataclass</code>","text":"<p>Downloading class for AQUA/MODIS data and cloud mask</p> Source code in <code>rs_tools/_src/data/modis/downloader_aqua.py</code> <pre><code>@dataclass\nclass MODISAquaDownload:\n    \"\"\"Downloading class for AQUA/MODIS data and cloud mask\"\"\"\n    start_date: str\n    end_date: str\n    start_time: str\n    end_time: str\n    save_dir: str\n    bounding_box: tuple[float, float, float, float]\n\n    def download(self) -&gt; List[str]:\n        aqua_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Aqua\",\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            processing_level='L1b',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"02\"\n            )\n        return aqua_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        aqua_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Aqua\",\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            processing_level='L2',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"35\"\n            )\n        return aqua_files\n</code></pre>"},{"location":"api/_src/data/modis/downloader_aqua/#rs_tools._src.data.modis.downloader_aqua.download","title":"<code>download(start_date='2020-10-01', end_date='2020-10-01', start_time='14:00:00', end_time='21:00:00', save_dir='./data/', region='-130 -15 -90 5', cloud_mask=True)</code>","text":"<p>Downloads AQUA MODIS data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>end_date</code> <code>str</code> <p>The end date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>start_time</code> <code>str</code> <p>The start time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'21:00:00'</code> <code>save_dir</code> <code>str</code> <p>The directory path to save the downloaded files.</p> <code>'./data/'</code> <code>region</code> <code>str</code> <p>The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".</p> <code>'-130 -15 -90 5'</code> <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/data/modis/downloader_aqua.py</code> <pre><code>def download(\n        start_date: str = \"2020-10-01\", \n        end_date: str = \"2020-10-01\",\n        start_time: str = \"14:00:00\",\n        end_time: str = \"21:00:00\",\n        save_dir: str = \"./data/\",\n        region: str = \"-130 -15 -90 5\",\n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads AQUA MODIS data including cloud mask\n\n    Args:\n        start_date (str): The start date of the period to download files for in the format \"YYYY-MM-DD\".\n        end_date (str): The end date of the period to download files for in the format \"YYYY-MM-DD\".\n        start_time (str): The start time of the period to download files for in the format \"HH:MM:SS\".\n        end_time (str): The end time of the period to download files for in the format \"HH:MM:SS\".\n        save_dir (str): The directory path to save the downloaded files.\n        region (str, optional): The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True).\n\n    Returns:\n        None\n    \"\"\"\n    bounding_box = tuple(map(lambda x: int(x), region.split(\" \")))\n    # Initialize AQUA Downloader\n    logger.info(\"Initializing AQUA Downloader...\")\n    dc_aqua_download = MODISAquaDownload(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        save_dir=Path(save_dir).joinpath(\"aqua\"),\n        bounding_box=bounding_box,\n    )\n    logger.info(\"Downloading AQUA Data...\")\n    modis_filenames = dc_aqua_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading AQUA Cloud Mask...\")\n        modis_filenames = dc_aqua_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished AQUA Downloading Script...\")\n</code></pre>"},{"location":"api/_src/data/modis/downloader_terra/","title":"Downloader Terra","text":"<p>A General Pipeline for create ML-Ready Data - Downloading the Data - Data Harmonization - Normalizing - Patching</p>"},{"location":"api/_src/data/modis/downloader_terra/#rs_tools._src.data.modis.downloader_terra.MODISTerraDownload","title":"<code>MODISTerraDownload</code>  <code>dataclass</code>","text":"<p>Downloading class for TERRA/MODIS data and cloud mask</p> Source code in <code>rs_tools/_src/data/modis/downloader_terra.py</code> <pre><code>@dataclass\nclass MODISTerraDownload:\n    \"\"\"Downloading class for TERRA/MODIS data and cloud mask\"\"\"\n    start_date: str\n    end_date: str\n    start_time: str\n    end_time: str\n    save_dir: str\n    bounding_box: tuple[float, float, float, float]\n\n    def download(self) -&gt; List[str]:\n        terra_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Terra\",\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            processing_level='L1b',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"02\"\n            )\n        return terra_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        terra_files = modis_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time, \n            end_time=self.end_time,\n            day_step=1,\n            satellite=\"Terra\",\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            processing_level='L2',\n            resolution=\"1KM\",\n            bounding_box=self.bounding_box,\n            day_night_flag=\"day\",\n            identifier= \"35\"\n            )\n        return terra_files\n</code></pre>"},{"location":"api/_src/data/modis/downloader_terra/#rs_tools._src.data.modis.downloader_terra.download","title":"<code>download(start_date='2020-10-01', end_date='2020-10-01', start_time='14:00:00', end_time='21:00:00', save_dir='./data/', region='-130 -15 -90 5', cloud_mask=True)</code>","text":"<p>Downloads TERRA MODIS data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>end_date</code> <code>str</code> <p>The end date of the period to download files for in the format \"YYYY-MM-DD\".</p> <code>'2020-10-01'</code> <code>start_time</code> <code>str</code> <p>The start time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the period to download files for in the format \"HH:MM:SS\".</p> <code>'21:00:00'</code> <code>save_dir</code> <code>str</code> <p>The directory path to save the downloaded files.</p> <code>'./data/'</code> <code>region</code> <code>str</code> <p>The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".</p> <code>'-130 -15 -90 5'</code> <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/data/modis/downloader_terra.py</code> <pre><code>def download(\n        start_date: str = \"2020-10-01\", \n        end_date: str = \"2020-10-01\",\n        start_time: str = \"14:00:00\",\n        end_time: str = \"21:00:00\",\n        save_dir: str = \"./data/\",\n        region: str = \"-130 -15 -90 5\",\n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads TERRA MODIS data including cloud mask\n\n    Args:\n        start_date (str): The start date of the period to download files for in the format \"YYYY-MM-DD\".\n        end_date (str): The end date of the period to download files for in the format \"YYYY-MM-DD\".\n        start_time (str): The start time of the period to download files for in the format \"HH:MM:SS\".\n        end_time (str): The end time of the period to download files for in the format \"HH:MM:SS\".\n        save_dir (str): The directory path to save the downloaded files.\n        region (str, optional): The geographic region to download files for in the format \"min_lon min_lat max_lon max_lat\".\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True).\n\n    Returns:\n        None\n    \"\"\"\n    bounding_box = tuple(map(lambda x: int(x), region.split(\" \")))\n    # Initialize TERRA Downloader\n    logger.info(\"Initializing TERRA Downloader...\")\n    dc_terra_download = MODISTerraDownload(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        save_dir=Path(save_dir).joinpath(\"terra\"),\n        bounding_box=bounding_box,\n    )\n    logger.info(\"Downloading TERRA Data...\")\n    modis_filenames = dc_terra_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading TERRA Cloud Mask...\")\n        modis_filenames = dc_terra_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished TERRA Downloading Script...\")\n</code></pre>"},{"location":"api/_src/data/msg/download/","title":"Download","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download","title":"<code>msg_download(start_date, end_date=None, start_time='00:00:00', end_time='23:59:00', daily_window_t0='00:00:00', daily_window_t1='23:59:00', time_step=None, satellite='MSG', instrument='HRSEVIRI', processing_level='L1', save_dir='.', eumdac_key='', eumdac_secret='')</code>","text":"<p>Downloads GOES satellite data for a specified time period and set of bands.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data download in the format 'YYYY-MM-DD'.</p> required <code>end_date</code> <code>str</code> <p>The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.</p> <code>None</code> <code>start_time</code> <code>str</code> <p>The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.</p> <code>'00:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.</p> <code>'23:59:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.</p> <code>'00:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.</p> <code>'23:59:00'</code> <code>time_step</code> <code>str</code> <p>The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.</p> <code>None</code> <code>satellite</code> <code>str</code> <p>The satellite. Default is MSG.</p> <code>'MSG'</code> <code>instrument</code> <code>str</code> <p>The data product to download. Default is 'HRSEVIRI', also implemented for Cloud Mask (CLM).</p> <code>'HRSEVIRI'</code> <code>processing_level</code> <code>str</code> <p>The processing level of the data. Default is 'L1'.</p> <code>'L1'</code> <code>save_dir</code> <code>str</code> <p>The directory where the downloaded files will be saved. Default is the current directory.</p> <code>'.'</code> <code>eumdac_key</code> <code>str</code> <p>The EUMETSAT Data Centre (EUMDAC) API key. If not provided, the user will be prompted to enter the key.</p> <code>''</code> <code>eumdac_secret</code> <code>str</code> <p>The EUMETSAT Data Centre (EUMDAC) API secret. If not provided, the user will be prompted to enter the secret.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of file paths for the downloaded files.</p> <p>Examples:</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_1","title":"=========================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--msg-level-1-test-cases","title":"MSG LEVEL 1 Test Cases","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_2","title":"=========================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day","title":"custom day","text":"<p>python scripts/msg-download.py 2018-10-01 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points","title":"custom day + end points","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 09:00:00 --end-time 12:00:00 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:05:00 --end-time 12:05:00</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window","title":"custom day + end points + time window","text":"<p>scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 </p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window-timestep","title":"custom day + end points + time window + timestep","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:15:00 python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:25:00</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_3","title":"===================================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--msg-cloud-mask-test-cases","title":"MSG CLOUD MASK Test Cases","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_4","title":"===================================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day_1","title":"custom day","text":"<p>python scripts/msg-download.py 2018-10-01 --instrument=CLM</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points_1","title":"custom day + end points","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --instrument=CLM </p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window_1","title":"custom day + end points + time window","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --instrument=CLM </p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--custom-day-end-points-time-window-timestep_1","title":"custom day + end points + time window + timestep","text":"<p>python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --time-step 00:25:00 --instrument=CLM</p>"},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_5","title":"====================","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--failure-test-cases","title":"FAILURE TEST CASES","text":""},{"location":"api/_src/data/msg/download/#rs_tools._src.data.msg.download.msg_download--_6","title":"====================","text":"Source code in <code>rs_tools/_src/data/msg/download.py</code> <pre><code>def msg_download(\n    start_date: str,\n    end_date: Optional[str]=None,\n    start_time: Optional[str]='00:00:00', # EUMDAC did not find any data for 00:00:00\n    end_time: Optional[str]='23:59:00', # EUMDAC did not find any data for 23:59:00\n    daily_window_t0: Optional[str]='00:00:00',\n    daily_window_t1: Optional[str]='23:59:00',\n    time_step: Optional[str]=None,\n    satellite: str=\"MSG\",\n    instrument: str =\"HRSEVIRI\",\n    processing_level: Optional[str] = \"L1\",\n    save_dir: Optional[str] = \".\",\n    eumdac_key: Optional[str]=\"\",\n    eumdac_secret: Optional[str]=\"\",\n):\n    \"\"\"\n    Downloads GOES satellite data for a specified time period and set of bands.\n\n    Args:\n        start_date (str): The start date of the data download in the format 'YYYY-MM-DD'.\n        end_date (str, optional): The end date of the data download in the format 'YYYY-MM-DD'. If not provided, the end date will be the same as the start date.\n        start_time (str, optional): The start time of the data download in the format 'HH:MM:SS'. Default is '00:00:00'.\n        end_time (str, optional): The end time of the data download in the format 'HH:MM:SS'. Default is '23:59:00'.\n        daily_window_t0 (str, optional): The start time of the daily window in the format 'HH:MM:SS'. Default is '00:00:00'. Used if e.g., only day/night measurements are required.\n        daily_window_t1 (str, optional): The end time of the daily window in the format 'HH:MM:SS'. Default is '23:59:00'. Used if e.g., only day/night measurements are required.\n        time_step (str, optional): The time step between each data download in the format 'HH:MM:SS'. If not provided, the default is 1 hour.\n        satellite (str): The satellite. Default is MSG.\n        instrument (str): The data product to download. Default is 'HRSEVIRI', also implemented for Cloud Mask (CLM).\n        processing_level (str, optional): The processing level of the data. Default is 'L1'.\n        save_dir (str, optional): The directory where the downloaded files will be saved. Default is the current directory.\n        eumdac_key (str, optional): The EUMETSAT Data Centre (EUMDAC) API key. If not provided, the user will be prompted to enter the key.\n        eumdac_secret (str, optional): The EUMETSAT Data Centre (EUMDAC) API secret. If not provided, the user will be prompted to enter the secret.\n\n    Returns:\n        list: A list of file paths for the downloaded files.\n\n    Examples:\n        # =========================\n        # MSG LEVEL 1 Test Cases\n        # =========================\n        # custom day\n        python scripts/msg-download.py 2018-10-01\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01\n        # custom day + end points\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 09:00:00 --end-time 12:00:00\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:05:00 --end-time 12:05:00\n        # custom day + end points + time window\n        scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 \n        # custom day + end points + time window + timestep\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:15:00\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-01 --start-time 00:05:00 --end-time 23:54:00 --daily-window-t0 09:00:00 --daily-window-t1 12:00:00 --time-step 00:25:00\n        # ===================================\n        # MSG CLOUD MASK Test Cases\n        # ===================================\n        # custom day\n        python scripts/msg-download.py 2018-10-01 --instrument=CLM\n        # custom day + end points\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --instrument=CLM \n        # custom day + end points + time window\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --instrument=CLM \n        # custom day + end points + time window + timestep\n        python scripts/msg-download.py 2018-10-01 --end-date 2018-10-05 --start-time 09:00:00 --end-time 12:00:00 --time-step 00:25:00 --instrument=CLM\n        # ====================\n        # FAILURE TEST CASES\n        # ====================\n    \"\"\"\n\n    # run checks\n    # check if eumdac login is available\n    token = _check_eumdac_login(eumdac_key=eumdac_key, eumdac_secret=eumdac_secret)\n    datastore = eumdac.DataStore(token)\n\n    # check if netcdf4 backend is available\n    _check_netcdf4_backend()\n\n    # check satellite details\n    _check_input_processing_level(processing_level=processing_level)\n    _check_instrument(instrument=instrument)\n    # check data product\n    data_product = f\"EO:EUM:DAT:{satellite}:{instrument}\"\n    logger.info(f\"Data Product: {data_product}\")\n    _check_data_product_name(data_product=data_product)\n\n    # check start/end dates/times\n    if end_date is None:\n        end_date = start_date\n\n    # combine date and time information\n    start_datetime_str = start_date + ' ' + start_time\n    end_datetime_str = end_date + ' ' + end_time\n    _check_datetime_format(start_datetime_str, end_datetime_str)\n    # datetime conversion \n    start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n\n    # define time step for data query                       \n    if time_step is None: \n        time_step = '1:00:00'\n        logger.info(\"No timedelta specified. Default is 1 hour.\")\n    _check_timedelta_format(time_delta=time_step)\n\n    # convert str to datetime object\n    hours, minutes, seconds = convert_str2time(time=time_step)\n    time_delta = timedelta(hours=hours, minutes=minutes, seconds=seconds)\n\n    _check_timedelta(time_delta=time_delta)\n\n    # Compile list of dates/times\n    list_of_dates = np.arange(start_datetime, end_datetime, time_delta).astype(datetime)\n    print('Times to check: ',list_of_dates[0], list_of_dates[-1])\n\n    window_date = '1991-10-19' # Add arbitrary date to convert into proper datetime object\n    start_datetime_window_str = window_date + ' ' + daily_window_t0\n    end_datetime_window_str = window_date + ' ' + daily_window_t1\n    _check_start_end_times(start_datetime=start_datetime, end_datetime=end_datetime)\n    # datetime conversion \n    daily_window_t0_datetime = datetime.strptime(start_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    daily_window_t1_datetime = datetime.strptime(end_datetime_window_str, \"%Y-%m-%d %H:%M:%S\")\n    _check_start_end_times(start_datetime=daily_window_t0_datetime, end_datetime=daily_window_t1_datetime)\n\n    # filter function - check that query times fall within desired time window\n    def is_in_between(date):\n       return daily_window_t0_datetime.time() &lt;= date.time() &lt;= daily_window_t1_datetime.time()\n\n    # compile new list of dates within desired time window\n    list_of_dates = list(filter(is_in_between, list_of_dates))\n\n    # check if save_dir is valid before attempting to download\n    _check_save_dir(save_dir=save_dir)\n\n    files = []\n\n    # create progress bars for dates and bands\n    pbar_time = tqdm.tqdm(list_of_dates)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Time - {itime}\")\n\n        sub_files_list = _download(time=itime, data_product=data_product, save_dir=save_dir, datastore=datastore)\n        if sub_files_list is None:\n            if itime != list_of_dates[-1]:\n                logger.info(f\"Could not find data for time {itime}. Trying to add 5 mins to timestamp.\")\n                time_delta = timedelta(hours=0, minutes=5, seconds=0)\n                itime_5 = itime+ time_delta\n                sub_files_list = _download(time=itime_5, data_product=data_product, save_dir=save_dir, datastore=datastore)\n        if sub_files_list is None:\n            if itime != list_of_dates[-1]:\n                logger.info(f\"Could not find data for time {itime}. Trying to add 10 mins to timestamp.\")\n                time_delta = timedelta(hours=0, minutes=10, seconds=0)\n                itime_10 = itime+ time_delta\n                sub_files_list = _download(time=itime_10, data_product=data_product, save_dir=save_dir, datastore=datastore)\n\n        if sub_files_list is None:\n            logger.info(f\"Could not find data for time {itime}. Skipping to next time.\")\n        else:\n            files += sub_files_list\n\n    return files\n</code></pre>"},{"location":"api/_src/data/msg/downloader_msg/","title":"Downloader Msg","text":""},{"location":"api/_src/data/msg/downloader_msg/#rs_tools._src.data.msg.downloader_msg.MSGDownload","title":"<code>MSGDownload</code>  <code>dataclass</code>","text":"<p>Downloading class for MSG data and cloud mask</p> Source code in <code>rs_tools/_src/data/msg/downloader_msg.py</code> <pre><code>@dataclass\nclass MSGDownload:\n    \"\"\"Downloading class for MSG data and cloud mask\"\"\"\n    start_date: str\n    end_date: str \n    start_time: str \n    end_time: str \n    daily_window_t0: str  \n    daily_window_t1: str  \n    time_step: str\n    save_dir: str \n\n    def download(self) -&gt; List[str]:\n        msg_files = msg_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite=\"MSG\",\n            save_dir=Path(self.save_dir).joinpath(\"L1b\"),\n            instrument=\"HRSEVIRI\",\n            processing_level='L1',\n        )\n        return msg_files\n\n    def download_cloud_mask(self) -&gt; List[str]:\n        msg_files = msg_download(\n            start_date=self.start_date,\n            end_date=self.end_date,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            daily_window_t0=self.daily_window_t0, \n            daily_window_t1=self.daily_window_t1, \n            time_step=self.time_step,\n            satellite=\"MSG\",\n            save_dir=Path(self.save_dir).joinpath(\"CM\"),\n            instrument=\"CLM\",\n            processing_level='L1',\n        )\n        return msg_files\n</code></pre>"},{"location":"api/_src/data/msg/downloader_msg/#rs_tools._src.data.msg.downloader_msg.download","title":"<code>download(start_date='2020-10-02', end_date='2020-10-02', start_time='14:00:00', end_time='20:00:00', daily_window_t0='14:00:00', daily_window_t1='14:30:00', time_step='00:15:00', save_dir='./data/', cloud_mask=True)</code>","text":"<p>Downloads MSG data including cloud mask</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>The start date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>end_date</code> <code>str</code> <p>The end date of the data to download (format: 'YYYY-MM-DD')</p> <code>'2020-10-02'</code> <code>start_time</code> <code>str</code> <p>The start time of the data to download (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>end_time</code> <code>str</code> <p>The end time of the data to download (format: 'HH:MM:SS')</p> <code>'20:00:00'</code> <code>daily_window_t0</code> <code>str</code> <p>The start time of the daily window (format: 'HH:MM:SS')</p> <code>'14:00:00'</code> <code>daily_window_t1</code> <code>str</code> <p>The end time of the daily window (format: 'HH:MM:SS')</p> <code>'14:30:00'</code> <code>time_step</code> <code>str</code> <p>The time step between consecutive data downloads (format: 'HH:MM:SS')</p> <code>'00:15:00'</code> <code>save_path</code> <code>str</code> <p>The path to save the downloaded data</p> required <code>cloud_mask</code> <code>bool</code> <p>Whether to download the cloud mask data (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <p>List[str]: List of downloaded file names</p> Source code in <code>rs_tools/_src/data/msg/downloader_msg.py</code> <pre><code>def download(\n        start_date: str=\"2020-10-02\",\n        end_date: str=\"2020-10-02\", \n        start_time: str=\"14:00:00\", \n        end_time: str=\"20:00:00\", \n        daily_window_t0: str=\"14:00:00\",  \n        daily_window_t1: str=\"14:30:00\",  \n        time_step: str=\"00:15:00\",\n        save_dir: str='./data/', \n        cloud_mask: bool = True\n):\n    \"\"\"\n    Downloads MSG data including cloud mask\n\n    Args:\n        start_date (str): The start date of the data to download (format: 'YYYY-MM-DD')\n        end_date (str): The end date of the data to download (format: 'YYYY-MM-DD')\n        start_time (str): The start time of the data to download (format: 'HH:MM:SS')\n        end_time (str): The end time of the data to download (format: 'HH:MM:SS')\n        daily_window_t0 (str): The start time of the daily window (format: 'HH:MM:SS')\n        daily_window_t1 (str): The end time of the daily window (format: 'HH:MM:SS')\n        time_step (str): The time step between consecutive data downloads (format: 'HH:MM:SS')\n        save_path (str): The path to save the downloaded data\n        cloud_mask (bool, optional): Whether to download the cloud mask data (default: True)\n\n    Returns:\n        List[str]: List of downloaded file names\n    \"\"\"\n    # Initialize MSG Downloader\n    logger.info(\"Initializing MSG Downloader...\")\n    dc_msg_download = MSGDownload(\n        start_date=start_date,\n        end_date=end_date,\n        start_time=start_time,\n        end_time=end_time,\n        daily_window_t0=daily_window_t0,\n        daily_window_t1=daily_window_t1,\n        time_step=time_step,\n        save_dir=Path(save_dir).joinpath(\"msg\"),\n    )\n    logger.info(\"Downloading MSG Data...\")\n    msg_filenames = dc_msg_download.download()\n    logger.info(\"Done!\")\n    if cloud_mask:\n        logger.info(\"Downloading MSG Cloud Mask...\")\n        msg_filenames = dc_msg_download.download_cloud_mask()\n        logger.info(\"Done!\")\n\n    logger.info(\"Finished MSG Downloading Script...\")\n</code></pre>"},{"location":"api/_src/datamodule/datasets/","title":"Datasets","text":""},{"location":"api/_src/datamodule/datasets/#rs_tools._src.datamodule.datasets.GeoDataset","title":"<code>GeoDataset</code>","text":"<p>             Bases: <code>BaseDataset</code></p> Source code in <code>rs_tools/_src/datamodule/datasets.py</code> <pre><code>class GeoDataset(BaseDataset):\n    def __init__(\n        self,\n        data_dir: List[str],\n        editors: List[Editor],\n        splits_dict: Dict,\n        ext: str=\"nc\",\n        limit: int=None,\n        load_coords: bool=True,\n        load_cloudmask: bool=True, \n        **kwargs\n    ):\n        \"\"\"\n        Initialize the GeoDataset class.\n\n        Args:\n            data_dir (List[str]): A list of directories containing the data files.\n            editors (List[Editor]): A list of editors for data preprocessing.\n            splits_dict (Dict, optional): A dictionary specifying the splits for the dataset. Defaults to None.\n            ext (str, optional): The file extension of the data files. Defaults to \"nc\".\n            limit (int, optional): The maximum number of files to load. Defaults to None.\n            load_coords (bool, optional): Whether to load the coordinates. Defaults to True.\n            load_cloudmask (bool, optional): Whether to load the cloud mask. Defaults to True.\n            **kwargs: Additional keyword arguments.\n\n        \"\"\"\n        self.data_dir = data_dir\n        self.editors = editors\n        self.splits_dict = splits_dict\n        self.ext = ext\n        self.limit = limit\n        self.load_coords = load_coords\n        self.load_cloudmask = load_cloudmask\n\n        self.files = self.get_files()\n\n        super().__init__(\n            data=self.files,\n            editors=self.editors,\n            ext=self.ext,\n            limit=self.limit,\n            **kwargs\n        )\n\n    def get_files(self):\n        # Get filenames from data_dir\n        files = get_list_filenames(data_path=self.data_dir, ext=self.ext)\n        # split files based on split criteria\n        files = get_split(files=files, split_dict=self.splits_dict)\n        return files\n\n    def __len__(self):\n        return len(self.files)\n\n    def getIndex(self, data_dict, idx):\n        # Attempt applying editors\n        try:\n            return self.convertData(data_dict)\n        except Exception as ex:\n            logging.error('Unable to convert %s: %s' % (self.files[idx], ex))\n            raise ex\n\n    def __getitem__(self, idx):\n        data_dict = {}\n        # Load dataset\n        ds: xr.Dataset = xr.load_dataset(self.files[idx], engine=\"netcdf4\")\n\n        # Extract data\n        data = ds.Rad.compute().to_numpy()\n        data_dict[\"data\"] = data\n        # Extract wavelengths\n        wavelengths = ds.band_wavelength.compute().to_numpy()\n        data_dict[\"wavelengths\"] = wavelengths\n\n        # Extract coordinates\n        if self.load_coords:\n            latitude = ds.latitude.compute().to_numpy()\n            longitude = ds.longitude.compute().to_numpy()\n            coords = np.stack([latitude, longitude], axis=0)\n            data_dict[\"coords\"] = coords\n\n        # Extract cloud mask\n        if self.load_cloudmask:\n            cloud_mask = ds.cloud_mask.compute().to_numpy()\n            data_dict[\"cloud_mask\"] = cloud_mask\n\n        # Apply editors\n        data, _ = self.getIndex(data_dict, idx)\n        return data\n</code></pre>"},{"location":"api/_src/datamodule/datasets/#rs_tools._src.datamodule.datasets.GeoDataset.__init__","title":"<code>__init__(data_dir, editors, splits_dict, ext='nc', limit=None, load_coords=True, load_cloudmask=True, **kwargs)</code>","text":"<p>Initialize the GeoDataset class.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>List[str]</code> <p>A list of directories containing the data files.</p> required <code>editors</code> <code>List[Editor]</code> <p>A list of editors for data preprocessing.</p> required <code>splits_dict</code> <code>Dict</code> <p>A dictionary specifying the splits for the dataset. Defaults to None.</p> required <code>ext</code> <code>str</code> <p>The file extension of the data files. Defaults to \"nc\".</p> <code>'nc'</code> <code>limit</code> <code>int</code> <p>The maximum number of files to load. Defaults to None.</p> <code>None</code> <code>load_coords</code> <code>bool</code> <p>Whether to load the coordinates. Defaults to True.</p> <code>True</code> <code>load_cloudmask</code> <code>bool</code> <p>Whether to load the cloud mask. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>rs_tools/_src/datamodule/datasets.py</code> <pre><code>def __init__(\n    self,\n    data_dir: List[str],\n    editors: List[Editor],\n    splits_dict: Dict,\n    ext: str=\"nc\",\n    limit: int=None,\n    load_coords: bool=True,\n    load_cloudmask: bool=True, \n    **kwargs\n):\n    \"\"\"\n    Initialize the GeoDataset class.\n\n    Args:\n        data_dir (List[str]): A list of directories containing the data files.\n        editors (List[Editor]): A list of editors for data preprocessing.\n        splits_dict (Dict, optional): A dictionary specifying the splits for the dataset. Defaults to None.\n        ext (str, optional): The file extension of the data files. Defaults to \"nc\".\n        limit (int, optional): The maximum number of files to load. Defaults to None.\n        load_coords (bool, optional): Whether to load the coordinates. Defaults to True.\n        load_cloudmask (bool, optional): Whether to load the cloud mask. Defaults to True.\n        **kwargs: Additional keyword arguments.\n\n    \"\"\"\n    self.data_dir = data_dir\n    self.editors = editors\n    self.splits_dict = splits_dict\n    self.ext = ext\n    self.limit = limit\n    self.load_coords = load_coords\n    self.load_cloudmask = load_cloudmask\n\n    self.files = self.get_files()\n\n    super().__init__(\n        data=self.files,\n        editors=self.editors,\n        ext=self.ext,\n        limit=self.limit,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/_src/datamodule/editor/","title":"Editor","text":""},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.BandOrderEditor","title":"<code>BandOrderEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Reorders bands in data dictionary.</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class BandOrderEditor(Editor):\n    \"\"\"\n    Reorders bands in data dictionary.\n    \"\"\"\n\n    def __init__(self, target_order, key=\"data\"):\n        \"\"\"\n        Args:\n            target_order (list): Order of bands\n            key (str): Key in dictionary to apply transformation\n        \"\"\"\n        self.target_order = target_order\n        self.key = key\n\n    def call(self, data_dict, **kwargs):\n        source_order = data_dict[\"wavelengths\"]\n        assert len(source_order) == len(self.target_order), \"Length of source and target wavelengths must match.\"\n        # Get indexes of bands to select\n        indexes = [np.where(source_order == wvl)[0][0] for wvl in self.target_order]\n        # Extract data\n        data = data_dict[self.key]\n        # Subselect bands\n        data = data[indexes]\n        # Update dictionary\n        data_dict[self.key] = data\n        data_dict[\"wavelengths\"] = np.array(self.target_order)\n        return data_dict\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.BandOrderEditor.__init__","title":"<code>__init__(target_order, key='data')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>target_order</code> <code>list</code> <p>Order of bands</p> required <code>key</code> <code>str</code> <p>Key in dictionary to apply transformation</p> <code>'data'</code> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>def __init__(self, target_order, key=\"data\"):\n    \"\"\"\n    Args:\n        target_order (list): Order of bands\n        key (str): Key in dictionary to apply transformation\n    \"\"\"\n    self.target_order = target_order\n    self.key = key\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.BandSelectionEditor","title":"<code>BandSelectionEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Selects a subset of available bands from data dictionary</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class BandSelectionEditor(Editor):\n    \"\"\"\n    Selects a subset of available bands from data dictionary\n    \"\"\"\n    def __init__(self, target_bands, key=\"data\"):\n        \"\"\"\n        Args:\n            target_bands (list): List of bands to select\n            key (str): Key in dictionary to apply transformation\n        \"\"\"\n        self.target_bands = target_bands\n        self.key = key\n\n    def call(self, data_dict, **kwargs):\n        source_bands = data_dict[\"wavelengths\"]\n        # Get indexes of bands to select\n        indexes = [np.where(source_bands == wvl)[0][0] for wvl in self.target_bands]\n        # Extract data\n        data = data_dict[self.key]\n        # Subselect bands\n        data = data[indexes]\n        assert data.shape[0] == len(self.target_bands)\n        # Update dictionary\n        data_dict[self.key] = data\n        data_dict[\"wavelengths\"] = np.array(self.target_bands)\n        return data_dict\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.BandSelectionEditor.__init__","title":"<code>__init__(target_bands, key='data')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>target_bands</code> <code>list</code> <p>List of bands to select</p> required <code>key</code> <code>str</code> <p>Key in dictionary to apply transformation</p> <code>'data'</code> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>def __init__(self, target_bands, key=\"data\"):\n    \"\"\"\n    Args:\n        target_bands (list): List of bands to select\n        key (str): Key in dictionary to apply transformation\n    \"\"\"\n    self.target_bands = target_bands\n    self.key = key\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.CoordNormEditor","title":"<code>CoordNormEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Normalize latitude and longitude coordinates</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class CoordNormEditor(Editor):\n    \"\"\"\n    Normalize latitude and longitude coordinates\n    \"\"\"\n    def __init__(self, key=\"coords\"):\n        self.key = key\n    def call(self, data_dict, **kwargs):\n        lats, lons = data_dict[\"coords\"]\n        # Normalize latitude and longitude to range [-1, 1]\n        lats = lats/90\n        lons = lons/180\n        # Update dictionary\n        data_dict[\"coords\"] = np.stack([lats, lons], axis=0)\n        return data_dict\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.NanDictEditor","title":"<code>NanDictEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Removes NaN values from data dictionary. Can also be used to replace NaN values of coordinates to remove off limb data.</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class NanDictEditor(Editor):\n    \"\"\"\n    Removes NaN values from data dictionary.\n    Can also be used to replace NaN values of coordinates to remove off limb data.\n    \"\"\"\n    def __init__(self, key=\"data\", fill_value=0):\n        self.key = key\n        self.fill_value = fill_value\n    def call(self, data_dict, **kwargs):\n        data = data_dict[self.key]\n        # Replace NaN values\n        data = np.nan_to_num(data, nan=self.fill_value)\n        # Update dictionary\n        data_dict[self.key] = data\n        return data_dict\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.NanMaskEditor","title":"<code>NanMaskEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Returns mask for NaN values in data dictionary</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class NanMaskEditor(Editor):\n    \"\"\"\n    Returns mask for NaN values in data dictionary\n    \"\"\"\n    def __init__(self, key=\"data\"):\n        self.key = key\n    def call(self, data_dict, **kwargs):\n        data = data_dict[self.key]\n        # Check if any band contains NaN values\n        mask = np.isnan(data).any(axis=0)\n        mask = mask.astype(int)\n        # Update dictionary\n        data_dict[\"nan_mask\"] = mask\n        return data_dict\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.RadUnitEditor","title":"<code>RadUnitEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Convert radiance values from mW/m^2/sr/cm^-1 to W/m^2/sr/um</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class RadUnitEditor(Editor):\n    \"\"\"\n    Convert radiance values from mW/m^2/sr/cm^-1 to W/m^2/sr/um\n    \"\"\"\n    def __init__(self, key=\"data\"):\n        self.key = key\n    def call(self, data_dict, **kwargs):\n        data = data_dict[self.key]\n        wavelengths = data_dict[\"wavelengths\"]\n        # Convert units\n        data = convert_units(data, wavelengths)\n        # Update dictionary\n        data_dict[self.key] = data\n        return data_dict\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.StackDictEditor","title":"<code>StackDictEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Stack data dictionary into a single array</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class StackDictEditor(Editor):\n    \"\"\"\n    Stack data dictionary into a single array\n    \"\"\"\n    def __init__(self, allowed_keys=[\"data\", \"cloud_mask\", \"nan_mask\", \"coords\"], axis=0):\n        self.allowed_keys = allowed_keys\n        self.axis = axis\n    def call(self, data_dict, **kwargs):\n        # Select keys\n        self.keys = [key for key in self.allowed_keys if key in data_dict.keys()]\n        # Select data\n        data = []\n        for key in self.keys:\n            values = data_dict[key]\n            if len(values.shape) == 2:\n                values = np.expand_dims(values, axis=self.axis)\n            data.append(values)\n        # Stack data\n        data = np.concatenate(data, axis=self.axis)\n        # Return numpy array\n        return data\n</code></pre>"},{"location":"api/_src/datamodule/editor/#rs_tools._src.datamodule.editor.ToTensorEditor","title":"<code>ToTensorEditor</code>","text":"<p>             Bases: <code>Editor</code></p> <p>Convert numpy array to PyTorch tensor</p> Source code in <code>rs_tools/_src/datamodule/editor.py</code> <pre><code>class ToTensorEditor(Editor):\n    \"\"\"\n    Convert numpy array to PyTorch tensor\n    \"\"\"\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n    def call(self, data, **kwargs):\n        # Convert to tensor\n        tensor = torch.as_tensor(data, dtype=self.dtype)\n        return tensor\n</code></pre>"},{"location":"api/_src/datamodule/utils/","title":"Utils","text":""},{"location":"api/_src/datamodule/utils/#rs_tools._src.datamodule.utils.get_date_from_file","title":"<code>get_date_from_file(filename)</code>","text":"<p>Extract date from filename.</p> <p>Parameters:</p> Name Type Description Default <code>filenames</code> <code>List[str]</code> <p>A list of filenames.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>List[str]: A list of dates extracted from the filenames.</p> Source code in <code>rs_tools/_src/datamodule/utils.py</code> <pre><code>def get_date_from_file(filename: str) -&gt; datetime:\n    \"\"\"\n    Extract date from filename.\n\n    Args:\n        filenames (List[str]): A list of filenames.\n\n    Returns:\n        List[str]: A list of dates extracted from the filenames.\n    \"\"\"\n    date = datetime.strptime(filename.split(\"_\")[0], \"%Y%m%d%H%M%S\")\n    return date\n</code></pre>"},{"location":"api/_src/datamodule/utils/#rs_tools._src.datamodule.utils.get_dates_from_files","title":"<code>get_dates_from_files(filenames)</code>","text":"<p>Extract dates from a list of filenames.</p> <p>Parameters:</p> Name Type Description Default <code>filenames</code> <code>List[str]</code> <p>A list of filenames.</p> required <p>Returns:</p> Type Description <code>List[datetime]</code> <p>List[str]: A list of dates extracted from the filenames.</p> Source code in <code>rs_tools/_src/datamodule/utils.py</code> <pre><code>def get_dates_from_files(filenames: List[str]) -&gt; List[datetime]:\n    \"\"\"\n    Extract dates from a list of filenames.\n\n    Args:\n        filenames (List[str]): A list of filenames.\n\n    Returns:\n        List[str]: A list of dates extracted from the filenames.\n    \"\"\"\n    dates = [datetime.strptime(filename.split(\"_\")[0], \"%Y%m%d%H%M%S\") for filename in filenames]\n    return dates\n</code></pre>"},{"location":"api/_src/datamodule/utils/#rs_tools._src.datamodule.utils.get_split","title":"<code>get_split(files, split_dict)</code>","text":"<p>Split files based on dataset specification.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List</code> <p>A list of files to be split.</p> required <code>split_dict</code> <code>DictConfig</code> <p>A dictionary-like object containing the dataset specification.</p> required <p>Returns:</p> Type Description <code>Tuple[List, List]</code> <p>Tuple[List, List]: A tuple containing two lists: the training set and the validation set.</p> Source code in <code>rs_tools/_src/datamodule/utils.py</code> <pre><code>def get_split(files: List, \n              split_dict: DictConfig) -&gt; Tuple[List, List]:\n    \"\"\"\n    Split files based on dataset specification.\n\n    Args:\n        files (List): A list of files to be split.\n        split_dict (DictConfig): A dictionary-like object containing the dataset specification.\n\n    Returns:\n        Tuple[List, List]: A tuple containing two lists: the training set and the validation set.\n    \"\"\"\n    # Extract dates from filenames\n    filenames = [file.split(\"/\")[-1] for file in files]\n    dates = get_dates_from_files(filenames)\n    # Convert to dataframe for easier manipulation\n    df = pd.DataFrame({\"filename\": filenames, \"files\": files, \"date\": dates})\n\n    # Check if years, months, and days are specified\n    if \"years\" not in split_dict.keys() or split_dict[\"years\"] is None:\n        logger.info(\"No years specified for split. Using all years.\")\n        split_dict[\"years\"] = df.date.dt.year.unique().tolist()\n    if \"months\" not in split_dict.keys() or split_dict[\"months\"] is None:\n        logger.info(\"No months specified for split. Using all months.\")\n        split_dict[\"months\"] = df.date.dt.month.unique().tolist()\n    if \"days\" not in split_dict.keys() or split_dict[\"days\"] is None:\n        logger.info(\"No days specified for split. Using all days.\")\n        split_dict[\"days\"] = df.date.dt.day.unique().tolist()\n\n    # Determine conditions specified split\n    condition = (df.date.dt.year.isin(split_dict[\"years\"])) &amp; \\\n                (df.date.dt.month.isin(split_dict[\"months\"])) &amp; \\\n                (df.date.dt.day.isin(split_dict[\"days\"]))\n\n    # Extract filenames based on conditions\n    split_files = df[condition].files.tolist()\n\n    # Check if files are allocated properly\n    if len(split_files) == 0:\n        raise ValueError(\"No files found. Check split specification.\")\n\n    return split_files\n</code></pre>"},{"location":"api/_src/datamodule/utils/#rs_tools._src.datamodule.utils.split_train_val","title":"<code>split_train_val(files, split_spec)</code>","text":"<p>Split files into training and validation sets based on dataset specification.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List</code> <p>A list of files to be split.</p> required <code>split_spec</code> <code>DictConfig</code> <p>A dictionary-like object containing the dataset specification.</p> required <p>Returns:</p> Type Description <code>Tuple[List, List]</code> <p>Tuple[List, List]: A tuple containing two lists: the training set and the validation set.</p> Source code in <code>rs_tools/_src/datamodule/utils.py</code> <pre><code>def split_train_val(files: List, split_spec: DictConfig) -&gt; Tuple[List, List]:\n    \"\"\"\n    Split files into training and validation sets based on dataset specification.\n\n    Args:\n        files (List): A list of files to be split.\n        split_spec (DictConfig): A dictionary-like object containing the dataset specification.\n\n    Returns:\n        Tuple[List, List]: A tuple containing two lists: the training set and the validation set.\n    \"\"\"\n    if \"train\" not in split_spec.keys() or \"val\" not in split_spec.keys():\n        raise ValueError(\"split_spec must contain 'train' and 'val' keys\")\n\n    train_files = get_split(files, split_spec[\"train\"])\n    val_files = get_split(files, split_spec[\"val\"])\n\n    return train_files, val_files\n</code></pre>"},{"location":"api/_src/geoprocessing/grid/","title":"Grid","text":""},{"location":"api/_src/geoprocessing/grid/#rs_tools._src.geoprocessing.grid.create_latlon_grid","title":"<code>create_latlon_grid(region, resolution)</code>","text":"<p>Create a latitude-longitude grid within the specified region.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>Tuple[float, float, float, float]</code> <p>The region of interest defined by (lon_min, lat_min, lon_max, lat_max), e.g. (-180, -90, 180, 90) for global grid.</p> required <code>resolution</code> <code>float</code> <p>The resolution of the grid in degrees.</p> required <p>Returns:</p> Type Description <p>Tuple[np.ndarray, np.ndarray]: A tuple containing two numpy arrays representing the latitudes and longitudes of the grid, respectively.</p> Source code in <code>rs_tools/_src/geoprocessing/grid.py</code> <pre><code>def create_latlon_grid(region: Tuple[float, float, float, float],\n                       resolution: float):\n    \"\"\"\n    Create a latitude-longitude grid within the specified region.\n\n    Args:\n        region (Tuple[float, float, float, float]): The region of interest defined by\n            (lon_min, lat_min, lon_max, lat_max), e.g. (-180, -90, 180, 90) for global grid.\n        resolution (float): The resolution of the grid in degrees.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple containing two numpy arrays representing\n            the latitudes and longitudes of the grid, respectively.\n    \"\"\"\n\n    lat_bnds = region[1], region[3]\n    lon_bnds = region[0], region[2]\n    latitudes = np.arange(lat_bnds[0], lat_bnds[1]+resolution, resolution)\n    longitudes = np.arange(lon_bnds[0], lon_bnds[1]+resolution, resolution)\n    return np.meshgrid(latitudes, longitudes)\n</code></pre>"},{"location":"api/_src/geoprocessing/interp/","title":"Interp","text":""},{"location":"api/_src/geoprocessing/interp/#rs_tools._src.geoprocessing.interp.resample_rioxarray","title":"<code>resample_rioxarray(ds, resolution=(1000, 1000), method='bilinear')</code>","text":"<p>Resamples a raster dataset using rasterio-xarray.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset to be resampled.</p> required <code>resolution</code> <code>int</code> <p>The desired resolution of the resampled dataset. Default is 1_000.</p> <code>(1000, 1000)</code> <code>method</code> <code>str</code> <p>The resampling method to be used. Default is \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The resampled dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/interp.py</code> <pre><code>def resample_rioxarray(ds: xr.Dataset, resolution: Tuple[int, int]=(1_000, 1_000), method: str=\"bilinear\") -&gt; xr.Dataset:\n    \"\"\"\n    Resamples a raster dataset using rasterio-xarray.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset to be resampled.\n        resolution (int): The desired resolution of the resampled dataset. Default is 1_000.\n        method (str): The resampling method to be used. Default is \"bilinear\".\n\n    Returns:\n        xr.Dataset: The resampled dataset.\n    \"\"\"\n\n    ds = ds.rio.reproject(\n        ds.rio.crs,\n        resolution=resolution,\n        resample=rioxarray_samplers[method], \n    )\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/match/","title":"Match","text":""},{"location":"api/_src/geoprocessing/match/#rs_tools._src.geoprocessing.match.match_timestamps","title":"<code>match_timestamps(times_data, times_clouds, cutoff=15)</code>","text":"<p>Matches timestamps of data and cloudmask files, if not measured at exactly the same time.</p> <p>Parameters:</p> Name Type Description Default <code>times_data</code> <code>List[str]</code> <p>Timestamps of data files.</p> required <code>times_clouds</code> <code>List[str]</code> <p>Timestamps of the cloud mask files.</p> required <code>cutoff</code> <code>str</code> <p>Maximum time difference in minutes to consider a match. Defaults to 15.</p> <code>15</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with the matched timestamps.</p> Source code in <code>rs_tools/_src/geoprocessing/match.py</code> <pre><code>def match_timestamps(times_data: List[str], times_clouds: List[str], cutoff: int=15) -&gt; pd.DataFrame:\n    \"\"\"\n    Matches timestamps of data and cloudmask files, if not measured at exactly the same time.\n\n    Args:\n        times_data (List[str]): Timestamps of data files.\n        times_clouds (List[str]): Timestamps of the cloud mask files.\n        cutoff (str, optional): Maximum time difference in minutes to consider a match. Defaults to 15.\n\n    Returns:\n        pd.DataFrame: A DataFrame with the matched timestamps.\n    \"\"\"\n    # Convert timestamps to datetime objects\n    timestamps_data = pd.to_datetime(times_data)\n    timestamps_clouds = pd.to_datetime(times_clouds)\n\n    matches_data = []\n    matches_clouds = []\n\n    # Loop through timestamps of data files\n    for time in timestamps_data:\n        # Find closest timestamp in cloud mask files\n        closest_time = timestamps_clouds[\n            np.abs((timestamps_clouds - time).total_seconds()).argmin()\n        ]\n        # Check if the closest timestamp is within the cutoff time\n        if np.abs((closest_time - time).total_seconds()) &lt;= pd.Timedelta(f'{cutoff}min').total_seconds():\n            matches_data.append(time.strftime(\"%Y%m%d%H%M%S\"))\n            matches_clouds.append(closest_time.strftime(\"%Y%m%d%H%M%S\"))\n        else:\n            logger.info(f\"No matching cloud mask found for {time}\")\n\n    matched_times = pd.DataFrame({\n        \"timestamps_data\": matches_data,\n        \"timestamps_cloudmask\": matches_clouds\n    })\n\n    return matched_times\n</code></pre>"},{"location":"api/_src/geoprocessing/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/reproject/#rs_tools._src.geoprocessing.reproject.calc_latlon","title":"<code>calc_latlon(ds)</code>","text":"<p>Calculate the latitude and longitude coordinates for the given dataset</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>Xarray Dataset to calculate the lat/lon coordinates for, with x and y coordinates</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>Xarray Dataset with the latitude and longitude coordinates added</p> Source code in <code>rs_tools/_src/geoprocessing/reproject.py</code> <pre><code>def calc_latlon(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Calculate the latitude and longitude coordinates for the given dataset\n\n    Args:\n        ds: Xarray Dataset to calculate the lat/lon coordinates for, with x and y coordinates\n\n    Returns:\n        Xarray Dataset with the latitude and longitude coordinates added\n    \"\"\"\n    XX, YY = np.meshgrid(ds.x.data, ds.y.data)\n    lons, lats = convert_x_y_to_lat_lon(ds.rio.crs, XX, YY)\n    # Check if lons and lons_trans are close in value\n    # Set inf to NaN values\n    lons[lons == np.inf] = np.nan\n    lats[lats == np.inf] = np.nan\n\n    ds = ds.assign_coords({\"latitude\": ([\"y\", \"x\"], lats), \"longitude\": ([\"y\", \"x\"], lons)})\n    ds.latitude.attrs[\"units\"] = \"degrees_north\"\n    ds.longitude.attrs[\"units\"] = \"degrees_east\"\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/units/","title":"Units","text":""},{"location":"api/_src/geoprocessing/units/#rs_tools._src.geoprocessing.units.convert_units","title":"<code>convert_units(data, wavelengths)</code>","text":"<p>Function to convert units from mW/m^2/sr/cm^-1 to W/m^2/sr/um in numpy array. Acts on each band separately.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array</code> <p>The input data to be converted.</p> required <code>wavelengths</code> <code>array</code> <p>The wavelengths of the input data.</p> required <p>Returns:</p> Type Description <code>array</code> <p>np.array: The converted data.</p> Source code in <code>rs_tools/_src/geoprocessing/units.py</code> <pre><code>def convert_units(data: np.array, wavelengths: np.array) -&gt; np.array:\n    \"\"\"\n    Function to convert units from mW/m^2/sr/cm^-1 to W/m^2/sr/um in numpy array.\n    Acts on each band separately.\n\n    Parameters:\n        data (np.array): The input data to be converted.\n        wavelengths (np.array): The wavelengths of the input data.\n\n    Returns:\n        np.array: The converted data.\n    \"\"\"\n    assert len(data) == len(wavelengths)\n    corrected_data = []\n    for i, wvl in enumerate(wavelengths):\n        corr_data = data[i] * 0.001 # to convert mW to W\n        corr_data = corr_data * 10000 / wvl**2 # to convert cm^-1 to um\n        corrected_data.append(corr_data)\n    return np.stack(corrected_data, axis=0)\n</code></pre>"},{"location":"api/_src/geoprocessing/units/#rs_tools._src.geoprocessing.units.convert_units_ds","title":"<code>convert_units_ds(ds, wavelengths)</code>","text":"<p>Function to convert units from mW/m^2/sr/cm^-1 to W/m^2/sr/um in xarray dataset. Acts on each band separately.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset to be converted.</p> required <code>wavelengths</code> <code>Dict[float]</code> <p>Dictionary of wavelengths of data for each band (i).</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The converted dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/units.py</code> <pre><code>def convert_units_ds(ds: xr.Dataset, wavelengths: Dict) -&gt; xr.Dataset:\n    \"\"\"\n    Function to convert units from mW/m^2/sr/cm^-1 to W/m^2/sr/um in xarray dataset.\n    Acts on each band separately.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset to be converted.\n        wavelengths (Dict[float]): Dictionary of wavelengths of data for each band (i).\n\n    Returns:\n        xr.Dataset: The converted dataset.\n    \"\"\"\n    for band in ds.data_vars:\n        ds[band] = ds[band] * 0.001  # to convert mW to W\n        ds[band] = ds[band] * 10000 / wavelengths[band]**2  # to convert cm^-1 to um\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/utils/","title":"Utils","text":""},{"location":"api/_src/geoprocessing/utils/#rs_tools._src.geoprocessing.utils.check_sat_FOV","title":"<code>check_sat_FOV(region, FOV)</code>","text":"<p>Check if the region is within the Field of View (FOV) of the satellite.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>Tuple[int, int, int, int]</code> <p>The region (lon_min, lat_min, lon_max, lat_max) to check if it is within the FOV</p> required <code>FOV</code> <code>Tuple[int, int]</code> <p>The Field of View (FOV) (lon_min, lat_min, lon_max, lat_max) of the satellite.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the region is within the FOV, False otherwise.</p> Source code in <code>rs_tools/_src/geoprocessing/utils.py</code> <pre><code>def check_sat_FOV(region: Tuple[int, int, int, int], FOV: Tuple[int, int, int, int]) -&gt; bool:\n    \"\"\"\n    Check if the region is within the Field of View (FOV) of the satellite.\n\n    Args:\n        region (Tuple[int, int, int, int]): The region (lon_min, lat_min, lon_max, lat_max) to check if it is within the FOV\n        FOV (Tuple[int, int]): The Field of View (FOV) (lon_min, lat_min, lon_max, lat_max) of the satellite.\n\n    Returns:\n        bool: True if the region is within the FOV, False otherwise.\n    \"\"\"\n    # Check if the region is within the Field of View (FOV) of the satellite.\n    if abs(region[0]) &lt;= abs(FOV[0]) and abs(region[1]) &lt;= abs(FOV[1]) and abs(region[2]) &lt;= abs(FOV[2]) and abs(region[3]) &lt;= abs(FOV[3]):\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/","title":"Geoprocessor Goes16","text":""},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing","title":"<code>GOES16GeoProcessing</code>  <code>dataclass</code>","text":"<p>A class for geoprocessing GOES-16 data.</p> <p>Attributes:</p> Name Type Description <code>resolution</code> <code>float</code> <p>The resolution in meters.</p> <code>read_path</code> <code>str</code> <p>The path to read the files from.</p> <code>save_path</code> <code>str</code> <p>The path to save the processed files to.</p> <code>region</code> <code>Tuple[str]</code> <p>The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).</p> <code>resample_method</code> <code>str</code> <p>The resampling method to use.</p> <p>Methods:</p> Name Description <code>goes_files</code> <p>Returns a list of all GOES files in the read path.</p> <code>preprocess_fn</code> <p>xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.</p> <code>preprocess_fn_radiances</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.</p> <code>preprocess_fn_cloudmask</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.</p> <code>preprocess_files</code> <p>Preprocesses the files in the read path and saves the processed files to the save path.</p> <code>preprocess_radiances</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.</p> <code>preprocess_cloud_mask</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>@dataclass\nclass GOES16GeoProcessing:\n    \"\"\"\n    A class for geoprocessing GOES-16 data.\n\n    Attributes:\n        resolution (float): The resolution in meters.\n        read_path (str): The path to read the files from.\n        save_path (str): The path to save the processed files to.\n        region (Tuple[str]): The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).\n        resample_method (str): The resampling method to use.\n\n    Methods:\n        goes_files(self) -&gt; List[str]: Returns a list of all GOES files in the read path.\n        preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.\n        preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.\n        preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.\n        preprocess_files(self): Preprocesses the files in the read path and saves the processed files to the save path.\n        preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.\n        preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.\n    \"\"\"\n    resolution: float\n    read_path: str\n    save_path: str\n    region: Optional[Tuple[int, int, int, int]]\n    resample_method: str\n\n    @property\n    def goes_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all GOES files in the read path.\n\n        Returns:\n            List[str]: A list of file paths.\n        \"\"\"\n        # get a list of all GOES files from specified path\n        files = get_list_filenames(self.read_path, \".nc\")\n        return files\n\n    def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n        \"\"\"\n        # copy to avoid modifying original dataset\n        ds = ds.copy() \n\n        # convert measurement angles to horizontal distance in meters\n        ds = correct_goes16_satheight(ds) \n        try:\n            # correct band coordinates to reorganize xarray dataset\n            ds = correct_goes16_bands(ds) \n        except AttributeError:\n            pass\n        # assign coordinate reference system\n        ds = add_goes16_crs(ds)\n\n        if self.region is not None:\n            logger.info(f\"Subsetting data to region: {self.region}\")\n            # subset data\n            lon_bnds = (self.region[0], self.region[2])\n            lat_bnds = (self.region[1], self.region[3])\n            # convert lat lon bounds to x y (in meters)\n            x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.FOV.crs, lon=lon_bnds, lat=lat_bnds, )\n            # check that region is within the satellite field of view\n            # compile satellite FOV\n            satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n            # compile region bounds in x y\n            region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n            if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n                raise ValueError(\"Region is not within the satellite field of view\")\n\n            ds = ds.sortby(\"x\").sortby(\"y\")\n            # slice based on x y bounds\n            ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n        else:\n            ds_subset = ds\n\n        if self.resolution is not None:\n            logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n            # resampling\n            ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n        # assign coordinates\n        ds_subset = calc_latlon(ds_subset)\n\n        return ds_subset, ds\n\n    def preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the GOES16 radiance dataset.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        variables = [\"Rad\", \"DQF\"] # \"Rad\" = radiance, \"DQF\" = data quality flag\n\n        # do core preprocess function (e.g. to correct band coordinates, subset data, resample, etc.)\n        ds_subset, ds = self.preprocess_fn(ds)\n\n        # select relevant variables\n        ds_subset = ds_subset[variables]\n        # convert measurement time (in seconds) to datetime\n        time_stamp = pd.to_datetime(ds.t.values) \n        time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\") \n        # assign bands data to each variable\n        ds_subset[variables] = ds_subset[variables].expand_dims({\"band\": ds.band.values})\n        # attach time coordinate\n        ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n        # drop variables that will no longer be needed\n        ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n        # assign band attributes to dataset\n        ds_subset.band.attrs = ds.band.attrs\n        # TODO: Correct wavelength assignment. This attaches 16 wavelengths to each band.\n        # assign band wavelength to each variable\n        ds_subset = ds_subset.assign_coords({\"band_wavelength\": ds.band_wavelength.values})\n        ds_subset.band_wavelength.attrs = ds.band_wavelength.attrs\n\n        return ds_subset\n\n    def preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the input dataset for GOES16 cloud masks.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        variables = [\"BCM\"]\n\n        # do core preprocess function\n        ds_subset, ds = self.preprocess_fn(ds)\n\n        # select relevant variable\n        ds_subset = ds_subset[variables]\n        # convert measurement time (in seconds) to datetime\n        time_stamp = pd.to_datetime(ds.t.values)\n        time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")\n        # assign time data to variable\n        ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n        # drop variables that will no longer be needed\n        ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n\n        return ds_subset\n\n    def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses radiances from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain radiance data\n        files = list(filter(lambda x: \"Rad\" in x, files))\n\n        # Check that all 16 bands are present\n        logger.info(f\"Number of radiance files: {len(files)}\")\n        assert len(files) == 16\n\n        # open multiple files as a single dataset\n        ds = [xr.open_mfdataset(ifile, preprocess=self.preprocess_fn_radiances, concat_dim=\"band\", combine=\"nested\") for\n              ifile in files]\n        # reinterpolate to match coordinates of the first image\n        ds = [ds[0]] + [ids.interp(x=ds[0].x, y=ds[0].y) for ids in ds[1:]]\n        # concatenate in new band dimension\n        ds = xr.concat(ds, dim=\"band\")\n\n        # Correct latitude longitude assignment after multiprocessing\n        ds['latitude'] = ds.latitude.isel(band=0)\n        ds['longitude'] = ds.longitude.isel(band=0)\n\n        # NOTE: Keep only certain relevant attributes\n        attrs_rad = ds[\"Rad\"].attrs\n\n        ds[\"Rad\"].attrs = {}\n        ds[\"Rad\"].attrs = dict(\n            long_name=attrs_rad[\"long_name\"],\n            standard_name=attrs_rad[\"standard_name\"],\n            units=attrs_rad[\"units\"],\n        )\n        ds[\"DQF\"].attrs = {}\n\n        return ds\n\n    def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses cloud mask from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain cloud mask data\n        files = list(filter(lambda x: \"ACMF\" in x, files))\n\n        # Check that only one file is present\n        logger.info(f\"Number of cloud mask files: {len(files)}\")\n        assert len(files) == 1\n\n        # open multiple files as a single dataset\n        ds = xr.open_mfdataset(files[0])\n        ds = self.preprocess_fn_cloudmask(ds)\n\n        # NOTE: Keep only certain relevant attributes\n        attrs_bcm = ds[\"BCM\"].attrs\n        ds = ds.rename({\"BCM\": \"cloud_mask\"})\n        ds[\"cloud_mask\"].attrs = {}\n        ds[\"cloud_mask\"].attrs = dict(\n            long_name=attrs_bcm[\"long_name\"],\n            standard_name=attrs_bcm[\"standard_name\"],\n            units=attrs_bcm[\"units\"],\n        )\n\n        return ds\n\n    def preprocess_files(self):\n        \"\"\"\n        Preprocesses multiple files in read path and saves processed files to save path.\n        \"\"\"\n        # get unique times from read path\n        unique_times = list(set(map(parse_goes16_dates_from_file, self.goes_files)))\n\n        pbar_time = tqdm(unique_times)\n\n        for itime in pbar_time:\n\n            pbar_time.set_description(f\"Processing: {itime}\")\n\n            # get files from unique times\n            files = list(filter(lambda x: itime in x, self.goes_files))\n\n            try:\n                # load radiances\n                ds = self.preprocess_radiances(files)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing bands\")\n                continue\n            try:\n                # load cloud mask\n                ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing cloud mask\")\n                continue\n            pbar_time.set_description(f\"Loaded data...\")\n            # interpolate cloud mask to data\n            # fill in zeros for all nan values\n            ds_clouds = ds_clouds.fillna(0)\n            # NOTE: Interpolation changes values from integers to floats\n            # NOTE: This is fixed through rounding \n            ds_clouds = ds_clouds.interp(x=ds.x, y=ds.y)\n            ds_clouds = ds_clouds.round()\n\n            # save cloud mask as data coordinate\n            ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values.squeeze())})\n            ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            # remove file if it already exists\n            itime_name = format_goes_dates(itime)\n            save_filename = Path(self.save_path).joinpath(f\"{itime_name}_goes16.nc\")\n            if os.path.exists(save_filename):\n                logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n                os.remove(save_filename)\n            # save to netcdf\n            pbar_time.set_description(f\"Saving to file...:{save_filename}\")\n            # TODO: Add \"metrics\" for printing (e.g., filesize)\n            ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.goes_files","title":"<code>goes_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all GOES files in the read path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file paths.</p>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_cloud_mask","title":"<code>preprocess_cloud_mask(files)</code>","text":"<p>Preprocesses cloud mask from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses cloud mask from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain cloud mask data\n    files = list(filter(lambda x: \"ACMF\" in x, files))\n\n    # Check that only one file is present\n    logger.info(f\"Number of cloud mask files: {len(files)}\")\n    assert len(files) == 1\n\n    # open multiple files as a single dataset\n    ds = xr.open_mfdataset(files[0])\n    ds = self.preprocess_fn_cloudmask(ds)\n\n    # NOTE: Keep only certain relevant attributes\n    attrs_bcm = ds[\"BCM\"].attrs\n    ds = ds.rename({\"BCM\": \"cloud_mask\"})\n    ds[\"cloud_mask\"].attrs = {}\n    ds[\"cloud_mask\"].attrs = dict(\n        long_name=attrs_bcm[\"long_name\"],\n        standard_name=attrs_bcm[\"standard_name\"],\n        units=attrs_bcm[\"units\"],\n    )\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_files","title":"<code>preprocess_files()</code>","text":"<p>Preprocesses multiple files in read path and saves processed files to save path.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_files(self):\n    \"\"\"\n    Preprocesses multiple files in read path and saves processed files to save path.\n    \"\"\"\n    # get unique times from read path\n    unique_times = list(set(map(parse_goes16_dates_from_file, self.goes_files)))\n\n    pbar_time = tqdm(unique_times)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Processing: {itime}\")\n\n        # get files from unique times\n        files = list(filter(lambda x: itime in x, self.goes_files))\n\n        try:\n            # load radiances\n            ds = self.preprocess_radiances(files)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing bands\")\n            continue\n        try:\n            # load cloud mask\n            ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing cloud mask\")\n            continue\n        pbar_time.set_description(f\"Loaded data...\")\n        # interpolate cloud mask to data\n        # fill in zeros for all nan values\n        ds_clouds = ds_clouds.fillna(0)\n        # NOTE: Interpolation changes values from integers to floats\n        # NOTE: This is fixed through rounding \n        ds_clouds = ds_clouds.interp(x=ds.x, y=ds.y)\n        ds_clouds = ds_clouds.round()\n\n        # save cloud mask as data coordinate\n        ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values.squeeze())})\n        ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        # remove file if it already exists\n        itime_name = format_goes_dates(itime)\n        save_filename = Path(self.save_path).joinpath(f\"{itime_name}_goes16.nc\")\n        if os.path.exists(save_filename):\n            logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n            os.remove(save_filename)\n        # save to netcdf\n        pbar_time.set_description(f\"Saving to file...:{save_filename}\")\n        # TODO: Add \"metrics\" for printing (e.g., filesize)\n        ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_fn","title":"<code>preprocess_fn(ds)</code>","text":"<p>Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[Dataset, Dataset]</code> <p>Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n    \"\"\"\n    # copy to avoid modifying original dataset\n    ds = ds.copy() \n\n    # convert measurement angles to horizontal distance in meters\n    ds = correct_goes16_satheight(ds) \n    try:\n        # correct band coordinates to reorganize xarray dataset\n        ds = correct_goes16_bands(ds) \n    except AttributeError:\n        pass\n    # assign coordinate reference system\n    ds = add_goes16_crs(ds)\n\n    if self.region is not None:\n        logger.info(f\"Subsetting data to region: {self.region}\")\n        # subset data\n        lon_bnds = (self.region[0], self.region[2])\n        lat_bnds = (self.region[1], self.region[3])\n        # convert lat lon bounds to x y (in meters)\n        x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.FOV.crs, lon=lon_bnds, lat=lat_bnds, )\n        # check that region is within the satellite field of view\n        # compile satellite FOV\n        satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n        # compile region bounds in x y\n        region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n        if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n            raise ValueError(\"Region is not within the satellite field of view\")\n\n        ds = ds.sortby(\"x\").sortby(\"y\")\n        # slice based on x y bounds\n        ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n    else:\n        ds_subset = ds\n\n    if self.resolution is not None:\n        logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n        # resampling\n        ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n    # assign coordinates\n    ds_subset = calc_latlon(ds_subset)\n\n    return ds_subset, ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_fn_cloudmask","title":"<code>preprocess_fn_cloudmask(ds)</code>","text":"<p>Preprocesses the input dataset for GOES16 cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the input dataset for GOES16 cloud masks.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    variables = [\"BCM\"]\n\n    # do core preprocess function\n    ds_subset, ds = self.preprocess_fn(ds)\n\n    # select relevant variable\n    ds_subset = ds_subset[variables]\n    # convert measurement time (in seconds) to datetime\n    time_stamp = pd.to_datetime(ds.t.values)\n    time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")\n    # assign time data to variable\n    ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n    # drop variables that will no longer be needed\n    ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n\n    return ds_subset\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_fn_radiances","title":"<code>preprocess_fn_radiances(ds)</code>","text":"<p>Preprocesses the GOES16 radiance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the GOES16 radiance dataset.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    variables = [\"Rad\", \"DQF\"] # \"Rad\" = radiance, \"DQF\" = data quality flag\n\n    # do core preprocess function (e.g. to correct band coordinates, subset data, resample, etc.)\n    ds_subset, ds = self.preprocess_fn(ds)\n\n    # select relevant variables\n    ds_subset = ds_subset[variables]\n    # convert measurement time (in seconds) to datetime\n    time_stamp = pd.to_datetime(ds.t.values) \n    time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\") \n    # assign bands data to each variable\n    ds_subset[variables] = ds_subset[variables].expand_dims({\"band\": ds.band.values})\n    # attach time coordinate\n    ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n    # drop variables that will no longer be needed\n    ds_subset = ds_subset.drop_vars([\"t\", \"y_image\", \"x_image\", \"goes_imager_projection\"])\n    # assign band attributes to dataset\n    ds_subset.band.attrs = ds.band.attrs\n    # TODO: Correct wavelength assignment. This attaches 16 wavelengths to each band.\n    # assign band wavelength to each variable\n    ds_subset = ds_subset.assign_coords({\"band_wavelength\": ds.band_wavelength.values})\n    ds_subset.band_wavelength.attrs = ds.band_wavelength.attrs\n\n    return ds_subset\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.GOES16GeoProcessing.preprocess_radiances","title":"<code>preprocess_radiances(files)</code>","text":"<p>Preprocesses radiances from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses radiances from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain radiance data\n    files = list(filter(lambda x: \"Rad\" in x, files))\n\n    # Check that all 16 bands are present\n    logger.info(f\"Number of radiance files: {len(files)}\")\n    assert len(files) == 16\n\n    # open multiple files as a single dataset\n    ds = [xr.open_mfdataset(ifile, preprocess=self.preprocess_fn_radiances, concat_dim=\"band\", combine=\"nested\") for\n          ifile in files]\n    # reinterpolate to match coordinates of the first image\n    ds = [ds[0]] + [ids.interp(x=ds[0].x, y=ds[0].y) for ids in ds[1:]]\n    # concatenate in new band dimension\n    ds = xr.concat(ds, dim=\"band\")\n\n    # Correct latitude longitude assignment after multiprocessing\n    ds['latitude'] = ds.latitude.isel(band=0)\n    ds['longitude'] = ds.longitude.isel(band=0)\n\n    # NOTE: Keep only certain relevant attributes\n    attrs_rad = ds[\"Rad\"].attrs\n\n    ds[\"Rad\"].attrs = {}\n    ds[\"Rad\"].attrs = dict(\n        long_name=attrs_rad[\"long_name\"],\n        standard_name=attrs_rad[\"standard_name\"],\n        units=attrs_rad[\"units\"],\n    )\n    ds[\"DQF\"].attrs = {}\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/geoprocessor_goes16/#rs_tools._src.geoprocessing.goes.geoprocessor_goes16.geoprocess","title":"<code>geoprocess(resolution=None, read_path='./', save_path='./', region=None, resample_method='bilinear')</code>","text":"<p>Geoprocesses GOES 16 files</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>float</code> <p>The resolution in meters to resample data to. Defaults to None.</p> <code>None</code> <code>read_path</code> <code>str</code> <p>The path to read the files from. Defaults to \"./\".</p> <code>'./'</code> <code>save_path</code> <code>str</code> <p>The path to save the geoprocessed files to. Defaults to \"./\".</p> <code>'./'</code> <code>region</code> <code>str</code> <p>The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.</p> <code>None</code> <code>resample_method</code> <code>str</code> <p>The resampling method to use. Defaults to \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/geoprocessing/goes/geoprocessor_goes16.py</code> <pre><code>def geoprocess(\n        resolution: float = None, #\u00a0defined in meters\n        read_path: str = \"./\",\n        save_path: str = \"./\",\n        region: str = None,\n        resample_method: str = \"bilinear\",\n):\n    \"\"\"\n    Geoprocesses GOES 16 files\n\n    Args:\n        resolution (float, optional): The resolution in meters to resample data to. Defaults to None.\n        read_path (str, optional): The path to read the files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the geoprocessed files to. Defaults to \"./\".\n        region (str, optional): The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.\n        resample_method (str, optional): The resampling method to use. Defaults to \"bilinear\".\n\n    Returns:\n        None\n    \"\"\"\n    print(\"PATH!!!:\", read_path)\n    # Initialize GOES 16 GeoProcessor\n    logger.info(f\"Initializing GOES16 GeoProcessor...\")\n    # Extracting region from str\n    if region is not None:\n        region = tuple(map(lambda x: int(x), region.split(\" \")))\n\n    goes16_geoprocessor = GOES16GeoProcessing(\n        resolution=resolution, \n        read_path=read_path, \n        save_path=save_path,\n        region=region,\n        resample_method=resample_method\n        )\n    logger.info(f\"GeoProcessing Files...\")\n    goes16_geoprocessor.preprocess_files()\n\n    logger.info(f\"Finished GOES 16 GeoProcessing Script...!\")\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/interp/","title":"Interp","text":""},{"location":"api/_src/geoprocessing/goes/interp/#rs_tools._src.geoprocessing.goes.interp.resample_rioxarray","title":"<code>resample_rioxarray(ds, resolution=1000, method='bilinear')</code>","text":"<p>Resamples a raster dataset using rasterio-xarray.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset to be resampled.</p> required <code>resolution</code> <code>int</code> <p>The desired resolution of the resampled dataset. Default is 1_000.</p> <code>1000</code> <code>method</code> <code>str</code> <p>The resampling method to be used. Default is \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The resampled dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/interp.py</code> <pre><code>def resample_rioxarray(ds: xr.Dataset, resolution: int=1_000, method: str=\"bilinear\") -&gt; xr.Dataset:\n    \"\"\"\n    Resamples a raster dataset using rasterio-xarray.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset to be resampled.\n        resolution (int): The desired resolution of the resampled dataset. Default is 1_000.\n        method (str): The resampling method to be used. Default is \"bilinear\".\n\n    Returns:\n        xr.Dataset: The resampled dataset.\n    \"\"\"\n\n    ds = ds.rio.reproject(\n        ds.rio.crs,\n        resolution=resolution,\n        resample=rioxarray_samplers[method], \n    )\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/goes/reproject/#rs_tools._src.geoprocessing.goes.reproject.add_goes16_crs","title":"<code>add_goes16_crs(ds)</code>","text":"<p>Adds the Coordinate Reference System (CRS) to the given GOES16 dataset.</p> <p>Parameters: - ds (xarray.Dataset): The dataset to which the CRS will be added.</p> <p>Returns: - xarray.Dataset: The dataset with the CRS added.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/reproject.py</code> <pre><code>def add_goes16_crs(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Adds the Coordinate Reference System (CRS) to the given GOES16 dataset.\n\n    Parameters:\n    - ds (xarray.Dataset): The dataset to which the CRS will be added.\n\n    Returns:\n    - xarray.Dataset: The dataset with the CRS added.\n    \"\"\"\n\n    # load CRS\n    cc = CRS.from_cf(ds.goes_imager_projection.attrs)\n\n    # assign CRS to dataarray\n    ds.rio.write_crs(cc.to_string(), inplace=True)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/validation/","title":"Validation","text":""},{"location":"api/_src/geoprocessing/goes/validation/#rs_tools._src.geoprocessing.goes.validation.correct_goes16_bands","title":"<code>correct_goes16_bands(ds)</code>","text":"<p>Corrects the band coordinates in a GOES-16 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset containing GOES-16 bands.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The corrected dataset with updated band coordinates.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/validation.py</code> <pre><code>def correct_goes16_bands(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Corrects the band coordinates in a GOES-16 dataset.\n\n    Parameters:\n        ds (xr.Dataset): The input dataset containing GOES-16 bands.\n\n    Returns:\n        xr.Dataset: The corrected dataset with updated band coordinates.\n\n    \"\"\"\n    # reassign coordinate\n    band_id_attrs = ds.band_id.attrs\n    ds = ds.assign_coords(band=ds.band_id.values)\n    ds.band.attrs = band_id_attrs\n\n    # drop bandid dims\n    ds = ds.drop_vars(\"band_id\")\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/goes/validation/#rs_tools._src.geoprocessing.goes.validation.correct_goes16_satheight","title":"<code>correct_goes16_satheight(ds)</code>","text":"<p>Convert measurement angle of GOES-16 satellite data to horizontal distance (in meters).</p> <p>Parameters: - ds (xr.Dataset): The input dataset containing the GOES-16 satellite data.</p> <p>Returns: - xr.Dataset: The dataset with corrected perspective height.</p> Source code in <code>rs_tools/_src/geoprocessing/goes/validation.py</code> <pre><code>def correct_goes16_satheight(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Convert measurement angle of GOES-16 satellite data to horizontal distance (in meters).\n\n    Parameters:\n    - ds (xr.Dataset): The input dataset containing the GOES-16 satellite data.\n\n    Returns:\n    - xr.Dataset: The dataset with corrected perspective height.\n    \"\"\"\n\n    # get perspective height\n    sat_height = ds.goes_imager_projection.attrs[\"perspective_point_height\"]\n\n    # reassign coordinates to correct height\n    x_attrs = ds.x.attrs\n    ds = ds.assign_coords({\"x\": ds.x.values * sat_height})\n    ds[\"x\"].attrs = x_attrs\n    ds[\"x\"].attrs[\"units\"] = \"meters\"\n\n    y_attrs = ds.y.attrs\n    ds = ds.assign_coords({\"y\": ds.y.values * sat_height})\n    ds[\"y\"].attrs = y_attrs\n    ds[\"y\"].attrs[\"units\"] = \"meters\"\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/","title":"Geoprocessor Modis","text":""},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing","title":"<code>MODISGeoProcessing</code>  <code>dataclass</code>","text":"Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>@dataclass\nclass MODISGeoProcessing:\n    satellite: str\n    read_path: str\n    save_path: str\n    \"\"\"\n    A class for geoprocessing MODIS data.\n\n    Attributes:\n        satellite (str): The satellite to geoprocess data for.\n        read_path (str): The path to read the files from.\n        save_path (str): The path to save the processed files to.\n\n    Methods:\n        modis_files(self) -&gt; List[str]: Returns a list of all MODIS files in the read path.\n        preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections.\n        preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.\n        preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.\n        preprocess_files(self): Preprocesses the files in the read path and saves the processed files to the save path.\n        preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.\n        preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.\n    \"\"\"\n    @property\n    def modis_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all MODIS files in the read path.\n\n        Returns:\n            List[str]: A list of file paths.\n        \"\"\"\n        # get a list of all MODIS filenames within the path\n        files = get_list_filenames(self.read_path, \".hdf\")\n        return files\n\n    def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Preprocesses the input dataset by applying corrections etc.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            ds (xr.Dataset):: The preprocessed dataset.\n        \"\"\"\n        # copy to avoid modifying original dataset\n        ds = ds.copy() \n\n        # assign coordinate reference system\n        ds = add_modis_crs(ds)\n\n        # TODO: Add functionality to resample data to specific resolution\n\n        return ds\n\n    def preprocess_fn_radiances(self, file: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the MODIS radiance dataset.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Load file using satpy scenes\n        scn = Scene(\n            reader=\"modis_l1b\",\n            filenames=file\n        )\n        # Load radiance bands\n        channels = get_modis_channel_numbers()\n        scn.load(channels, generate=False, calibration='radiance')\n\n        # change to xarray data\n        ds = scn.to_xarray_dataset()  \n\n        # do core preprocess function (e.g. resample, add crs etc.)\n        ds = self.preprocess_fn(ds) \n\n        # Store the attributes in a dict before concatenation\n        attrs_dict = {x: ds[x].attrs for x in channels}\n\n        # concatinate in new band dimension, and defining a new variable name\n        # NOTE: Concatination overwrites attrs of bands.\n        ds = ds.assign(Rad=xr.concat(list(map(lambda x: ds[x], channels)), dim=\"band\"))\n        # drop duplicate variables\n        ds = ds.drop(list(map(lambda x: x, channels)))\n        # rename band dimensions\n        ds = ds.assign_coords(band=list(map(lambda x: x, channels)))\n\n        # convert measurement time (in seconds) to datetime\n        time_stamp = pd.to_datetime(ds.attrs['start_time'])\n        time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")  \n        # assign bands and time data to each variable\n        ds = ds.assign_coords({\"time\": [time_stamp]})\n\n        # NOTE: Keep only certain relevant attributes\n        ds.attrs = {}\n        ds.attrs = dict(\n            calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n            standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n            platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n            sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n            units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n        )\n\n        # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n        # assign band wavelengths \n        ds = ds.assign_coords({\"band_wavelength\": list(MODIS_WAVELENGTHS.values())})   \n\n        return ds\n\n    def preprocess_fn_cloudmask(self, file: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the input dataset for MODIS cloud masks.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"    \n        # Load file using satpy scenes\n        scn = Scene(\n            reader=\"modis_l2\",\n            filenames=file\n        )\n        # Load cloud mask data\n        datasets = scn.available_dataset_names()\n        # Needs to be loaded at 1000 m resolution for all channels to match\n        scn.load(datasets, generate=False, resolution=1000) \n\n        # change to xarray data\n        ds = scn.to_xarray_dataset()\n\n        return ds\n\n    def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses radiances from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        identifier = MODIS_NAME_TO_ID[self.satellite]\n\n        # Check that all files contain radiance data\n        file = list(filter(lambda x: identifier in x, files))\n\n        # Check that only one file is selected\n        logger.info(f\"Number of radiance files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_radiances(file)\n\n        return ds\n\n\n    def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses cloud mask from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        identifier = MODIS_NAME_TO_ID[f'{self.satellite}_cloud']\n\n        # Check that all files contain radiance data\n        file = list(filter(lambda x: identifier in x, files))\n\n        # Check that only one file is selected\n        logger.info(f\"Number of cloud mask files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_cloudmask(file)\n\n        return ds\n\n\n    def preprocess_files(self):\n        \"\"\"\n        Preprocesses multiple files in read path and saves processed files to save path.\n        \"\"\"\n        # get unique times from read path\n        unique_times = list(set(map(parse_modis_dates_from_file, self.modis_files)))\n\n        pbar_time = tqdm(unique_times)\n\n        for itime in pbar_time:\n\n            pbar_time.set_description(f\"Processing: {itime}\")\n\n            # get files from unique times\n            files = list(filter(lambda x: itime in x, self.modis_files))\n\n            try:\n                # load radiances\n                ds = self.preprocess_radiances(files)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to error loading\")\n                continue\n            try:\n                # load cloud mask\n                ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing cloud mask\")\n                continue\n\n            # save cloud mask as data coordinate\n            ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values)})\n            # add cloud mask attrs to dataset\n            ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n            # remove crs from dataset\n            ds = ds.drop_vars(\"crs\")\n            ds = ds.drop_vars(\"spatial_ref\")\n\n            # remove attrs that cause netcdf error\n            for attr in [\"start_time\", \"end_time\", \"area\", \"_satpy_id\"]:\n                ds[\"cloud_mask\"].attrs.pop(attr)\n\n            for var in ds.data_vars:\n                ds[var].attrs = {}\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            # remove file if it already exists\n            itime_name = format_modis_dates(itime)\n            save_filename = Path(self.save_path).joinpath(f\"{itime_name}_{self.satellite}.nc\")\n            if os.path.exists(save_filename):\n                logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n                os.remove(save_filename)\n\n            # save to netcdf\n            ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.modis_files","title":"<code>modis_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all MODIS files in the read path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file paths.</p>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.save_path","title":"<code>save_path: str</code>  <code>instance-attribute</code>","text":"<p>A class for geoprocessing MODIS data.</p> <p>Attributes:</p> Name Type Description <code>satellite</code> <code>str</code> <p>The satellite to geoprocess data for.</p> <code>read_path</code> <code>str</code> <p>The path to read the files from.</p> <code>save_path</code> <code>str</code> <p>The path to save the processed files to.</p> <p>Functions:</p> Name Description <code>modis_files</code> <p>Returns a list of all MODIS files in the read path.</p> <code>preprocess_fn</code> <p>xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections.</p> <code>preprocess_fn_radiances</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.</p> <code>preprocess_fn_cloudmask</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.</p> <code>preprocess_files</code> <p>Preprocesses the files in the read path and saves the processed files to the save path.</p> <code>preprocess_radiances</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.</p> <code>preprocess_cloud_mask</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.</p>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_cloud_mask","title":"<code>preprocess_cloud_mask(files)</code>","text":"<p>Preprocesses cloud mask from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses cloud mask from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    identifier = MODIS_NAME_TO_ID[f'{self.satellite}_cloud']\n\n    # Check that all files contain radiance data\n    file = list(filter(lambda x: identifier in x, files))\n\n    # Check that only one file is selected\n    logger.info(f\"Number of cloud mask files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_cloudmask(file)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_files","title":"<code>preprocess_files()</code>","text":"<p>Preprocesses multiple files in read path and saves processed files to save path.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_files(self):\n    \"\"\"\n    Preprocesses multiple files in read path and saves processed files to save path.\n    \"\"\"\n    # get unique times from read path\n    unique_times = list(set(map(parse_modis_dates_from_file, self.modis_files)))\n\n    pbar_time = tqdm(unique_times)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Processing: {itime}\")\n\n        # get files from unique times\n        files = list(filter(lambda x: itime in x, self.modis_files))\n\n        try:\n            # load radiances\n            ds = self.preprocess_radiances(files)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to error loading\")\n            continue\n        try:\n            # load cloud mask\n            ds_clouds = self.preprocess_cloud_mask(files)[\"cloud_mask\"]\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing cloud mask\")\n            continue\n\n        # save cloud mask as data coordinate\n        ds = ds.assign_coords({\"cloud_mask\": ((\"y\", \"x\"), ds_clouds.values)})\n        # add cloud mask attrs to dataset\n        ds[\"cloud_mask\"].attrs = ds_clouds.attrs\n\n        # remove crs from dataset\n        ds = ds.drop_vars(\"crs\")\n        ds = ds.drop_vars(\"spatial_ref\")\n\n        # remove attrs that cause netcdf error\n        for attr in [\"start_time\", \"end_time\", \"area\", \"_satpy_id\"]:\n            ds[\"cloud_mask\"].attrs.pop(attr)\n\n        for var in ds.data_vars:\n            ds[var].attrs = {}\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        # remove file if it already exists\n        itime_name = format_modis_dates(itime)\n        save_filename = Path(self.save_path).joinpath(f\"{itime_name}_{self.satellite}.nc\")\n        if os.path.exists(save_filename):\n            logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n            os.remove(save_filename)\n\n        # save to netcdf\n        ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_fn","title":"<code>preprocess_fn(ds)</code>","text":"<p>Preprocesses the input dataset by applying corrections etc.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Name Type Description <code>ds</code> <code>Dataset</code> <p>: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Preprocesses the input dataset by applying corrections etc.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        ds (xr.Dataset):: The preprocessed dataset.\n    \"\"\"\n    # copy to avoid modifying original dataset\n    ds = ds.copy() \n\n    # assign coordinate reference system\n    ds = add_modis_crs(ds)\n\n    # TODO: Add functionality to resample data to specific resolution\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_fn_cloudmask","title":"<code>preprocess_fn_cloudmask(file)</code>","text":"<p>Preprocesses the input dataset for MODIS cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_fn_cloudmask(self, file: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the input dataset for MODIS cloud masks.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"    \n    # Load file using satpy scenes\n    scn = Scene(\n        reader=\"modis_l2\",\n        filenames=file\n    )\n    # Load cloud mask data\n    datasets = scn.available_dataset_names()\n    # Needs to be loaded at 1000 m resolution for all channels to match\n    scn.load(datasets, generate=False, resolution=1000) \n\n    # change to xarray data\n    ds = scn.to_xarray_dataset()\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_fn_radiances","title":"<code>preprocess_fn_radiances(file)</code>","text":"<p>Preprocesses the MODIS radiance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_fn_radiances(self, file: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the MODIS radiance dataset.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Load file using satpy scenes\n    scn = Scene(\n        reader=\"modis_l1b\",\n        filenames=file\n    )\n    # Load radiance bands\n    channels = get_modis_channel_numbers()\n    scn.load(channels, generate=False, calibration='radiance')\n\n    # change to xarray data\n    ds = scn.to_xarray_dataset()  \n\n    # do core preprocess function (e.g. resample, add crs etc.)\n    ds = self.preprocess_fn(ds) \n\n    # Store the attributes in a dict before concatenation\n    attrs_dict = {x: ds[x].attrs for x in channels}\n\n    # concatinate in new band dimension, and defining a new variable name\n    # NOTE: Concatination overwrites attrs of bands.\n    ds = ds.assign(Rad=xr.concat(list(map(lambda x: ds[x], channels)), dim=\"band\"))\n    # drop duplicate variables\n    ds = ds.drop(list(map(lambda x: x, channels)))\n    # rename band dimensions\n    ds = ds.assign_coords(band=list(map(lambda x: x, channels)))\n\n    # convert measurement time (in seconds) to datetime\n    time_stamp = pd.to_datetime(ds.attrs['start_time'])\n    time_stamp = time_stamp.strftime(\"%Y-%m-%d %H:%M\")  \n    # assign bands and time data to each variable\n    ds = ds.assign_coords({\"time\": [time_stamp]})\n\n    # NOTE: Keep only certain relevant attributes\n    ds.attrs = {}\n    ds.attrs = dict(\n        calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n        standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n        platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n        sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n        units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n    )\n\n    # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n    # assign band wavelengths \n    ds = ds.assign_coords({\"band_wavelength\": list(MODIS_WAVELENGTHS.values())})   \n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.MODISGeoProcessing.preprocess_radiances","title":"<code>preprocess_radiances(files)</code>","text":"<p>Preprocesses radiances from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses radiances from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    identifier = MODIS_NAME_TO_ID[self.satellite]\n\n    # Check that all files contain radiance data\n    file = list(filter(lambda x: identifier in x, files))\n\n    # Check that only one file is selected\n    logger.info(f\"Number of radiance files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_radiances(file)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/geoprocessor_modis/#rs_tools._src.geoprocessing.modis.geoprocessor_modis.geoprocess","title":"<code>geoprocess(satellite, read_path='./', save_path='./')</code>","text":"<p>Geoprocesses MODIS files</p> <p>Parameters:</p> Name Type Description Default <code>satellite</code> <code>str</code> <p>The satellite of the data to geoprocess.</p> required <code>read_path</code> <code>str</code> <p>The path to read the files from. Defaults to \"./\".</p> <code>'./'</code> <code>save_path</code> <code>str</code> <p>The path to save the geoprocessed files to. Defaults to \"./\".</p> <code>'./'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/geoprocessing/modis/geoprocessor_modis.py</code> <pre><code>def geoprocess(\n        satellite: str,\n        read_path: str = \"./\",\n        save_path: str = \"./\"\n):\n    \"\"\"\n    Geoprocesses MODIS files\n\n    Args:\n        satellite (str, optional): The satellite of the data to geoprocess.\n        read_path (str, optional): The path to read the files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the geoprocessed files to. Defaults to \"./\".\n\n    Returns:\n        None\n    \"\"\"\n    # Initialize MODIS GeoProcessor\n    logger.info(f\"Initializing {satellite.upper()} GeoProcessor...\")\n\n    modis_geoprocessor = MODISGeoProcessing(\n        satellite=satellite, \n        read_path=read_path, \n        save_path=save_path\n        )\n    logger.info(f\"GeoProcessing Files...\")\n    modis_geoprocessor.preprocess_files()\n\n    logger.info(f\"Finished {satellite.upper()} GeoProcessing Script...!\")\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/interp/","title":"Interp","text":""},{"location":"api/_src/geoprocessing/modis/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/modis/reproject/#rs_tools._src.geoprocessing.modis.reproject.add_modis_crs","title":"<code>add_modis_crs(ds)</code>","text":"<p>Adds the Coordinate Reference System (CRS) to the given MODIS dataset.</p> <p>Parameters: - ds (xarray.Dataset): The dataset to which the CRS will be added.</p> <p>Returns: - xarray.Dataset: The dataset with the CRS added.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/reproject.py</code> <pre><code>def add_modis_crs(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Adds the Coordinate Reference System (CRS) to the given MODIS dataset.\n\n    Parameters:\n    - ds (xarray.Dataset): The dataset to which the CRS will be added.\n\n    Returns:\n    - xarray.Dataset: The dataset with the CRS added.\n    \"\"\"\n    # define CRS of MODIS dataset\n    crs = 'WGS84'\n\n    # load source CRS from the WKT string\n    cc = CRS(crs)\n\n    # assign CRS to dataarray\n    ds.rio.write_crs(cc, inplace=True)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/rescale/","title":"Rescale","text":""},{"location":"api/_src/geoprocessing/modis/rescale/#rs_tools._src.geoprocessing.modis.rescale.convert_integers2radiances","title":"<code>convert_integers2radiances(da)</code>","text":"<p>Function to convert scaled integers to radiances. Scaled integers are unitless, radiances are given in W^2/m^2/um/sr</p> Source code in <code>rs_tools/_src/geoprocessing/modis/rescale.py</code> <pre><code>def convert_integers2radiances(\n        da: xr.DataArray, \n    ) -&gt; xr.DataArray:\n    \"\"\"\n    Function to convert scaled integers to radiances.\n    Scaled integers are unitless, radiances are given in W^2/m^2/um/sr\n    \"\"\"\n\n    radiance_scale = da.radiance_scales\n    radiance_offsets = da.radiance_offsets \n\n    radiance_scale = np.expand_dims(radiance_scale, axis=(1,2))\n    radiance_offsets = np.expand_dims(radiance_offsets, axis=(1,2))\n\n    assert radiance_offsets.shape == radiance_scale.shape\n\n    corrected_data = (da - radiance_offsets)*radiance_scale\n\n    #TODO - change attributes to have units\n    #TODO - change attributes to change name of variable\n    #TODO - change attributes to change rescaling\n\n    return corrected_data\n</code></pre>"},{"location":"api/_src/geoprocessing/modis/rescale/#rs_tools._src.geoprocessing.modis.rescale.convert_integers2reflectances","title":"<code>convert_integers2reflectances(da)</code>","text":"<p>Function to convert scaled integers to reflectances. Scaled integers and reflectances are both unitless.</p> Source code in <code>rs_tools/_src/geoprocessing/modis/rescale.py</code> <pre><code>def convert_integers2reflectances(\n        da: xr.DataArray, \n    ) -&gt; xr.DataArray:\n    \"\"\"\n    Function to convert scaled integers to reflectances.\n    Scaled integers and reflectances are both unitless.\n    \"\"\"\n\n    reflectance_scale = da.reflectance_scales\n    reflectance_offsets = da.reflectance_offsets \n\n    reflectance_scale = np.expand_dims(reflectance_scale, axis=(1,2))\n    reflectance_offsets = np.expand_dims(reflectance_offsets, axis=(1,2))\n\n    assert reflectance_offsets.shape == reflectance_scale.shape\n\n    corrected_data = (da - reflectance_offsets)*reflectance_scale\n\n    return corrected_data\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/","title":"Geoprocessor Msg","text":""},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing","title":"<code>MSGGeoProcessing</code>  <code>dataclass</code>","text":"<p>A class for geoprocessing MSG data.</p> <p>Attributes:</p> Name Type Description <code>resolution</code> <code>float</code> <p>The resolution in meters.</p> <code>read_path</code> <code>str</code> <p>The path to read the files from.</p> <code>save_path</code> <code>str</code> <p>The path to save the processed files to.</p> <code>region</code> <code>Tuple[str]</code> <p>The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).</p> <code>resample_method</code> <code>str</code> <p>The resampling method to use.</p> <p>Methods:</p> Name Description <code>msg_files</code> <p>Returns a list of all MSG files in the read path.</p> <code>preprocess_fn</code> <p>xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.</p> <code>preprocess_fn_radiances</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.</p> <code>preprocess_fn_cloudmask</code> <p>xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.</p> <code>preprocess_files</code> <p>Preprocesses the files in the read path and saves the processed files to the save path.</p> <code>preprocess_radiances</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.</p> <code>preprocess_cloud_mask</code> <p>List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>@dataclass\nclass MSGGeoProcessing:\n    \"\"\"\n    A class for geoprocessing MSG data.\n\n    Attributes:\n        resolution (float): The resolution in meters.\n        read_path (str): The path to read the files from.\n        save_path (str): The path to save the processed files to.\n        region (Tuple[str]): The region of interest defined by the bounding box coordinates (lon_min, lat_min, lon_max, lat_max).\n        resample_method (str): The resampling method to use.\n\n    Methods:\n        msg_files(self) -&gt; List[str]: Returns a list of all MSG files in the read path.\n        preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]: Preprocesses the input dataset by applying corrections, subsetting, and resampling.\n        preprocess_fn_radiances(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for radiances.\n        preprocess_fn_cloudmask(self, ds: xr.Dataset) -&gt; xr.Dataset: Preprocesses the input dataset for cloud mask.\n        preprocess_files(self): Preprocesses the files in the read path and saves the processed files to the save path.\n        preprocess_radiances(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the radiances from the input files.\n        preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset: Preprocesses the cloud mask from the input files.\n    \"\"\"\n    resolution: float\n    read_path: str\n    save_path: str\n    region: Optional[Tuple[int, int, int, int]]\n    resample_method: str\n\n    @property\n    def msg_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all MSG files in the read path.\n\n        Returns:\n            List[str]: A list of file paths.\n        \"\"\"\n        # get a list of all MSG radiance files from specified path\n        files_radiances = get_list_filenames(self.read_path, \".nat\")\n        # get a list of all MSG cloud mask files from specified path\n        files_cloudmask = get_list_filenames(self.read_path, \".grb\")\n        return files_radiances, files_cloudmask\n\n    def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n        \"\"\"\n        Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n        Args:\n            ds (xr.Dataset): The input dataset.\n\n        Returns:\n            Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n        \"\"\"\n        # copy to avoid modifying original dataset\n        ds = ds.copy() \n\n        # assign coordinate reference system\n        ds = add_msg_crs(ds)\n\n        if self.region is not None:\n            logger.info(f\"Subsetting data to region: {self.region}\")\n            # subset data\n            lon_bnds = (self.region[0], self.region[2])\n            lat_bnds = (self.region[1], self.region[3])\n            # convert lat lon bounds to x y (in meters)\n            x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.rio.crs, lon=lon_bnds, lat=lat_bnds, )\n            # check that region is within the satellite field of view\n            # compile satellite FOV\n            satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n            # compile region bounds in x y\n            region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n            if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n                raise ValueError(\"Region is not within the satellite field of view\")\n\n            ds = ds.sortby(\"x\").sortby(\"y\")\n            # slice based on x y bounds\n            ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n        else:\n            ds_subset = ds\n\n        if self.resolution is not None:\n            logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n            # resampling\n            ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n        # assign coordinates\n        ds_subset = calc_latlon(ds_subset)\n\n        return ds_subset, ds\n\n    def preprocess_fn_radiances(self, file: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses the MSG radiance dataset.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n\n        # Load file using satpy scenes\n        scn = Scene(\n            reader=\"seviri_l1b_native\",\n            filenames=file\n        )\n        # Load radiance bands\n        channels = [x for x in scn.available_dataset_names() if x!='HRV']\n        assert len(channels) == 11, \"Number of channels is not 11\"\n\n        scn.load(channels, generate=False, calibration='radiance')\n\n        # change to xarray data\n        ds = scn.to_xarray()\n\n        # attach cloud mask as data variable before preprocessing\n        ds = ds.assign(cloud_mask=((\"y\", \"x\"), cloud_mask))\n\n        # reset coordinates for resampling/reprojecting\n        # this drops all {channel}_acq_time coordinates\n        ds = ds.reset_coords(drop=True)\n\n        # do core preprocess function (e.g. resample, add crs etc.)\n        ds_subset, ds = self.preprocess_fn(ds) \n\n        # Store the attributes in a dict before concatenation\n        attrs_dict = {x: ds_subset[x].attrs for x in channels}\n\n        # concatinate in new band dimension\n        # NOTE: Concatination overwrites attrs of bands.\n        ds_subset = ds_subset.assign(Rad=xr.concat(list(map(lambda x: ds_subset[x], channels)), dim=\"band\"))\n        # rename band dimensions\n        ds_subset = ds_subset.assign_coords(band=list(map(lambda x: x, channels)))\n\n        # re-index coordinates\n        ds_subset = ds_subset.set_coords(['latitude', 'longitude', 'cloud_mask'])\n\n        # drop variables that will no longer be needed\n        ds_subset = ds_subset.drop(list(map(lambda x: x, channels)))\n\n        # extract measurement time\n        time_stamp = attrs_dict[list(attrs_dict.keys())[0]]['start_time']\n        # assign bands and time data to each variable\n        ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n\n        # NOTE: Keep only certain relevant attributes\n        ds_subset.attrs = {}\n        ds_subset.attrs = dict(\n            calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n            standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n            platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n            sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n            units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n            orbital_parameters=attrs_dict[list(attrs_dict.keys())[0]][\"orbital_parameters\"]\n        )\n\n        # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n        # assign band wavelengths \n        ds_subset = ds_subset.assign_coords({\"band_wavelength\": list(MSG_WAVELENGTHS.values())}) \n\n        return ds_subset\n\n    def preprocess_fn_cloudmask(self, file: List[str]) -&gt; np.array:\n        \"\"\"\n        Preprocesses the input dataset for MSG cloud masks.\n\n        Args:\n            file (List[str]): The input file.\n\n        Returns:\n            np.array: The preprocessed cloud mask dataset.\n        \"\"\"\n\n        grbs = pygrib.open(file[0])\n        # Loop over all messages in the GRIB file\n        for grb in grbs:\n            if grb.name == 'Cloud mask':\n                # Extract values from grb and return np.array\n                cloud_mask = grb.values\n                return cloud_mask\n\n    def preprocess_radiances(self, files: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses radiances from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain radiance data\n        file = list(filter(lambda x: \".nat\" in x, files))\n\n        # Check that only one file is selected\n        logger.info(f\"Number of radiance files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_radiances(file, cloud_mask=cloud_mask)\n\n        return ds\n\n    def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n        \"\"\"\n        Preprocesses cloud mask from the input files.\n\n        Args:\n            files (List[str]): The list of file paths.\n\n        Returns:\n            xr.Dataset: The preprocessed dataset.\n        \"\"\"\n        # Check that all files contain cloud mask data\n        file = list(filter(lambda x: \"CLMK\" in x, files))\n\n        # Check that only one file is present\n        logger.info(f\"Number of cloud mask files: {len(file)}\")\n        assert len(file) == 1\n\n        # load file using satpy, convert to xarray dataset, and preprocess\n        ds = self.preprocess_fn_cloudmask(file)\n\n        return ds\n\n    def preprocess_files(self):\n        \"\"\"\n        Preprocesses multiple files in read path and saves processed files to save path.\n        \"\"\"\n        # get unique times from read path\n        files_radiances, files_cloudmask = self.msg_files\n        unique_times_radiances = list(set(map(parse_msg_dates_from_file, files_radiances)))\n        unique_times_cloudmask = list(set(map(parse_msg_dates_from_file, files_cloudmask)))\n\n        df_matches = match_timestamps(unique_times_radiances, unique_times_cloudmask, cutoff=15) \n\n        pbar_time = tqdm(df_matches[\"timestamps_data\"].values)\n\n        for itime in pbar_time:\n\n            pbar_time.set_description(f\"Processing: {itime}\")\n\n            # get cloud mask file for specific time\n            itime_cloud = df_matches.loc[df_matches[\"timestamps_data\"] == itime, \"timestamps_cloudmask\"].values[0]\n            files_cloud = list(filter(lambda x: itime_cloud in x, files_cloudmask))\n\n            try:\n                # load cloud mask\n                cloud_mask = self.preprocess_cloud_mask(files_cloud)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to missing cloud mask\")\n                continue\n\n            # get data files for specific time\n            files = list(filter(lambda x: itime in x, files_radiances))\n\n            try:\n                # load radiances and attach cloud mask\n                ds = self.preprocess_radiances(files, cloud_mask=cloud_mask)\n            except AssertionError:\n                logger.error(f\"Skipping {itime} due to error loading\")\n                continue\n\n             # remove crs from dataset\n            ds = ds.drop_vars('msg_seviri_fes_3km') \n\n            # remove attrs that cause netcdf error\n            for var in ds.data_vars:\n                ds[var].attrs.pop('grid_mapping', None)\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            # remove file if it already exists\n            save_filename = Path(self.save_path).joinpath(f\"{itime}_msg.nc\")\n            if os.path.exists(save_filename):\n                logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n                os.remove(save_filename)\n\n            # save to netcdf\n            ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.msg_files","title":"<code>msg_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all MSG files in the read path.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file paths.</p>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_cloud_mask","title":"<code>preprocess_cloud_mask(files)</code>","text":"<p>Preprocesses cloud mask from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_cloud_mask(self, files: List[str]) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses cloud mask from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain cloud mask data\n    file = list(filter(lambda x: \"CLMK\" in x, files))\n\n    # Check that only one file is present\n    logger.info(f\"Number of cloud mask files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_cloudmask(file)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_files","title":"<code>preprocess_files()</code>","text":"<p>Preprocesses multiple files in read path and saves processed files to save path.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_files(self):\n    \"\"\"\n    Preprocesses multiple files in read path and saves processed files to save path.\n    \"\"\"\n    # get unique times from read path\n    files_radiances, files_cloudmask = self.msg_files\n    unique_times_radiances = list(set(map(parse_msg_dates_from_file, files_radiances)))\n    unique_times_cloudmask = list(set(map(parse_msg_dates_from_file, files_cloudmask)))\n\n    df_matches = match_timestamps(unique_times_radiances, unique_times_cloudmask, cutoff=15) \n\n    pbar_time = tqdm(df_matches[\"timestamps_data\"].values)\n\n    for itime in pbar_time:\n\n        pbar_time.set_description(f\"Processing: {itime}\")\n\n        # get cloud mask file for specific time\n        itime_cloud = df_matches.loc[df_matches[\"timestamps_data\"] == itime, \"timestamps_cloudmask\"].values[0]\n        files_cloud = list(filter(lambda x: itime_cloud in x, files_cloudmask))\n\n        try:\n            # load cloud mask\n            cloud_mask = self.preprocess_cloud_mask(files_cloud)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to missing cloud mask\")\n            continue\n\n        # get data files for specific time\n        files = list(filter(lambda x: itime in x, files_radiances))\n\n        try:\n            # load radiances and attach cloud mask\n            ds = self.preprocess_radiances(files, cloud_mask=cloud_mask)\n        except AssertionError:\n            logger.error(f\"Skipping {itime} due to error loading\")\n            continue\n\n         # remove crs from dataset\n        ds = ds.drop_vars('msg_seviri_fes_3km') \n\n        # remove attrs that cause netcdf error\n        for var in ds.data_vars:\n            ds[var].attrs.pop('grid_mapping', None)\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        # remove file if it already exists\n        save_filename = Path(self.save_path).joinpath(f\"{itime}_msg.nc\")\n        if os.path.exists(save_filename):\n            logger.info(f\"File already exists. Overwriting file: {save_filename}\")\n            os.remove(save_filename)\n\n        # save to netcdf\n        ds.to_netcdf(save_filename, engine=\"netcdf4\")\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_fn","title":"<code>preprocess_fn(ds)</code>","text":"<p>Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>Dataset</code> <p>The input dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[Dataset, Dataset]</code> <p>Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_fn(self, ds: xr.Dataset) -&gt; Tuple[xr.Dataset, xr.Dataset]:\n    \"\"\"\n    Preprocesses the input dataset by applying corrections, subsetting, and resampling etc.\n\n    Args:\n        ds (xr.Dataset): The input dataset.\n\n    Returns:\n        Tuple[xr.Dataset, xr.Dataset]: The preprocessed dataset and the original dataset.\n    \"\"\"\n    # copy to avoid modifying original dataset\n    ds = ds.copy() \n\n    # assign coordinate reference system\n    ds = add_msg_crs(ds)\n\n    if self.region is not None:\n        logger.info(f\"Subsetting data to region: {self.region}\")\n        # subset data\n        lon_bnds = (self.region[0], self.region[2])\n        lat_bnds = (self.region[1], self.region[3])\n        # convert lat lon bounds to x y (in meters)\n        x_bnds, y_bnds = convert_lat_lon_to_x_y(ds.rio.crs, lon=lon_bnds, lat=lat_bnds, )\n        # check that region is within the satellite field of view\n        # compile satellite FOV\n        satellite_FOV = (min(ds.x.values), min(ds.y.values), max(ds.x.values), max(ds.y.values))\n        # compile region bounds in x y\n        region_xy = (x_bnds[0], y_bnds[0], x_bnds[1], y_bnds[1])\n        if not check_sat_FOV(region_xy, FOV=satellite_FOV):\n            raise ValueError(\"Region is not within the satellite field of view\")\n\n        ds = ds.sortby(\"x\").sortby(\"y\")\n        # slice based on x y bounds\n        ds_subset = ds.sel(y=slice(y_bnds[0], y_bnds[1]), x=slice(x_bnds[0], x_bnds[1]))\n    else:\n        ds_subset = ds\n\n    if self.resolution is not None:\n        logger.info(f\"Resampling data to resolution: {self.resolution} m\")\n        # resampling\n        ds_subset = resample_rioxarray(ds_subset, resolution=(self.resolution, self.resolution), method=self.resample_method)\n\n    # assign coordinates\n    ds_subset = calc_latlon(ds_subset)\n\n    return ds_subset, ds\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_fn_cloudmask","title":"<code>preprocess_fn_cloudmask(file)</code>","text":"<p>Preprocesses the input dataset for MSG cloud masks.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>array</code> <p>np.array: The preprocessed cloud mask dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_fn_cloudmask(self, file: List[str]) -&gt; np.array:\n    \"\"\"\n    Preprocesses the input dataset for MSG cloud masks.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        np.array: The preprocessed cloud mask dataset.\n    \"\"\"\n\n    grbs = pygrib.open(file[0])\n    # Loop over all messages in the GRIB file\n    for grb in grbs:\n        if grb.name == 'Cloud mask':\n            # Extract values from grb and return np.array\n            cloud_mask = grb.values\n            return cloud_mask\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_fn_radiances","title":"<code>preprocess_fn_radiances(file, cloud_mask)</code>","text":"<p>Preprocesses the MSG radiance dataset.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>List[str]</code> <p>The input file.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_fn_radiances(self, file: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses the MSG radiance dataset.\n\n    Args:\n        file (List[str]): The input file.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n\n    # Load file using satpy scenes\n    scn = Scene(\n        reader=\"seviri_l1b_native\",\n        filenames=file\n    )\n    # Load radiance bands\n    channels = [x for x in scn.available_dataset_names() if x!='HRV']\n    assert len(channels) == 11, \"Number of channels is not 11\"\n\n    scn.load(channels, generate=False, calibration='radiance')\n\n    # change to xarray data\n    ds = scn.to_xarray()\n\n    # attach cloud mask as data variable before preprocessing\n    ds = ds.assign(cloud_mask=((\"y\", \"x\"), cloud_mask))\n\n    # reset coordinates for resampling/reprojecting\n    # this drops all {channel}_acq_time coordinates\n    ds = ds.reset_coords(drop=True)\n\n    # do core preprocess function (e.g. resample, add crs etc.)\n    ds_subset, ds = self.preprocess_fn(ds) \n\n    # Store the attributes in a dict before concatenation\n    attrs_dict = {x: ds_subset[x].attrs for x in channels}\n\n    # concatinate in new band dimension\n    # NOTE: Concatination overwrites attrs of bands.\n    ds_subset = ds_subset.assign(Rad=xr.concat(list(map(lambda x: ds_subset[x], channels)), dim=\"band\"))\n    # rename band dimensions\n    ds_subset = ds_subset.assign_coords(band=list(map(lambda x: x, channels)))\n\n    # re-index coordinates\n    ds_subset = ds_subset.set_coords(['latitude', 'longitude', 'cloud_mask'])\n\n    # drop variables that will no longer be needed\n    ds_subset = ds_subset.drop(list(map(lambda x: x, channels)))\n\n    # extract measurement time\n    time_stamp = attrs_dict[list(attrs_dict.keys())[0]]['start_time']\n    # assign bands and time data to each variable\n    ds_subset = ds_subset.assign_coords({\"time\": [time_stamp]})\n\n    # NOTE: Keep only certain relevant attributes\n    ds_subset.attrs = {}\n    ds_subset.attrs = dict(\n        calibration=attrs_dict[list(attrs_dict.keys())[0]][\"calibration\"],\n        standard_name=attrs_dict[list(attrs_dict.keys())[0]][\"standard_name\"],\n        platform_name=attrs_dict[list(attrs_dict.keys())[0]][\"platform_name\"],\n        sensor=attrs_dict[list(attrs_dict.keys())[0]][\"sensor\"],\n        units=attrs_dict[list(attrs_dict.keys())[0]][\"units\"],\n        orbital_parameters=attrs_dict[list(attrs_dict.keys())[0]][\"orbital_parameters\"]\n    )\n\n    # TODO: Correct wavelength assignment. This attaches 36++ wavelengths to each band.\n    # assign band wavelengths \n    ds_subset = ds_subset.assign_coords({\"band_wavelength\": list(MSG_WAVELENGTHS.values())}) \n\n    return ds_subset\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.MSGGeoProcessing.preprocess_radiances","title":"<code>preprocess_radiances(files, cloud_mask)</code>","text":"<p>Preprocesses radiances from the input files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>The list of file paths.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>xr.Dataset: The preprocessed dataset.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def preprocess_radiances(self, files: List[str], cloud_mask: np.array) -&gt; xr.Dataset:\n    \"\"\"\n    Preprocesses radiances from the input files.\n\n    Args:\n        files (List[str]): The list of file paths.\n\n    Returns:\n        xr.Dataset: The preprocessed dataset.\n    \"\"\"\n    # Check that all files contain radiance data\n    file = list(filter(lambda x: \".nat\" in x, files))\n\n    # Check that only one file is selected\n    logger.info(f\"Number of radiance files: {len(file)}\")\n    assert len(file) == 1\n\n    # load file using satpy, convert to xarray dataset, and preprocess\n    ds = self.preprocess_fn_radiances(file, cloud_mask=cloud_mask)\n\n    return ds\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.geoprocess","title":"<code>geoprocess(resolution=None, read_path='./', save_path='./', region=None, resample_method='bilinear')</code>","text":"<p>Geoprocesses MSG files</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>float</code> <p>The resolution in meters to resample data to. Defaults to None.</p> <code>None</code> <code>read_path</code> <code>str</code> <p>The path to read the files from. Defaults to \"./\".</p> <code>'./'</code> <code>save_path</code> <code>str</code> <p>The path to save the geoprocessed files to. Defaults to \"./\".</p> <code>'./'</code> <code>region</code> <code>str</code> <p>The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.</p> <code>None</code> <code>resample_method</code> <code>str</code> <p>The resampling method to use. Defaults to \"bilinear\".</p> <code>'bilinear'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def geoprocess(\n        resolution: float = None, #\u00a0defined in meters\n        read_path: str = \"./\",\n        save_path: str = \"./\",\n        region: str = None,\n        resample_method: str = \"bilinear\",\n):\n    \"\"\"\n    Geoprocesses MSG files\n\n    Args:\n        resolution (float, optional): The resolution in meters to resample data to. Defaults to None.\n        read_path (str, optional): The path to read the files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the geoprocessed files to. Defaults to \"./\".\n        region (str, optional): The geographic region to extract (\"lon_min, lat_min, lon_max, lat_max\"). Defaults to None.\n        resample_method (str, optional): The resampling method to use. Defaults to \"bilinear\".\n\n    Returns:\n        None\n    \"\"\"\n    # Initialize MSG GeoProcessor\n    logger.info(f\"Initializing MSG GeoProcessor...\")\n    # Extracting region from str\n    if region is not None:\n        region = tuple(map(lambda x: int(x), region.split(\" \")))\n\n    msg_geoprocessor = MSGGeoProcessing(\n        resolution=resolution, \n        read_path=read_path, \n        save_path=save_path,\n        region=region,\n        resample_method=resample_method\n        )\n    logger.info(f\"GeoProcessing Files...\")\n    msg_geoprocessor.preprocess_files()\n\n    logger.info(f\"Finished MSG GeoProcessing Script...!\")\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/geoprocessor_msg/#rs_tools._src.geoprocessing.msg.geoprocessor_msg.parse_msg_dates_from_file","title":"<code>parse_msg_dates_from_file(file)</code>","text":"<p>Parses the date and time information from a MSG file name.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The file name to parse.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The parsed date and time in the format 'YYYYJJJHHMM'.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/geoprocessor_msg.py</code> <pre><code>def parse_msg_dates_from_file(file: str):\n    \"\"\"\n    Parses the date and time information from a MSG file name.\n\n    Args:\n        file (str): The file name to parse.\n\n    Returns:\n        str: The parsed date and time in the format 'YYYYJJJHHMM'.\n    \"\"\"\n    timestamp = Path(file).name.split(\"-\")[-2]\n    timestamp = timestamp.split(\".\")[0]\n    return timestamp\n</code></pre>"},{"location":"api/_src/geoprocessing/msg/reproject/","title":"Reproject","text":""},{"location":"api/_src/geoprocessing/msg/reproject/#rs_tools._src.geoprocessing.msg.reproject.add_msg_crs","title":"<code>add_msg_crs(ds)</code>","text":"<p>Adds the Coordinate Reference System (CRS) to the given MSG dataset.</p> <p>Parameters: - ds (xarray.Dataset): The dataset to which the CRS will be added.</p> <p>Returns: - xarray.Dataset: The dataset with the CRS added.</p> Source code in <code>rs_tools/_src/geoprocessing/msg/reproject.py</code> <pre><code>def add_msg_crs(ds: xr.Dataset) -&gt; xr.Dataset:\n    \"\"\"\n    Adds the Coordinate Reference System (CRS) to the given MSG dataset.\n\n    Parameters:\n    - ds (xarray.Dataset): The dataset to which the CRS will be added.\n\n    Returns:\n    - xarray.Dataset: The dataset with the CRS added.\n    \"\"\"\n    # the CRS is stored in data variable attributes of \"msg_seviri_fes_3km\"\n    var = \"msg_seviri_fes_3km\"\n    crs_wkt = ds[var].crs_wkt\n\n    # load source CRS from the WKT string\n    cc = CRS(crs_wkt)\n\n    # assign CRS to dataarray\n    ds.rio.write_crs(cc, inplace=True)\n\n    return ds\n</code></pre>"},{"location":"api/_src/preprocessing/normalize/","title":"Normalize","text":""},{"location":"api/_src/preprocessing/prepatcher/","title":"Prepatcher","text":""},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.PrePatcher","title":"<code>PrePatcher</code>  <code>dataclass</code>","text":"<p>A class for preprocessing and saving patches from NetCDF files.</p> <p>Attributes:</p> Name Type Description <code>read_path</code> <code>str</code> <p>The path to the directory containing the NetCDF files.</p> <code>save_path</code> <code>str</code> <p>The path to save the patches.</p> <code>patch_size</code> <code>int</code> <p>The size of each patch.</p> <code>stride_size</code> <code>int</code> <p>The stride size for generating patches.</p> <code>nan_cutoff</code> <code>float</code> <p>The cutoff value for allowed NaN count in a patch.</p> <code>save_filetype</code> <code>str</code> <p>The file type to save patches as. Options are [nc, np].</p> <p>Methods:</p> Name Description <code>nc_files</code> <p>Returns a list of all NetCDF filenames in the read_path directory.</p> <code>save_patches</code> <p>Preprocesses and saves patches from the NetCDF files.</p> Source code in <code>rs_tools/_src/preprocessing/prepatcher.py</code> <pre><code>@dataclass(frozen=True)\nclass PrePatcher:\n    \"\"\"\n    A class for preprocessing and saving patches from NetCDF files.\n\n    Attributes:\n        read_path (str): The path to the directory containing the NetCDF files.\n        save_path (str): The path to save the patches.\n        patch_size (int): The size of each patch.\n        stride_size (int): The stride size for generating patches.\n        nan_cutoff (float): The cutoff value for allowed NaN count in a patch.\n        save_filetype (str): The file type to save patches as. Options are [nc, np].\n\n    Methods:\n        nc_files(self) -&gt; List[str]: Returns a list of all NetCDF filenames in the read_path directory.\n        save_patches(self): Preprocesses and saves patches from the NetCDF files.\n    \"\"\"\n\n    read_path: str\n    save_path: str \n    patch_size: int\n    stride_size: int \n    nan_cutoff: float\n    save_filetype: str\n\n    @property\n    def nc_files(self) -&gt; List[str]:\n        \"\"\"\n        Returns a list of all NetCDF filenames in the read_path directory.\n\n        Returns:\n            List[str]: A list of NetCDF filenames.\n        \"\"\"\n        # get list of all filenames within the path\n        files = get_list_filenames(self.read_path, \".nc\")\n        return files\n\n    def save_patches(self):\n        \"\"\"\n        Preprocesses and saves patches from the NetCDF files.\n        \"\"\"\n        pbar = tqdm(self.nc_files)\n\n        for ifile in pbar:\n            # extract &amp; log timestamp\n            itime = str(Path(ifile).name).split(\"_\")[0]\n            pbar.set_description(f\"Processing: {itime}\")\n            # open dataset\n            ds = xr.open_dataset(ifile, engine=\"netcdf4\")\n            # extract radiance data array\n            da = ds.Rad\n            # define patch parameters\n            patches = dict(x=self.patch_size, y=self.patch_size)\n            strides = dict(x=self.stride_size, y=self.stride_size)\n            # start patching\n            patcher = XRDAPatcher(da=da, patches=patches, strides=strides)\n\n            # check if save path exists, and create if not\n            if not os.path.exists(self.save_path):\n                os.makedirs(self.save_path)\n\n            for i, ipatch in tqdm(enumerate(patcher), total=len(patcher)):\n                data = ipatch.data # extract data patch\n                if _check_nan_count(data, self.nan_cutoff):\n                    if self.save_filetype == \"nc\":\n                        # reconvert to dataset to attach band_wavelength and time\n                        ipatch = ipatch.to_dataset(name='Rad')\n                        ipatch = ipatch.assign_coords({'time': ds.time.values})\n                        ipatch = ipatch.assign_coords({'band_wavelength': ds.band_wavelength.values})\n                        # compile filename\n                        file_path = Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\")\n                        # remove file if it already exists\n                        if os.path.exists(file_path):\n                            os.remove(file_path)\n                        # save patch to netcdf                  \n                        ipatch.to_netcdf(Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\"), engine=\"netcdf4\")\n                    elif self.save_filetype == \"np\":\n                        # save as numpy files\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_radiance_patch_{i}\"), data)\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_latitude_patch_{i}\"), ipatch.latitude.values)\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_longitude_patch_{i}\"), ipatch.longitude.values)\n                        np.save(Path(self.save_path).joinpath(f\"{itime}_cloudmask_patch_{i}\"), ipatch.cloud_mask.values)\n                else:\n                    logger.info(f'NaN count exceeded for patch {i} of timestamp {itime}.')\n</code></pre>"},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.PrePatcher.nc_files","title":"<code>nc_files: List[str]</code>  <code>property</code>","text":"<p>Returns a list of all NetCDF filenames in the read_path directory.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of NetCDF filenames.</p>"},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.PrePatcher.save_patches","title":"<code>save_patches()</code>","text":"<p>Preprocesses and saves patches from the NetCDF files.</p> Source code in <code>rs_tools/_src/preprocessing/prepatcher.py</code> <pre><code>def save_patches(self):\n    \"\"\"\n    Preprocesses and saves patches from the NetCDF files.\n    \"\"\"\n    pbar = tqdm(self.nc_files)\n\n    for ifile in pbar:\n        # extract &amp; log timestamp\n        itime = str(Path(ifile).name).split(\"_\")[0]\n        pbar.set_description(f\"Processing: {itime}\")\n        # open dataset\n        ds = xr.open_dataset(ifile, engine=\"netcdf4\")\n        # extract radiance data array\n        da = ds.Rad\n        # define patch parameters\n        patches = dict(x=self.patch_size, y=self.patch_size)\n        strides = dict(x=self.stride_size, y=self.stride_size)\n        # start patching\n        patcher = XRDAPatcher(da=da, patches=patches, strides=strides)\n\n        # check if save path exists, and create if not\n        if not os.path.exists(self.save_path):\n            os.makedirs(self.save_path)\n\n        for i, ipatch in tqdm(enumerate(patcher), total=len(patcher)):\n            data = ipatch.data # extract data patch\n            if _check_nan_count(data, self.nan_cutoff):\n                if self.save_filetype == \"nc\":\n                    # reconvert to dataset to attach band_wavelength and time\n                    ipatch = ipatch.to_dataset(name='Rad')\n                    ipatch = ipatch.assign_coords({'time': ds.time.values})\n                    ipatch = ipatch.assign_coords({'band_wavelength': ds.band_wavelength.values})\n                    # compile filename\n                    file_path = Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\")\n                    # remove file if it already exists\n                    if os.path.exists(file_path):\n                        os.remove(file_path)\n                    # save patch to netcdf                  \n                    ipatch.to_netcdf(Path(self.save_path).joinpath(f\"{itime}_patch_{i}.nc\"), engine=\"netcdf4\")\n                elif self.save_filetype == \"np\":\n                    # save as numpy files\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_radiance_patch_{i}\"), data)\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_latitude_patch_{i}\"), ipatch.latitude.values)\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_longitude_patch_{i}\"), ipatch.longitude.values)\n                    np.save(Path(self.save_path).joinpath(f\"{itime}_cloudmask_patch_{i}\"), ipatch.cloud_mask.values)\n            else:\n                logger.info(f'NaN count exceeded for patch {i} of timestamp {itime}.')\n</code></pre>"},{"location":"api/_src/preprocessing/prepatcher/#rs_tools._src.preprocessing.prepatcher.prepatch","title":"<code>prepatch(read_path='./', save_path='./', patch_size=256, stride_size=256, nan_cutoff=0.5, save_filetype='nc')</code>","text":"<p>Patches satellite data into smaller patches for training. Args:     read_path (str, optional): The path to read the input files from. Defaults to \"./\".     save_path (str, optional): The path to save the extracted patches. Defaults to \"./\".     patch_size (int, optional): The size of each patch. Defaults to 256.     stride_size (int, optional): The stride size for patch extraction. Defaults to 256.     nan_cutoff (float): The cutoff value for allowed NaN count in a patch. Defaults to 0.1.     save_filetype (str, optional): The file type to save patches as. Options are [nc, np]</p> <p>Returns:</p> Type Description <p>None</p> Source code in <code>rs_tools/_src/preprocessing/prepatcher.py</code> <pre><code>def prepatch(\n        read_path: str = \"./\",\n        save_path: str = \"./\",\n        patch_size: int = 256,\n        stride_size: int = 256,\n        nan_cutoff: float = 0.5, \n        save_filetype: str = 'nc'\n):\n    \"\"\"\n    Patches satellite data into smaller patches for training.\n    Args:\n        read_path (str, optional): The path to read the input files from. Defaults to \"./\".\n        save_path (str, optional): The path to save the extracted patches. Defaults to \"./\".\n        patch_size (int, optional): The size of each patch. Defaults to 256.\n        stride_size (int, optional): The stride size for patch extraction. Defaults to 256.\n        nan_cutoff (float): The cutoff value for allowed NaN count in a patch. Defaults to 0.1.\n        save_filetype (str, optional): The file type to save patches as. Options are [nc, np]\n\n    Returns:\n        None\n    \"\"\"\n    _check_filetype(file_type=save_filetype)\n\n    # Initialize Prepatcher\n    logger.info(f\"Initializing Prepatcher...\")\n    prepatcher = PrePatcher(\n        read_path=read_path, \n        save_path=save_path,\n        patch_size=patch_size,\n        stride_size=stride_size,\n        nan_cutoff=nan_cutoff,\n        save_filetype=save_filetype\n        )\n    logger.info(f\"Patching Files...: {save_path}\")\n    prepatcher.save_patches()\n\n    logger.info(f\"Finished Prepatching Script...!\")\n</code></pre>"},{"location":"api/_src/utils/io/","title":"Io","text":""},{"location":"api/_src/utils/io/#rs_tools._src.utils.io.get_files","title":"<code>get_files(datasets_spec, ext='.nc')</code>","text":"<p>Get a list of filenames based on the provided datasets specification.</p> <p>Parameters:</p> Name Type Description Default <code>datasets_spec</code> <code>DictConfig</code> <p>The datasets specification containing the path and extension.</p> required <code>ext</code> <code>str</code> <p>The file extension to filter the search. Defaults to \".nc\".</p> <code>'.nc'</code> <p>Returns:</p> Type Description <p>List[str]: A list of filenames.</p> Source code in <code>rs_tools/_src/utils/io.py</code> <pre><code>def get_files(datasets_spec: DictConfig, ext=\".nc\"):\n    \"\"\"\n    Get a list of filenames based on the provided datasets specification.\n\n    Args:\n        datasets_spec (DictConfig): The datasets specification containing the path and extension.\n        ext (str, optional): The file extension to filter the search. Defaults to \".nc\".\n\n    Returns:\n        List[str]: A list of filenames.\n\n    \"\"\"\n    data_path = datasets_spec.data_path\n    return get_list_filenames(data_path=data_path, ext=ext)\n</code></pre>"},{"location":"api/_src/utils/io/#rs_tools._src.utils.io.get_list_filenames","title":"<code>get_list_filenames(data_path='./', ext='*')</code>","text":"<p>Loads a list of file names within a directory.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The directory path to search for files. Defaults to \"./\".</p> <code>'./'</code> <code>ext</code> <code>str</code> <p>The file extension to filter the search. Defaults to \"*\".</p> <code>'*'</code> <p>Returns:</p> Type Description <p>List[str]: A sorted list of file names matching the given extension within the directory.</p> Source code in <code>rs_tools/_src/utils/io.py</code> <pre><code>def get_list_filenames(data_path: str=\"./\", ext: str=\"*\"):\n    \"\"\"\n    Loads a list of file names within a directory.\n\n    Args:\n        data_path (str, optional): The directory path to search for files. Defaults to \"./\".\n        ext (str, optional): The file extension to filter the search. Defaults to \"*\".\n\n    Returns:\n        List[str]: A sorted list of file names matching the given extension within the directory.\n    \"\"\"\n    pattern = f\"*{ext}\"\n    return sorted(glob.glob(os.path.join(data_path, \"**\", pattern), recursive=True))\n</code></pre>"},{"location":"api/_src/utils/math/","title":"Math","text":""},{"location":"api/_src/utils/math/#rs_tools._src.utils.math.bounds_and_points_to_step","title":"<code>bounds_and_points_to_step(xmin, xmax, Nx)</code>","text":"<p>Calculates the dx from the minmax Eq:     Lx = abs(xmax - xmin)     dx = Lx / (Nx - 1)</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Array | float</code> <p>the input start point</p> required <code>xmax</code> <code>Array | float</code> <p>the input end point</p> required <code>Nx</code> <code>int | float</code> <p>the number of points</p> required <p>Returns:</p> Name Type Description <code>dx</code> <code>Array | float</code> <p>the distance between each of the steps.</p> Source code in <code>rs_tools/_src/utils/math.py</code> <pre><code>def bounds_and_points_to_step(xmin: float, xmax: float, Nx: float) -&gt; float:\n    \"\"\"Calculates the dx from the minmax\n    Eq:\n        Lx = abs(xmax - xmin)\n        dx = Lx / (Nx - 1)\n\n    Args:\n        xmin (Array | float): the input start point\n        xmax (Array | float): the input end point\n        Nx (int | float): the number of points\n\n    Returns:\n        dx (Array | float): the distance between each of the\n            steps.\n    \"\"\"\n    Lx = abs(float(xmax) - float(xmin))\n    return float(Lx) / (float(Nx) - 1.0)\n</code></pre>"},{"location":"api/_src/utils/math/#rs_tools._src.utils.math.bounds_and_step_to_points","title":"<code>bounds_and_step_to_points(xmin, xmax, dx)</code>","text":"<p>Calculates the number of points from the endpoints and the stepsize</p> Eq <p>Nx = 1 + floor((xmax-xmin)) / dx)</p> <p>Parameters:</p> Name Type Description Default <code>xmin</code> <code>Array | float</code> <p>the input start point</p> required <code>xmax</code> <code>Array | float</code> <p>the input end point</p> required <code>dx</code> <code>Array | float</code> <p>stepsize between each point.</p> required <p>Returns:</p> Name Type Description <code>Nx</code> <code>Array | int</code> <p>the number of points</p> Source code in <code>rs_tools/_src/utils/math.py</code> <pre><code>def bounds_and_step_to_points(xmin: float, xmax: float, dx: float) -&gt; int:\n    \"\"\"Calculates the number of points from the\n    endpoints and the stepsize\n\n    Eq:\n        Nx = 1 + floor((xmax-xmin)) / dx)\n\n    Args:\n        xmin (Array | float): the input start point\n        xmax (Array | float): the input end point\n        dx (Array | float): stepsize between each point.\n\n    Returns:\n        Nx (Array | int): the number of points\n    \"\"\"\n    Lx = abs(float(xmax) - float(xmin))\n    return int(floor(1.0 + float(Lx) / float(dx)))\n</code></pre>"},{"location":"datasets/goes/","title":"GOES 16","text":"<p>Below are some notes on NOAA's GOES satellites, specifically focussing on the Advanced Baseline Imager (ABI).</p>"},{"location":"datasets/goes/#goes-satellites","title":"GOES Satellites","text":""},{"location":"datasets/goes/#goes-16_1","title":"GOES-16","text":"<ul> <li>Launched on 19 November 2016, operational since 18 December 2017</li> <li>Longitude central point: -75.2</li> </ul>"},{"location":"datasets/goes/#goes-17-no-longer-operational","title":"GOES-17 [no longer operational]","text":"<ul> <li>Launched on 1 March 2018, operational from 12 February 2019 to 4 January 2023 at longitude -136.9</li> <li>Replaced by GOES-18 due to issues with its Advanced Baseline Imager (ABI) instrument</li> <li>Moved to longitude -104.7 (between GOES-16 and GOES-18) and serves as backup for the operational satellites</li> </ul>"},{"location":"datasets/goes/#goes-18","title":"GOES-18","text":"<ul> <li>Launched on 1 March 2022, operational since 4 January 2023 (replaced GOES-17)</li> <li>Longitude central point: -136.9</li> </ul>"},{"location":"datasets/goes/#goes-instruments","title":"GOES Instruments","text":"<p>Earth-facing: - Advanced Baseline Imager (ABI) - Geostationary Lightning Mapper (GLM)]</p> <p>Sun-facing: - Extreme Ultraviolet and X-ray Irradiance Sensors (EXIS) - Solar Ultraviolet Imager (SUVI)</p> <p>Space environment: - Magnetometer (MAG) - Space Environment In-Situ Suite (SEISS)</p>"},{"location":"datasets/goes/#abi-data","title":"ABI Data","text":""},{"location":"datasets/goes/#processing-levels","title":"Processing Levels","text":"<ul> <li>Level-0: Raw instrument measurements</li> <li>Level-1B: Calibrated and geolocated radiances</li> <li>Level-2: Derived geophysical variables</li> <li>Level-3: Geophysical variables mapped on uniform space-time grid</li> </ul>"},{"location":"datasets/goes/#level-1b-spectral-bands-resolution","title":"Level-1B: Spectral Bands &amp; Resolution","text":""},{"location":"datasets/goes/#level-2-clear-sky-mask-acm","title":"Level 2: Clear Sky Mask (ACM)","text":"<p>The clear sky mask algorithm uses the GOES ABI visible, near-infrared and infrared bands to automatically assign one of the following 4 classes to each pixel: - cloudy - probably cloudy - probably clear - clear</p> <p>ACM data is provided at the native 2km resolution on the ABI fixed grid for full disk, CONUS, and mesoscale coverage regions, at the same temporal resolution as ABI L1b data.</p>"},{"location":"datasets/goes/#naming-conventions","title":"Naming Conventions","text":"<p>GOES ABI Level 1b and 2 data are named according to the following naming conventions:</p> <p><code>\\&lt;SE\\&gt;\\_\\&lt;DSN\\&gt;\\_\\&lt;PID\\&gt;\\_\\&lt;Obs Start Date &amp; Time\\&gt;\\_\\&lt;Obs End Date &amp; Time\\&gt;\\_\\&lt;Creation Date &amp; Time\\&gt;.\\&lt;FE\\&gt;</code></p> <p>where: - SE = System Environment - DSN = Data Short Name - PID = Platform Identifier - Obs Start Date &amp; Time = Observation Period Start Date &amp; Time - Obs End Date &amp; Time = Observation Period End Date &amp; Time - Creation Date &amp; Time = File Creation Date &amp; Time - FE = File Extension</p>"},{"location":"datasets/goes/#working-with-level-1b-data","title":"Working with Level-1B Data","text":"<p>GOES/ABI radiances are provided in \\(mW/m^2/sr/cm^{-1}\\), i.e. the data is normalised to wavenumbers. In order to convert the data to \\(W/m^2/sr/um\\), the data needs to be multiplied by \\(10^{-7}\\); \\(mW = 10^{-3} W\\), \\(cm^{-1} = 10^4 {um}\\).</p>"},{"location":"datasets/goes/#data-format-access","title":"Data Format &amp; Access","text":"<p>GOES Data can be explored in the following buckets:</p> <p>AWS: - GOES-16 AWS S3 Explorer - GOES-17 AWS S3 Explorer - GOES-18 AWS S3 Explorer</p> <p>Google Cloud: - GOES-16 Google Cloud Bucket Explorer - GOES-17 Google Cloud Bucket Explorer - GOES-18 Google Cloud Bucket Explorer </p>"},{"location":"datasets/goes/#software-tools","title":"Software Tools","text":"<p>GOES2GO - Software download  * allows downloading of GOES data from AWS</p>"},{"location":"datasets/goes/#qa","title":"Q/A","text":""},{"location":"datasets/modis/","title":"MODIS","text":"<p>Below are some notes on NASA's Terra and Aqua satellites, specifically focussing on the Moderate Resolution Imaging Spectrometer (MODIS).</p>"},{"location":"datasets/modis/#terra-aqua-satellites","title":"Terra &amp; Aqua Satellites","text":"<p>The Terra and Aqua satellites, launched in 1999 and 2002 respectively, are cornerstones in NASA's Earth Observation Program, and have collected invaluable measurements of Earth's land, oceans, cryosphere and atmosphere over the last two decades.  Both satellites follow a near-polar, Sun-synchronous orbit, i.e. they pass over the same point on Earth at the same time each day. Terra orbits the Earth as part of the Morning Train, while Aqua is part of the Afternoon Train of satellites. Both satellites image the same part of Earth approximately 3 hours apart, and together provide global coverage every 1 - 2 days. The orbit repeat cycle for each satellite is 16 days.</p>"},{"location":"datasets/modis/#terra-instruments","title":"Terra Instruments","text":"<ul> <li>Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER)</li> <li>Clouds and the Earth\u2019s Radiant Energy System (CERES)</li> <li>Multi-angle Imaging SpectroRadiometer (MISR)</li> <li>Moderate Resolution Imaging Spectrometer (MODIS)</li> <li>Measurement of Pollution in the Troposphere (MOPITT)</li> </ul>"},{"location":"datasets/modis/#aqua-instruments","title":"Aqua Instruments","text":"<ul> <li>Atmospheric Infrared Sounder (AIRS)</li> <li>Advanced Microwave Sounding Unit  (AMS-U)</li> <li>Humidity Sounder for Brazil (HSB)</li> <li>Advanced Microwave Scanning Radiometer for EOS (AMSR-E)</li> <li>Moderate Resolution Imaging Spectrometer (MODIS)</li> <li>Cloud's and the Earth's Radiant Energy System (CERES)</li> </ul>"},{"location":"datasets/modis/#modis-data","title":"MODIS Data","text":""},{"location":"datasets/modis/#spectral-bands-resolution","title":"Spectral Bands &amp; Resolution","text":"<p>Both satellites share the MODIS instrument, which collects data in 36 spectral channels:</p> <p></p> <p>Image Credit: Wikipedia</p>"},{"location":"datasets/modis/#processing-levels","title":"Processing Levels","text":"<ul> <li>Level-0: Raw instrument measurements (swath product)</li> <li>Level-1A: Scans of raw radiances in counts (swath product)</li> <li>Level-1B: Calibrated radiances at 250, 500, 1000 m resolution (swath product)</li> <li>Level-2: Derived geophysical variables at the same resolution and location as Level-1 source data (swath products)</li> <li>Level-2G: Level-2 data mapped on a uniform space-time grid scale (Sinusoidal)</li> <li>Level-3: Gridded variables in derived spatial and/or temporal resolutions</li> <li>Level-4: Model output or results from analyses of lower-level data</li> </ul> <p>In this project, we mainly work with Level 1-B data.</p>"},{"location":"datasets/modis/#naming-conventions","title":"Naming Conventions","text":"<p>The MODIS Level-1 data files are named according to the following naming conventions: SATXX.AYYYYDD.HHDD.CCC.YYYYDDDHHMMSS.hdf</p> <ul> <li>SAT: Satellite (Terra -&gt; MOD, Aqua -&gt; MYD, Combined Product -&gt; MCD)</li> <li>XX: Other product details (e.g. QKM -&gt; 250 m, HKM -&gt; 500 m, 1KM -&gt; 1000 m resolution)</li> <li>AYYYYDD: Julian Day of Acquisition</li> <li>HHDD: Time of Acquisition</li> <li>CCC: Collection</li> <li>YYYYDDDHHMMSS: Julian Day of Production</li> </ul>"},{"location":"datasets/modis/#day-night-mode","title":"Day &amp; Night Mode","text":"<p>MODIS continuously collects measurements in either day mode, night mode, or mixed mode, depending on the time of day of the are under observation. During each orbit, 9 day mode, 9 night mode, and 2 mixed mode granules are measured respectively. To reduce storage space and transmission of files containing no useful data, Level-1B allows writing of 250 m and 500 m data files to be turned off for granules that contain no day mode scans. When data are transmitted in night mode, the Reflective Solar Bands (bands 1-19) are empty and appear to contain fill values of \"65535\". While day and mixed mode files are usually 200-300 MB large, night mode files are often &lt;100 MB.</p>"},{"location":"datasets/modis/#working-with-level-1b-data","title":"Working with Level-1B Data","text":"<p>A jupyter notebook on how to download, open, and plot Level-1B data is provided in the main repository. Below, we summarise some of the key takeaways from working with Level-1B data. </p> <p>Each Level-1B granule contains multiple data variables, including (1) the science data on a pixel-to-pixel basis, (2) uncertainty information on a pixel-to-pixel basis, (3) geolocation data for the pixels, and (4) metadata. Exactly how many variables are contained in each file depends on the spatial resolution. The 1KM aggregated data, for instance, contains 27 data variables, for which the science data is stored in the following variable names:</p> <p>EV_250_Aggr1km_RefSB: Earth View 250M Aggregated 1km Reflective Solar Bands Scaled Integers * Bands: 1, 2</p> <p>EV_500_Aggr1km_RefSB: Earth View 500M Aggregated 1km Reflective Solar Bands Scaled Integers * Bands: 3, 4, 5, 6, 7</p> <p>EV_1KM_RefS: Earth View 1KM Reflective Solar Bands Scaled Integers * Bands: 8, 9, 10, 11, 12, 13lo, 13hi, 14lo, 14hi, 15, 16, 17, 18, 19, 26</p> <p>EV_1KM_Emissive: Earth View 1KM Emissive Bands Scaled Integers * Bands: 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36</p> <p>Note that the data is provided in \"scaled integers\" and does not yet include units. To convert the scaled integers into radiances (or reflectances), the data needs to be corrected by subtracting the radiance_offsets (reflectance_offsets) and multiplying with the radiance_scales (reflectance_scales). After this correction step, radiances are provided in \\(W/m^2/\\micro m/sr\\), while reflectance is still unit-less. More information on this correction step is provided in the MODIS Level-1B User Guide.</p> <p>In addition to the science data, geolocation information is provided for each granule. Note that the size of the latitude/longitude coordinates (406, 271) does not match the size of the data (2030, 1354). This is because geolocation data is only provided for a subset of the pixels. To match the geolocation data to the size of the data, the provided latitude/longitude coordinates can be interpolated.</p>"},{"location":"datasets/modis/#level-2-cloud-mask-35_l2","title":"Level 2: Cloud Mask (35_L2)","text":"<p>The MODIS Cloud Mask product is a Level 2 product generated at 1-km and 250-m (at nadir) spatial resolutions from MODIS visible, near-infrared and infrared bands to automatically assign one of 4 classes to each pixel:</p> <ul> <li>cloudy</li> <li>probably cloudy</li> <li>probably clear</li> <li>clear</li> </ul> <p>There are two MODIS Cloud Mask data product files: MOD35_L2 (for Terra satellite) and MYD35_L2 (for Aqua satellite).</p>"},{"location":"datasets/modis/#data-format-access","title":"Data Format &amp; Access","text":"<p>MODIS data is provided in the .hdf file format.</p> <p>MODIS data can be downloaded via NASA's EarthData graphical user interface (https://ladsweb.modaps.eosdis.nasa.gov/search/) and data archive (https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/61), or via the USGS (https://e4ftl01.cr.usgs.gov/). Note that the latter brings you directly to a US government computer that hosts the MODIS data (MOLA --&gt; AQUA data, MOLT --&gt; TERRA data, MOTA --&gt; AQUA &amp; TERRA data); the terms and conditions of usage should be respected when accessing this resource. As part of rs_tools, we developed easy-to-use download scripts to download data directly from NASA EarthData.</p>"},{"location":"datasets/modis/#software-tools","title":"Software Tools","text":"<p>Google Earth Engine.</p> <ul> <li>gee-tool</li> <li>wxee - xarray-based</li> </ul> <p>Other Tools</p> <ul> <li>modis-tool</li> <li>pansat</li> <li>py-modis</li> </ul>"},{"location":"datasets/modis/#qa","title":"Q/A","text":"<p>Noah</p> <ul> <li>How long it takes to download an image or set of images this way?</li> <li>Is there a \"cutoff\" date for when the images aren't available? --&gt; Still collecting data today.</li> <li>Can you pre-filter based on cloud coverage? --&gt; I don't think so?</li> </ul>"},{"location":"datasets/msg/","title":"MSG","text":"<p>Below are some notes on the Meteosat Second Generation (MSG) Spinning Enhanced Visible and Infrared Imager (SEVIRI) instrument.</p>"},{"location":"datasets/msg/#msg-satellites","title":"MSG Satellites","text":"<p>The MSG satellite family consists of four operational meteorological satellites in geostationary orbit: Meteosat-8, Meteosat-9, Meteosat-10, and Meteosat-11. Information about launch dates and operational periods is given below. All satellites have a primary field of view over Europe. After their main operational period, Meteosat-8 and Meteosat-10 were relocated to perform measurements over the Indian Ocean. During this time, they are referred to as Meteosat-8 (IODC) and Meteosat-10 (IODC).  MSG superseded the Meteosat First Generation (MFG) and is followed by the Metesat Third Generation (MTG). All satellites contain the same instruments, some of which were also flown on other Meteosat satellites.</p> <p>Meteosat-8: * Launch Date: 28 Aug 2002 * End of Life: 04 Jul 2016 * Position: 3.7\u00b0 E</p> <p>Meteosat-8 (IODC): * Launch Date: 15 Sep 2016 * End of Life: Nov 2022 * Position: 41.5\u00b0 E</p> <p>Meteosat-9: * Launch Date: 21 Dec 2005 * End of Life: 01 Apr 2022 * Position: 3.5\u00b0 E</p> <p>Meteosat-9 (IODC): * Launch Date: 01 Jul 2022 * End of Life: Operational (as of 01/2024) * Position: 45.5\u00b0 E</p> <p>Meteosat-10: * Launch Date: 05 Jul 2012 * End of Life: Operational (as of 01/2024) * Position: 0.0\u00b0 E</p> <p>Meteosat-11: * Launch Date: 15 Jul 2015 * End of Life: Operational (as of 01/2024) * Position: 9.5\u00b0 E</p>"},{"location":"datasets/msg/#instruments","title":"Instruments:","text":"<ul> <li>Data Collection Service (DCS)</li> <li>Geostationary Search and Rescue (GOES&amp;R)</li> <li>Geostationary Earth Radiation Budget (GERB)</li> <li>Spinning Enhanced Visible and Infrared Imager (SEVIRI)</li> </ul>"},{"location":"datasets/msg/#seviri-data","title":"SEVIRI Data","text":""},{"location":"datasets/msg/#spectral-bands-resolution","title":"Spectral Bands &amp; Resolution","text":"<p>The SEVIRI instruments measure 12 spectral channels.</p> <p></p> <p>Image Credit: V\u00e1zquez-Navarro et al. A fast method for the retrieval of integrated longwave and shortwave top-of-atmosphere irradiances from MSG/SEVIRI (RRUMS), Atmospheric Measurement Techniques 5(4):4969-5008 (2012), DOI: 10.5194/amtd-5-4969-2012 </p> <p>The resolution of the instrument is 1 km at the sub-satellite point (SSP) for the high resolution visible (HRV) channel, and 3 km for all other channels. The resolution decreases closer to the image limb.</p> <p></p> <p>Image Credit: Eissa et al. Validation of the Surface Downwelling Solar Irradiance Estimates of the HelioClim-3 Database in Egypt, Remote Sens. 2015, 7(7), 9269-9291, DOI: 10.3390/rs70709269</p>"},{"location":"datasets/msg/#instrument-field-of-view","title":"Instrument Field-of-view","text":"<p>An example field-of-view (FOV) the satellites positioned at 0.0\u00b0 E  and over the Indian Ocean are shown below.</p> <p></p> <p></p> <p>Credit: EUMETSAT</p> <p>In addition to the full disk measurements, the rapid scan mode is performed over the following FOV.</p> <p></p> <p>Credit: EUMETSAT</p>"},{"location":"datasets/msg/#processing-levels","title":"Processing Levels","text":"<ul> <li>Level-0: Raw instrument measurements.</li> <li>Level-1.5: Geolocated and radiometrically pre-processed image data, ready for further processing. Spacecraft specific effects have been removed. More information about Level-1.5 data can be found in the technical user guide.</li> </ul>"},{"location":"datasets/msg/#naming-conventions","title":"Naming Conventions","text":"<p>The MSG Level-1 data files are named according to the following naming conventions: MSG#-IIII-MSGXX-0100-NA-YYYYMMDDHHMMSS.ssssZ-NA.nat, for example MSG4-SEVI-MSG15-0100-NA-20211110081242.766000000Z-NA.nat.</p> <ul> <li>IIII: Instrument details (e.g. SEVI for SEVIRI)</li> <li>XX-0100: likely referring to specific product or data types</li> <li>NA: not locationally constrained</li> <li>YYYYMMDD: Acquisition date</li> <li>HHMMSS: Acquisition time</li> <li>ssss: likely referring to sub-second acquisition time</li> <li>Z: Zulu / UTC time zone</li> <li>NA: not locationally constrained</li> </ul>"},{"location":"datasets/msg/#satellite-number-eg-1-meteosat-8-2-meteosat-9-etc","title":": Satellite number (e.g. 1 -&gt; Meteosat-8, 2 -&gt; Meteosat-9, etc.)","text":""},{"location":"datasets/msg/#measurement-frequencies","title":"Measurement Frequencies","text":"<p>Full disk images are measured every 15 mins.</p>"},{"location":"datasets/msg/#day-night-mode","title":"Day &amp; Night Mode","text":""},{"location":"datasets/msg/#cloud-mask","title":"Cloud Mask","text":"<p>Cloud masks are provided (e.g. as data product EO:EUM:DAT:MSG:CLM), and contain 4 types of pixel classifications: * 0 = clear sky over land * 1 = clear sky over water * 2 = clouds * 3 = not processed</p> <p></p>"},{"location":"datasets/msg/#working-with-level-1-data","title":"Working with Level-1 Data","text":"<p>MSG/SEVIRI radiances are provided in \\(mW/m^2/sr/cm^{-1}\\), i.e. the data is normalised to wavenumbers. In order to convert the data to \\(W/m^2/sr/um\\), the data needs to be multiplied by \\(10^{-7}\\); \\(mW = 10^{-3} W\\), \\(cm^{-1} = 10^4 {um}\\).</p>"},{"location":"datasets/msg/#data-format-access","title":"Data Format &amp; Access","text":"<p>SEVIRI data can be downloaded via EUMETSAT's Data Access portal. The following data products are available:</p> <ul> <li>EO:EUM:DAT:MSG:CLM - Cloud Mask (0 degree position)</li> <li>EO:EUM:DAT:MSG:CLM-IODC - Cloud Mask (Indian Ocean)</li> <li> <p>EO:EUM:DAT:MSG:RSS-CLM - Rapid Scan Cloud Mask</p> </li> <li> <p>EO:EUM:DAT:MSG:HRSEVIRI - High Rate SEVIRI Level 1.5 Image Data (0 degree position)</p> </li> <li>EO:EUM:DAT:MSG:HRSEVIRI-IODC - High Rate SEVIRI Level 1.5 Image Data (Indian Ocean)</li> <li>EO:EUM:DAT:MSG:MSG15-RSS - Rapid Scan High Rate SEVIRI Level 1.5 Image Data</li> </ul> <p>The data is provided in .nat format. Each file contains data of shape (3712, 3712).</p>"},{"location":"datasets/msg/#software-tools","title":"Software Tools","text":""},{"location":"pipelines/goes16/","title":"GOES16 Pipeline","text":"In\u00a0[66]: Copied! <pre>import autoroot\nimport os\nfrom dotenv import load_dotenv\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport rasterio\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\nxr.set_options(\n    keep_attrs=True, \n    display_expand_data=False, \n    display_expand_coords=False, \n    display_expand_data_vars=False, \n    display_expand_indexes=False\n)\nnp.set_printoptions(threshold=10, edgeitems=2)\n\n%matplotlib inline\n</pre> import autoroot import os from dotenv import load_dotenv import xarray as xr import numpy as np import matplotlib.pyplot as plt import cartopy.crs as ccrs import rasterio import cartopy import cartopy.crs as ccrs import cartopy.feature as cfeature from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER xr.set_options(     keep_attrs=True,      display_expand_data=False,      display_expand_coords=False,      display_expand_data_vars=False,      display_expand_indexes=False ) np.set_printoptions(threshold=10, edgeitems=2)  %matplotlib inline In\u00a0[48]: Copied! <pre>cwd = autoroot.root\n</pre> cwd = autoroot.root <p>This is arguably the most important part. We need to define where we want to save the data.</p> <p>Note: The data is very heavy! So make sure you have adequate space.</p> In\u00a0[21]: Copied! <pre>save_dir = os.getenv(\"ITI_DATA_SAVEDIR\")\n</pre> save_dir = os.getenv(\"ITI_DATA_SAVEDIR\") In\u00a0[22]: Copied! <pre>!cat $autoroot.root/config/example/download.yaml\n</pre> !cat $autoroot.root/config/example/download.yaml <pre># PERIOD\nperiod:\n  start_date: '2020-10-01'\n  start_time: '00:00:00'\n  end_date: '2020-10-31'\n  end_time: '23:59:00'\n\n# CLOUD MASK\ncloud_mask: True\n  \n# PATH FOR SAVING DATA\nsave_dir: data\n\ndefaults:\n  - _self_\n  \n</pre> <p>We will change the save directory, start/end time, and the time step.</p> <pre>python rs_tools \\\n    satellite=goes \\\n    stage=download \\\n    save_dir=\"/path/to/savedir\" \\\n    period.start_date=\"2020-10-01\" \\\n    period.end_date=\"2020-10-02\" \\\n    period.start_time=\"09:00:00\" \\\n    period.end_time=\"21:00:00\" \\\n    satellite.download.time_step=\"6:00:00\"\n</pre> <p>We have an extensive geoprocessing steps to be able to</p> <p>We can peek into the <code>rs_tools/config/example/download.yaml</code> configuration file to see some of the options we have to modify this.</p> In\u00a0[23]: Copied! <pre>!cat $autoroot.root/config/example/satellite/goes.yaml\n</pre> !cat $autoroot.root/config/example/satellite/goes.yaml <pre>download:\n  _target_: rs_tools._src.data.goes.downloader_goes16.download\n  save_dir: ${save_dir}/goes16/raw\n  start_date: ${period.start_date}\n  start_time: ${period.start_time}\n  end_date: ${period.end_date}\n  end_time: ${period.end_time}\n  daily_window_t0: \"14:00:00\"\n  daily_window_t1: \"20:00:00\"\n  time_step: \"1:00:00\"\n\ngeoprocess:\n  _target_: rs_tools._src.geoprocessing.goes.geoprocessor_goes16.geoprocess\n  read_path: ${read_path}/goes16/raw\n  save_path: ${save_path}/goes16/geoprocessed\n  resolution: null\n  region: \"-130 -15 -90 5\"\n  resample_method: bilinear\n\n# preprocess:\n\npatch:\n  _target_: rs_tools._src.preprocessing.prepatcher.prepatch\n  read_path: ${read_path}/goes16/geoprocessed\n  save_path: ${save_path}/goes16/analysis\n  patch_size: ${patch_size}\n  stride_size: ${stride_size}\n  nan_cutoff: ${nan_cutoff}\n  save_filetype: ${save_filetype}\n</pre> <p>In particular, we will focus on the <code>geoprocess</code> step within the configuration. The most important options are the <code>resolution</code> and the <code>region</code>. The resolution is a float or integer that is measured in km.</p> <p>Below, we have an example of the command we</p> <pre>python rs_tools \\\n    satellite=goes \\\n    stage=geoprocess \\\n    read_path=\"/path/to/savedir/\" \\\n    save_path=\"/path/to/savedir/\" \\\n    satellite.geoprocess.resolution=5000\n</pre> <p>We can see the saved data are clean</p> <pre>/path/to/savedir/goes16/geoprocessed/20201001150019_goes16.nc\n/path/to/savedir/goes16/geoprocessed/20201002150019_goes16.nc\n</pre> In\u00a0[67]: Copied! <pre>ds = xr.open_dataset(f\"{save_dir}/goes16/geoprocessed/20201001150019_goes16.nc\", engine=\"netcdf4\")\n</pre> ds = xr.open_dataset(f\"{save_dir}/goes16/geoprocessed/20201001150019_goes16.nc\", engine=\"netcdf4\") In\u00a0[68]: Copied! <pre>ds\n</pre> ds Out[68]: <pre>&lt;xarray.Dataset&gt; Size: 10MB\nDimensions:          (x: 302, y: 207, time: 1, band_wavelength: 16, band: 16)\nCoordinates: (8)\nData variables: (2)\nAttributes: (12/30)\n    naming_authority:          gov.nesdis.noaa\n    Conventions:               CF-1.7\n    standard_name_vocabulary:  CF Standard Name Table (v35, 20 July 2016)\n    institution:               DOC/NOAA/NESDIS &gt; U.S. Department of Commerce,...\n    project:                   GOES\n    production_site:           RBU\n    ...                        ...\n    timeline_id:               ABI Mode 6\n    date_created:              2020-10-01T15:09:56.5Z\n    time_coverage_start:       2020-10-01T15:00:19.6Z\n    time_coverage_end:         2020-10-01T15:09:50.4Z\n    LUT_Filenames:             SpaceLookParams(FM1A_CDRL79RevP_PR_09_00_02)-6...\n    id:                        ae981973-758f-4213-b71e-e619d91ddddb</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>x: 302</li><li>y: 207</li><li>time: 1</li><li>band_wavelength: 16</li><li>band: 16</li></ul></li><li>Coordinates: (8)<ul><li>x(x)float64-4.64e+06 -4.63e+06 ... -1.63e+06axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metre<pre>array([-4640312.504387, -4630312.504387, -4620312.504387, ..., -1650312.504387,\n       -1640312.504387, -1630312.504387])</pre></li><li>y(y)float645.431e+05 5.331e+05 ... -1.517e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metre<pre>array([  543098.816912,   533098.816912,   523098.816912, ..., -1496901.183088,\n       -1506901.183088, -1516901.183088])</pre></li><li>time(time)&lt;U16'2020-10-01 15:05'<pre>array(['2020-10-01 15:05'], dtype='&lt;U16')</pre></li><li>band_wavelength(band_wavelength)float320.47 0.64 0.87 ... 12.27 13.27long_name :ABI band central wavelengthstandard_name :sensor_band_central_radiation_wavelengthunits :um<pre>array([ 0.47,  0.64,  0.87,  1.38,  1.61,  2.25,  3.89,  6.17,  6.93,  7.34,\n        8.44,  9.61, 10.33, 11.19, 12.27, 13.27], dtype=float32)</pre></li><li>band(band)int81 2 3 4 5 6 7 ... 11 12 13 14 15 16long_name :ABI band numberstandard_name :sensor_band_identifierunits :1<pre>array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n      dtype=int8)</pre></li><li>latitude(y, x)float64...units :degrees_north<pre>[62514 values with dtype=float64]</pre></li><li>longitude(y, x)float64...units :degrees_east<pre>[62514 values with dtype=float64]</pre></li><li>cloud_mask(y, x)float64...long_name :ABI L2+ Clear Sky Maskstandard_name :cloud_binary_maskunits :1<pre>[62514 values with dtype=float64]</pre></li></ul></li><li>Data variables: (2)<ul><li>Rad(band, y, x)float32...long_name :ABI L1b Radiancesstandard_name :toa_outgoing_radiance_per_unit_wavelengthunits :W m-2 sr-1 um-1grid_mapping :goes_imager_projection<pre>[1000224 values with dtype=float32]</pre></li><li>DQF(band, y, x)float32...grid_mapping :goes_imager_projection<pre>[1000224 values with dtype=float32]</pre></li></ul></li><li>Indexes: (5)<ul><li>xPandasIndex<pre>PandasIndex(Index([ -4640312.504387059,  -4630312.504387059,  -4620312.504387059,\n        -4610312.504387059,  -4600312.504387059,  -4590312.504387059,\n        -4580312.504387059,  -4570312.504387059,  -4560312.504387059,\n        -4550312.504387059,\n       ...\n       -1720312.5043870592, -1710312.5043870592, -1700312.5043870592,\n       -1690312.5043870592, -1680312.5043870592, -1670312.5043870592,\n       -1660312.5043870592, -1650312.5043870592, -1640312.5043870592,\n       -1630312.5043870592],\n      dtype='float64', name='x', length=302))</pre></li><li>yPandasIndex<pre>PandasIndex(Index([ 543098.8169122998,  533098.8169122998, 523098.81691229984,\n       513098.81691229984, 503098.81691229984, 493098.81691229984,\n       483098.81691229984, 473098.81691229984, 463098.81691229984,\n       453098.81691229984,\n       ...\n         -1426901.1830877,   -1436901.1830877,   -1446901.1830877,\n         -1456901.1830877,   -1466901.1830877,   -1476901.1830877,\n         -1486901.1830877,   -1496901.1830877,   -1506901.1830877,\n         -1516901.1830877],\n      dtype='float64', name='y', length=207))</pre></li><li>timePandasIndex<pre>PandasIndex(Index(['2020-10-01 15:05'], dtype='object', name='time'))</pre></li><li>band_wavelengthPandasIndex<pre>PandasIndex(Index([0.4699999988079071, 0.6399999856948853, 0.8700000047683716,\n       1.3799999952316284, 1.6100000143051147,               2.25,\n        3.890000104904175,  6.170000076293945,  6.929999828338623,\n        7.340000152587891,    8.4399995803833,  9.609999656677246,\n       10.329999923706055,   11.1899995803833, 12.270000457763672,\n       13.270000457763672],\n      dtype='float32', name='band_wavelength'))</pre></li><li>bandPandasIndex<pre>PandasIndex(Index([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], dtype='int8', name='band'))</pre></li></ul></li><li>Attributes: (30)naming_authority :gov.nesdis.noaaConventions :CF-1.7standard_name_vocabulary :CF Standard Name Table (v35, 20 July 2016)institution :DOC/NOAA/NESDIS &gt; U.S. Department of Commerce, National Oceanic and Atmospheric Administration, National Environmental Satellite, Data, and Information Servicesproject :GOESproduction_site :RBUproduction_environment :OEspatial_resolution :1km at nadirMetadata_Conventions :Unidata Dataset Discovery v1.0orbital_slot :GOES-Eastplatform_ID :G16instrument_type :GOES R Series Advanced Baseline Imagerscene_id :Full Diskinstrument_ID :FM1title :ABI L1b Radiancessummary :Single reflective band ABI L1b Radiance Products are digital maps of outgoing radiance values at the top of the atmosphere for visible and near-IR bands.keywords :SPECTRAL/ENGINEERING &gt; VISIBLE WAVELENGTHS &gt; VISIBLE RADIANCEkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Earth Science Keywords, Version 7.0.0.0.0iso_series_metadata_id :a70be540-c38b-11e0-962b-0800200c9a66license :Unclassified data.  Access is restricted to approved users only.processing_level :National Aeronautics and Space Administration (NASA) L1bcdm_data_type :Imagedataset_name :OR_ABI-L1b-RadF-M6C01_G16_s20202751500196_e20202751509504_c20202751509565.ncproduction_data_source :Realtimetimeline_id :ABI Mode 6date_created :2020-10-01T15:09:56.5Ztime_coverage_start :2020-10-01T15:00:19.6Ztime_coverage_end :2020-10-01T15:09:50.4ZLUT_Filenames :SpaceLookParams(FM1A_CDRL79RevP_PR_09_00_02)-633686100.0.h5 QTableBand01(FM1A_CDRL79RevH_DO_07_00_00)-582860861.0.h5 CalTargetTimeIntervals(FM1A_CDRL79RevP_DO_08_00_01)-611906620.0.h5 BandSaturationLimits(FM1A_CDRL79RevH_DO_08_00_00)-600000000.0.h5 SolarSpaceLookParams(FM1A_CDRL79RevH_DO_09_00_00)-600765435.0.h5 DeadRowListParams(FM1A_CDRL79RevH_DO_08_00_00)-600000000.0.h5 Mirror_Record(FM1A_CDRL79RevG_DO_07_00_00)-582860861.0.h5 KalmanAstroConsts(FM1A_CDRL79RevH_DO_08_00_00)-600000000.0.xml KalmanFilterControls(FM1A_CDRL79RevJ_PR_09_02_06)-651055200.0.xml KalmanMeasMaxSensibles(FMAA_INT_ONLY_DO_09_01_00)-651039038.0.xml KalmanPreprocessorControls(FM1A_CDRL79RevJ_DO_07_00_00)-582860861.0.xml KalmanReferenceData(FM1A_CDRL79RevH_DO_08_00_00)-888.0.xml KalmanStarCatalogs(FM1A_CDRL79RevH_DO_08_00_00)-600000000.0.xml ABI_NavigationRDP_Band01(FM1A_CDRL79RevJ_DO_07_00_00)-582860861.0.xml ABI_NavigationParameters_Band01(FM1A_CDRL79RevH_DO_07_00_00)-582860861.0.xml ABI_ResamplingImplementation_Band01(FM1A_CDRL79RevH_DO_07_02_00)-602129336.0.xml ABI_ResamplingParameters_Band01(FM1A_CDRL79RevJ_DO_07_00_00)-582860861.0.xml StarLookParams(FM1A_CDRL79RevH_DO_08_00_00)-600000000.0.h5 StarDetectionParams(FM1A_CDRL79RevJ_DO_07_00_00)-582860861.0.xml ResamplingScaledConversion(FMAA_INT_ONLY_DO_08_00_00)-1111.0.xml BlockReleaseRegions(FMAA_INT_ONLY_DO_08_00_00)-2222.0.csv VNIR_RetrievalParameters(FM1A_CDRL79RevH_DO_08_00_00)-600000000.0.h5 SCT_Record(FM1A_CDRL79RevM_DO_09_00_00)-600765435.0.h5 ICM_ConversionConsts(FM1A_CDRL43-18_DO_09_01_00)-651038920.0.h5 ICM_SensorCoefficients(FM1A_TMABI_18_159_TMABI_18_533_DO_09_01_00)-651038920.0.h5id :ae981973-758f-4213-b71e-e619d91ddddb</li></ul> In\u00a0[80]: Copied! <pre># in an even better way \nfig = plt.figure()\nax = plt.axes(projection=ccrs.PlateCarree())\n# ax.set_extent([-20, -10, 30, 60])\n# out[\"1\"].plot(ax=ax, transform=ccrs.PlateCarree())\n# ax.pcolormesh(out[\"1\"].longitude, out[\"1\"].latitude, out[\"1\"].values)\nds.isel(band=0).Rad.plot.pcolormesh(x=\"longitude\", y=\"latitude\", transform=ccrs.PlateCarree())\n\n\nax.set(xlim=[-140, -70,],\n      ylim=[ -40, 10])\n\n# # Add map features with Cartopy \n# ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m', \n#                                             edgecolor='face', \n#                                             facecolor='lightgray'))\nax.coastlines()\n# Plot lat/lon grid \ngl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n                  linewidth=0.1, color='k', alpha=1, \n                  linestyle='--')\ngl.top_labels = False\ngl.right_labels = False\ngl.xformatter = LONGITUDE_FORMATTER\ngl.yformatter = LATITUDE_FORMATTER\ngl.xlabel_style = {'size': 8}\ngl.ylabel_style = {'size': 8} \nplt.tight_layout()\nplt.show()\n</pre> # in an even better way  fig = plt.figure() ax = plt.axes(projection=ccrs.PlateCarree()) # ax.set_extent([-20, -10, 30, 60]) # out[\"1\"].plot(ax=ax, transform=ccrs.PlateCarree()) # ax.pcolormesh(out[\"1\"].longitude, out[\"1\"].latitude, out[\"1\"].values) ds.isel(band=0).Rad.plot.pcolormesh(x=\"longitude\", y=\"latitude\", transform=ccrs.PlateCarree())   ax.set(xlim=[-140, -70,],       ylim=[ -40, 10])  # # Add map features with Cartopy  # ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m',  #                                             edgecolor='face',  #                                             facecolor='lightgray')) ax.coastlines() # Plot lat/lon grid  gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,                   linewidth=0.1, color='k', alpha=1,                    linestyle='--') gl.top_labels = False gl.right_labels = False gl.xformatter = LONGITUDE_FORMATTER gl.yformatter = LATITUDE_FORMATTER gl.xlabel_style = {'size': 8} gl.ylabel_style = {'size': 8}  plt.tight_layout() plt.show() In\u00a0[49]: Copied! <pre>!cat $cwd/config/example/patch.yaml\n</pre> !cat $cwd/config/example/patch.yaml <pre># PATH WITH GEOPROCESSED DATA\nread_path: data\n\n# PATH FOR SAVING PATCHES\nsave_path: data\n\n#\u00a0PATCH PARAMETERS\npatch_size: 256\nstride_size: 256\n\n# NAN CUTOFF\nnan_cutoff: 0.5\n\n# FILETYPE TO SAVE [nc = netcdf, np = numpy]\nsave_filetype: nc\n\ndefaults:\n  - _self_</pre> In\u00a0[26]: Copied! <pre>!PYTHONPATH=\".\" python rs_tools \\\n    satellite=goes \\\n    stage=patch \\\n    read_path=/pool/usuarios/juanjohn/data/iti/ \\\n    save_path=/pool/usuarios/juanjohn/data/iti/ \\\n    nan_cutoff=1.0 \\\n    patch_size=16 \\\n    stride_size=16\n</pre> !PYTHONPATH=\".\" python rs_tools \\     satellite=goes \\     stage=patch \\     read_path=/pool/usuarios/juanjohn/data/iti/ \\     save_path=/pool/usuarios/juanjohn/data/iti/ \\     nan_cutoff=1.0 \\     patch_size=16 \\     stride_size=16 <pre>python: can't open file '/home/juanjohn/projects/rs_tools/config/example/rs_tools': [Errno 2] No such file or directory\n</pre> <p>The most important arguments are the <code>patch_size</code> and <code>stride_size</code> argument. The patch_size dictates how big the patches should be and the stride_size dictates how much space should be between patches. For complete overlap, the stride size should be the <code>patch_size-1</code>. For no overlap, the stride size should be <code>patch_size</code></p> In\u00a0[81]: Copied! <pre>ds = xr.open_dataset(f\"{save_dir}/goes16/analysis/20201001150019_patch_0.nc\", engine=\"netcdf4\")\n</pre> ds = xr.open_dataset(f\"{save_dir}/goes16/analysis/20201001150019_patch_0.nc\", engine=\"netcdf4\") In\u00a0[83]: Copied! <pre># in an even better way \nfig = plt.figure()\nax = plt.axes(projection=ccrs.PlateCarree())\n# ax.set_extent([-20, -10, 30, 60])\n# out[\"1\"].plot(ax=ax, transform=ccrs.PlateCarree())\n# ax.pcolormesh(out[\"1\"].longitude, out[\"1\"].latitude, out[\"1\"].values)\nds.isel(band=0).Rad.plot.pcolormesh(x=\"longitude\", y=\"latitude\", transform=ccrs.PlateCarree())\n\n\nax.set(xlim=[-140, -70,],\n      ylim=[ -40, 10])\n\n# # Add map features with Cartopy \n# ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m', \n#                                             edgecolor='face', \n#                                             facecolor='lightgray'))\nax.coastlines()\n# Plot lat/lon grid \ngl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n                  linewidth=0.1, color='k', alpha=1, \n                  linestyle='--')\ngl.top_labels = False\ngl.right_labels = False\ngl.xformatter = LONGITUDE_FORMATTER\ngl.yformatter = LATITUDE_FORMATTER\ngl.xlabel_style = {'size': 8}\ngl.ylabel_style = {'size': 8} \nplt.tight_layout()\nplt.show()\n</pre> # in an even better way  fig = plt.figure() ax = plt.axes(projection=ccrs.PlateCarree()) # ax.set_extent([-20, -10, 30, 60]) # out[\"1\"].plot(ax=ax, transform=ccrs.PlateCarree()) # ax.pcolormesh(out[\"1\"].longitude, out[\"1\"].latitude, out[\"1\"].values) ds.isel(band=0).Rad.plot.pcolormesh(x=\"longitude\", y=\"latitude\", transform=ccrs.PlateCarree())   ax.set(xlim=[-140, -70,],       ylim=[ -40, 10])  # # Add map features with Cartopy  # ax.add_feature(cfeature.NaturalEarthFeature('physical', 'land', '10m',  #                                             edgecolor='face',  #                                             facecolor='lightgray')) ax.coastlines() # Plot lat/lon grid  gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,                   linewidth=0.1, color='k', alpha=1,                    linestyle='--') gl.top_labels = False gl.right_labels = False gl.xformatter = LONGITUDE_FORMATTER gl.yformatter = LATITUDE_FORMATTER gl.xlabel_style = {'size': 8} gl.ylabel_style = {'size': 8}  plt.tight_layout() plt.show() In\u00a0[60]: Copied! <pre>import mlx.data as dx\nfrom rs_tools._src.utils.io import get_list_filenames\n</pre> import mlx.data as dx from rs_tools._src.utils.io import get_list_filenames <pre>/home/juanjohn/miniconda/envs/rs_tools/lib/python3.11/site-packages/goes2go/data.py:519: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n  within=pd.to_timedelta(config[\"nearesttime\"].get(\"within\", \"1H\")),\n/home/juanjohn/miniconda/envs/rs_tools/lib/python3.11/site-packages/goes2go/NEW.py:188: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n  within=pd.to_timedelta(config[\"nearesttime\"].get(\"within\", \"1H\")),\n</pre> In\u00a0[63]: Copied! <pre>list_of_files = get_list_filenames(f\"{save_dir}/goes16/analysis/\", \".nc\")\n</pre> list_of_files = get_list_filenames(f\"{save_dir}/goes16/analysis/\", \".nc\") In\u00a0[54]: Copied! <pre>def load_fn(\n    file: str, \n    load_coords: bool=True,\n    load_cloudmask: bool=True,\n):\n    data_dict = {}\n    # Load dataset\n    ds: xr.Dataset = xr.load_dataset(file, engine=\"netcdf4\")\n\n    # Extract data\n    data = ds.Rad.compute().to_numpy()\n    data_dict[\"data\"] = data\n    # Extract wavelengths\n    wavelengths = ds.band_wavelength.compute().to_numpy()\n    data_dict[\"wavelengths\"] = wavelengths\n\n    # Extract coordinates\n    if load_coords:\n        latitude = ds.latitude.compute().to_numpy()\n        longitude = ds.longitude.compute().to_numpy()\n        coords = np.stack([latitude, longitude], axis=0)\n        data_dict[\"coords\"] = coords\n\n    # Extract cloud mask\n    if load_cloudmask:\n        cloud_mask = ds.cloud_mask.compute().to_numpy()\n        data_dict[\"cloud_mask\"] = cloud_mask\n\n    return data_dict\n</pre> def load_fn(     file: str,      load_coords: bool=True,     load_cloudmask: bool=True, ):     data_dict = {}     # Load dataset     ds: xr.Dataset = xr.load_dataset(file, engine=\"netcdf4\")      # Extract data     data = ds.Rad.compute().to_numpy()     data_dict[\"data\"] = data     # Extract wavelengths     wavelengths = ds.band_wavelength.compute().to_numpy()     data_dict[\"wavelengths\"] = wavelengths      # Extract coordinates     if load_coords:         latitude = ds.latitude.compute().to_numpy()         longitude = ds.longitude.compute().to_numpy()         coords = np.stack([latitude, longitude], axis=0)         data_dict[\"coords\"] = coords      # Extract cloud mask     if load_cloudmask:         cloud_mask = ds.cloud_mask.compute().to_numpy()         data_dict[\"cloud_mask\"] = cloud_mask      return data_dict In\u00a0[56]: Copied! <pre>idata = load_fn(f\"{save_dir}/goes16/analysis/20201001150019_patch_0.nc\")\n</pre> idata = load_fn(f\"{save_dir}/goes16/analysis/20201001150019_patch_0.nc\") In\u00a0[85]: Copied! <pre>from torch.utils.data import Dataset, DataLoader\nfrom typing import Optional, Callable\n\nclass NCDataReader(Dataset):\n    def __init__(self, data_dir: str, ext: str=\".nc\", transforms: Optional[Callable]=None):\n        self.data_dir = data_dir\n        self.data_filenames = get_list_filenames(data_dir, ext)\n        self.transforms = transforms\n\n    def __getitem__(self, ind) -&gt; np.ndarray:\n        img_path = self.data_filenames[ind]\n        x = load_fn(img_path)\n        if self.transforms is not None:\n            x = self.transforms(x)\n        return x\n\n    def __len__(self):\n        return len(self.data_filenames)\n</pre> from torch.utils.data import Dataset, DataLoader from typing import Optional, Callable  class NCDataReader(Dataset):     def __init__(self, data_dir: str, ext: str=\".nc\", transforms: Optional[Callable]=None):         self.data_dir = data_dir         self.data_filenames = get_list_filenames(data_dir, ext)         self.transforms = transforms      def __getitem__(self, ind) -&gt; np.ndarray:         img_path = self.data_filenames[ind]         x = load_fn(img_path)         if self.transforms is not None:             x = self.transforms(x)         return x      def __len__(self):         return len(self.data_filenames) In\u00a0[88]: Copied! <pre>ds = NCDataReader(f\"{save_dir}/goes16/analysis\")\ndl = DataLoader(ds, batch_size=2)\n</pre> ds = NCDataReader(f\"{save_dir}/goes16/analysis\") dl = DataLoader(ds, batch_size=2) In\u00a0[90]: Copied! <pre>out = next(iter(dl))\n</pre> out = next(iter(dl)) In\u00a0[92]: Copied! <pre>out[\"data\"].shape\n</pre> out[\"data\"].shape Out[92]: <pre>torch.Size([2, 16, 8, 8])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"pipelines/goes16/#goes16-pipeline","title":"GOES16 Pipeline\u00b6","text":""},{"location":"pipelines/goes16/#save-directory","title":"Save Directory\u00b6","text":""},{"location":"pipelines/goes16/#download","title":"Download\u00b6","text":""},{"location":"pipelines/goes16/#geoprocessing","title":"GeoProcessing\u00b6","text":""},{"location":"pipelines/goes16/#patching","title":"Patching\u00b6","text":""},{"location":"pipelines/goes16/#dataloading","title":"DataLoading\u00b6","text":""},{"location":"pipelines/goes16/#data-splits","title":"Data Splits\u00b6","text":""},{"location":"pipelines/goes16_/","title":"GOES16 Pipeline","text":"<p>In this tutorial, we will walk through how one can download data and prep for any further machine learning work with the GOES16 dataset. We will  1) download the data, 2) harmonize the data, 3) and create patches that are ready for consumption.</p>"},{"location":"pipelines/goes16_/#downloading-data","title":"Downloading Data","text":"<p>Firstly, we will download some data. The default configuration will consist of downloading over a years worth of data.</p> <pre><code>python rs_tools satellite=goes stage=download\n</code></pre>"},{"location":"pipelines/goes16_/#configuration","title":"Configuration","text":"<p>We can peek into the <code>rs_tools/config/example/download.yaml</code> configuration file to see some of the options we have to modify this.</p> <pre><code># PERIOD\nperiod:\n  start_date: '2020-10-01'\n  start_time: '00:00:00'\n  end_date: '2020-10-31'\n  end_time: '23:59:00'\n\n# CLOUD MASK\ncloud_mask: True\n\n# PATH FOR SAVING DATA\nsave_dir: data\n\ndefaults:\n  - _self_\n</code></pre> <p>We see that we can change some of the configurations available.  For this example, we will change this to download only a subset of data.</p> <p>We also have some more general options for the user to change which are satellite specific. These can be found in <code>rs_tools/config/example/satellite/goes.yaml</code></p> <pre><code>download:\n  _target_: rs_tools._src.data.goes.downloader_goes16.download\n  save_dir: ${save_dir}/goes16/raw\n  start_date: ${period.start_date}\n  start_time: ${period.start_time}\n  end_date: ${period.end_date}\n  end_time: ${period.end_time}\n  daily_window_t0: \"14:00:00\"\n  daily_window_t1: \"20:00:00\"\n  time_step: \"1:00:00\"\n</code></pre> <p>We will change the save directory, start/end time, and the time step.</p> <pre><code>python rs_tools satellite=goes stage=download save_dir=\"/path/to/savedir\" period.start_date=\"2020-10-01\" period.end_date=\"2020-10-02\" period.start_time=\"09:00:00\" period.end_time=\"21:00:00\" satellite.download.time_step=\"6:00:00\"\n</code></pre> <p>We notice that there are some files that should be available for processing.  In particular, we have two sets of files: the Level 1 Radiances and the Cloud Mask.</p> <pre><code>/path/to/savedir/goes16/raw/goes16/L1b\n/path/to/savedir/goes16/raw/goes16/CM\n</code></pre> <p>We use both in this project.</p>"},{"location":"pipelines/goes16_/#geoprocessing","title":"Geoprocessing","text":"<p>We have an extensive geoprocessing steps to be able to </p> <p>We can peek into the <code>rs_tools/config/example/download.yaml</code> configuration file to see some of the options we have to modify this.</p> <pre><code># PERIOD\ngeoprocess:\n  _target_: rs_tools._src.geoprocessing.goes.geoprocessor_goes16.geoprocess\n  read_path: ${read_path}/goes16/raw\n  save_path: ${save_path}/goes16/geoprocessed\n  resolution: null\n  region: \"-130 -15 -90 5\"\n  resample_method: bilinear\n</code></pre> <p>The most important options are the <code>resolution</code> and the <code>region</code>. The resolution is a float or integer that is measured in km.</p> <p>Below, we have an example of the command we </p> <pre><code>python rs_tools satellite=goes stage=download save_dir=\"/path/to/new/save/dir\" period.start_date=\"2020-10-01\" period.end_date=\"2020-10-02\" period.start_time=\"09:00:00\" period.end_time=\"23:00:00\" satellite.time_step=\"6:00:00\"\n</code></pre> <p>We can see the saved data are clean</p> <pre><code>/path/to/savedir/goes16/geoprocessed/20201001150019_goes16.nc\n/path/to/savedir/goes16/geoprocessed/20201002150019_goes16.nc\n</code></pre> <pre><code>PYTHONPATH=\".\" python rs_tools satellite=goes stage=download save_dir=\"/pool/usuarios/juanjohn/data/iti/raw\" period.start_date=\"2020-10-01\" period.end_date=\"2020-10-02\" period.start_time=\"09:00:00\" period.end_time=\"21:00:00\" satellite.download.time_step=\"6:00:00\"\n\nPYTHONPATH=\".\" python rs_tools satellite=goes stage=geoprocess read_path=\"/pool/usuarios/juanjohn/data/iti/\" save_path=\"/pool/usuarios/juanjohn/data/iti/\" satellite.geoprocess.resolution=10000\n\nPYTHONPATH=\".\" python rs_tools satellite=goes stage=patch read_path=\"/pool/usuarios/juanjohn/data/iti/\" save_path=\"/pool/usuarios/juanjohn/data/iti/\" nan_cutoff=1.0 patch_size=8\n</code></pre> <p>Warning:  The data is quite heavy.  Make sure you have enough space!</p>"},{"location":"pipelines/goes16_/#geoprocessing-data","title":"GeoProcessing Data","text":"<p>In this stage, we will do some light geoprocessing to \"harmonize\" the data.</p> <pre><code>python rs_tools satellite=goes stage=geoprocess read_path=\"/path/to/new/save/dir\"\n</code></pre> <p>TODO: Explain the Concepts: - Start/End Date - Start/End Time - Daily Window - Time Step</p>"},{"location":"scripts/gen_pages/","title":"Gen pages","text":"In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\nfrom mkdocs_gen_files import Nav\nfrom typing import Iterable, Mapping, Union\nimport dataclasses\n</pre> import mkdocs_gen_files from mkdocs_gen_files import Nav from typing import Iterable, Mapping, Union import dataclasses In\u00a0[\u00a0]: Copied! <pre>class CustomNav(Nav):\n    def __init__(self):\n        super().__init__()\n\n    @dataclasses.dataclass\n    class Item:\n        level: int\n        title: str\n        filename: Union[str, None]\n\n    @classmethod\n    def _items(cls, data: Mapping, level: int) -&gt; Iterable[Item]:\n        for key, value in data.items():\n            if key is not None:\n                if key == \"rs_tools\":\n                    title = \"RSTOOLS\"\n                elif key == \"_src\":\n                    title = \"Source\"\n                elif key == \"modis\":\n                    title = \"MODIS\"\n                elif key == \"msg\":\n                    title = \"MSG\"\n                elif key == \"goes\":\n                    title = \"GOES\"\n                else:\n                    title = key.title()\n\n                # title = title.replace(\"_\", \" \")\n                # title = title.replace(\"Matern\", \"Mat\u00e9rn\")\n\n                yield cls.Item(level=level, title=title, filename=value.get(None))\n                yield from cls._items(value, level + 1)\n</pre> class CustomNav(Nav):     def __init__(self):         super().__init__()      @dataclasses.dataclass     class Item:         level: int         title: str         filename: Union[str, None]      @classmethod     def _items(cls, data: Mapping, level: int) -&gt; Iterable[Item]:         for key, value in data.items():             if key is not None:                 if key == \"rs_tools\":                     title = \"RSTOOLS\"                 elif key == \"_src\":                     title = \"Source\"                 elif key == \"modis\":                     title = \"MODIS\"                 elif key == \"msg\":                     title = \"MSG\"                 elif key == \"goes\":                     title = \"GOES\"                 else:                     title = key.title()                  # title = title.replace(\"_\", \" \")                 # title = title.replace(\"Matern\", \"Mat\u00e9rn\")                  yield cls.Item(level=level, title=title, filename=value.get(None))                 yield from cls._items(value, level + 1) In\u00a0[\u00a0]: Copied! <pre>nav = CustomNav()\n</pre> nav = CustomNav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\"rs_tools\").rglob(\"*.py\")):\n    module_path = path.relative_to(\".\").with_suffix(\"\")\n    doc_path = path.relative_to(\"rs_tools\").with_suffix(\".md\")\n    full_doc_path = Path(\"api\", doc_path)\n\n    parts = list(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n        continue\n    elif parts[-1] == \"__main__\":\n        continue\n\n    # final_part = parts[-1].title()\n    # parts = parts[:-1] + [final_part]\n    nav[parts] = doc_path.as_posix()\n\n    # print(full_doc_path)\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        identifier = \".\".join(parts)\n        title = identifier.split(\".\")[-1].replace(\"_\", \" \").title()\n        # if title == \"Gps\":\n        #     title = \"GPs\"\n        # elif title == \"Rbf\":\n        #     title = \"RBF\"\n\n        # if \"Matern\" in title:\n        #     title = title.replace(\"Matern\", \"Mat\u00e9rn\")\n\n        print(f\"# {title}\\n\", file=fd)\n        print(\"::: \" + identifier, file=fd)  #\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(Path(\"rs_tools\").rglob(\"*.py\")):     module_path = path.relative_to(\".\").with_suffix(\"\")     doc_path = path.relative_to(\"rs_tools\").with_suffix(\".md\")     full_doc_path = Path(\"api\", doc_path)      parts = list(module_path.parts)      if parts[-1] == \"__init__\":         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")         continue     elif parts[-1] == \"__main__\":         continue      # final_part = parts[-1].title()     # parts = parts[:-1] + [final_part]     nav[parts] = doc_path.as_posix()      # print(full_doc_path)     with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         identifier = \".\".join(parts)         title = identifier.split(\".\")[-1].replace(\"_\", \" \").title()         # if title == \"Gps\":         #     title = \"GPs\"         # elif title == \"Rbf\":         #     title = \"RBF\"          # if \"Matern\" in title:         #     title = title.replace(\"Matern\", \"Mat\u00e9rn\")          print(f\"# {title}\\n\", file=fd)         print(\"::: \" + identifier, file=fd)  #      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"api/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"api/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"}]}